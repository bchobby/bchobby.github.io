<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>图像处理实战02-yolov5目标检测 - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="图像处理实战02-yolov5目标检测" />
<meta property="og:description" content="yolov5 YOLOv5 是一种目标检测算法，它是 YOLO (You Only Look Once) 系列算法的最新版本。YOLOv5 采用了一种新的架构，它包括一个基于 CSPNet (Cross Stage Partial Network) 的主干网络以及一系列改进的技巧，如多尺度训练、数据增强、网络混合精度训练等，从而实现了更快的检测速度和更好的检测精度。
YOLOv5 支持多种类型的目标检测任务，如物体检测、人脸检测、车辆检测等，可以应用于各种实际场景，如智能安防、自动驾驶、机器人视觉等。同时，YOLOv5 还提供了预训练的模型和开源代码，方便开发者进行模型的训练和应用。
github地址：https://github.com/ultralytics/yolov5/blob/master/README.zh-CN.md
官网：https://ultralytics.com/
发展历程 YOLO（You Only Look Once）是一系列的目标检测模型，由Joseph Redmon等人开发。以下是YOLO系列的发展历程：
YOLOv1：于2015年首次提出，是YOLO系列的第一个版本。YOLOv1通过将目标检测任务转化为回归问题，将图像划分为网格并预测每个网格的边界框和类别概率。然而，YOLOv1存在定位不准确和对小目标敏感的问题。
YOLOv2（YOLO9000）：于2016年提出，是YOLO系列的第二个版本。YOLOv2通过引入Darknet-19网络结构、使用anchor boxes和多尺度预测来改进检测性能。同时，YOLOv2还引入了目标类别的语义分割，可以检测更多类别的目标。
YOLOv3：于2018年提出，是YOLO系列的第三个版本。YOLOv3针对YOLOv2存在的问题进行了改进，引入了多尺度预测、使用FPN结构和使用更小的anchor boxes等技术，提高了检测精度和对小目标的检测能力。
YOLOv4：于2020年提出，是YOLO系列的第四个版本。YOLOv4在YOLOv3的基础上引入了一系列改进，包括CSPDarknet53作为主干网络、使用SAM和PANet模块来提取特征、使用YOLOv3和YOLOv4的预训练权重进行初始化等，提高了检测性能和速度。
YOLOv5：于2020年提出，是YOLO系列的第五个版本。YOLOv5采用了轻量化的网络结构，提高了检测的速度，并引入了一些新功能，如YOLOv5-seg分割模型、Paddle Paddle导出功能、YOLOv5 AutoCache自动缓存功能和Comet日志记录和可视化集成功能。
总体而言，YOLO系列模型通过不断的改进和优化，提高了目标检测的性能和速度，并在计算机视觉领域取得了重要的突破。
yolov8 YOLOv8是YOLO系列模型的一个变种，它在YOLOv5的基础上进行了改进和优化。YOLOv8模型包含了检测（Detect）、分割（Segment）和姿态估计（Pose）、跟踪（Track）以及分类（Classify）等功能。下面是对这些功能的简要说明：
检测（Detect）：YOLOv8模型能够对图像或视频中的目标进行实时的物体检测。它通过预测目标的边界框和类别信息来完成检测任务。
分割（Segment）：YOLOv8模型还支持目标分割的功能，即将图像中的每个像素进行分类，将不同的目标区域进行分割。这个功能可以用于识别图像中的不同物体，并进行更精确的定位和分析。
姿态估计（Pose）：YOLOv8模型还可以对检测到的目标进行姿态估计，即推断目标在三维空间中的姿态信息。这对于一些需要了解目标的方向和位置的应用非常有用，比如人体姿态分析、机器人导航等。
跟踪（Track）：YOLOv8模型还具有目标跟踪的功能，即在视频中连续追踪相同目标的位置和轨迹。这对于视频监控、自动驾驶等应用非常重要。
分类（Classify）：除了目标检测和分割功能之外，YOLOv8模型还可以对检测到的目标进行分类，即给出目标的类别信息。这对于了解目标的属性和进行更细粒度的分析非常重要。
总而言之，YOLOv8模型综合了多种功能，包括检测、分割、姿态估计、跟踪和分类等，使其具备了更广泛的应用领域和更强大的功能。
github地址：https://github.com/ultralytics/ultralytics
v5入门示例 安装 克隆 repo，并要求在 Python&gt;=3.7.0 环境中安装 requirements.txt ，且要求 PyTorch&gt;=1.7 。
micromamba create prefix=d:/python380 python=3.8 #创建3.8的虚拟环境 micromamba activate d:/python380 git clone https://github.com/ultralytics/yolov5 # clone cd yolov5 pip install -r requirements." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/0200732ff767bb3f06de5506da0fad83/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-15T18:23:50+08:00" />
<meta property="article:modified_time" content="2023-06-15T18:23:50+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">图像处理实战02-yolov5目标检测</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="yolov5_0"></a>yolov5</h2> 
<p>YOLOv5 是一种目标检测算法，它是 YOLO (You Only Look Once) 系列算法的最新版本。YOLOv5 采用了一种新的架构，它包括一个基于 CSPNet (Cross Stage Partial Network) 的主干网络以及一系列改进的技巧，如多尺度训练、数据增强、网络混合精度训练等，从而实现了更快的检测速度和更好的检测精度。</p> 
<p>YOLOv5 支持多种类型的目标检测任务，如物体检测、人脸检测、车辆检测等，可以应用于各种实际场景，如智能安防、自动驾驶、机器人视觉等。同时，YOLOv5 还提供了预训练的模型和开源代码，方便开发者进行模型的训练和应用。</p> 
<p>github地址：https://github.com/ultralytics/yolov5/blob/master/README.zh-CN.md<br> 官网：https://ultralytics.com/</p> 
<h3><a id="_7"></a>发展历程</h3> 
<p>YOLO（You Only Look Once）是一系列的目标检测模型，由Joseph Redmon等人开发。以下是YOLO系列的发展历程：</p> 
<ol><li> <p>YOLOv1：于2015年首次提出，是YOLO系列的第一个版本。YOLOv1通过将目标检测任务转化为回归问题，将图像划分为网格并预测每个网格的边界框和类别概率。然而，YOLOv1存在定位不准确和对小目标敏感的问题。</p> </li><li> <p>YOLOv2（YOLO9000）：于2016年提出，是YOLO系列的第二个版本。YOLOv2通过引入Darknet-19网络结构、使用anchor boxes和多尺度预测来改进检测性能。同时，YOLOv2还引入了目标类别的语义分割，可以检测更多类别的目标。</p> </li><li> <p>YOLOv3：于2018年提出，是YOLO系列的第三个版本。YOLOv3针对YOLOv2存在的问题进行了改进，引入了多尺度预测、使用FPN结构和使用更小的anchor boxes等技术，提高了检测精度和对小目标的检测能力。</p> </li><li> <p>YOLOv4：于2020年提出，是YOLO系列的第四个版本。YOLOv4在YOLOv3的基础上引入了一系列改进，包括CSPDarknet53作为主干网络、使用SAM和PANet模块来提取特征、使用YOLOv3和YOLOv4的预训练权重进行初始化等，提高了检测性能和速度。</p> </li><li> <p>YOLOv5：于2020年提出，是YOLO系列的第五个版本。YOLOv5采用了轻量化的网络结构，提高了检测的速度，并引入了一些新功能，如YOLOv5-seg分割模型、Paddle Paddle导出功能、YOLOv5 AutoCache自动缓存功能和Comet日志记录和可视化集成功能。</p> </li></ol> 
<p>总体而言，YOLO系列模型通过不断的改进和优化，提高了目标检测的性能和速度，并在计算机视觉领域取得了重要的突破。</p> 
<h3><a id="yolov8_22"></a>yolov8</h3> 
<p>YOLOv8是YOLO系列模型的一个变种，它在YOLOv5的基础上进行了改进和优化。YOLOv8模型包含了检测（Detect）、分割（Segment）和姿态估计（Pose）、跟踪（Track）以及分类（Classify）等功能。下面是对这些功能的简要说明：</p> 
<ol><li> <p>检测（Detect）：YOLOv8模型能够对图像或视频中的目标进行实时的物体检测。它通过预测目标的边界框和类别信息来完成检测任务。</p> </li><li> <p>分割（Segment）：YOLOv8模型还支持目标分割的功能，即将图像中的每个像素进行分类，将不同的目标区域进行分割。这个功能可以用于识别图像中的不同物体，并进行更精确的定位和分析。</p> </li><li> <p>姿态估计（Pose）：YOLOv8模型还可以对检测到的目标进行姿态估计，即推断目标在三维空间中的姿态信息。这对于一些需要了解目标的方向和位置的应用非常有用，比如人体姿态分析、机器人导航等。</p> </li><li> <p>跟踪（Track）：YOLOv8模型还具有目标跟踪的功能，即在视频中连续追踪相同目标的位置和轨迹。这对于视频监控、自动驾驶等应用非常重要。</p> </li><li> <p>分类（Classify）：除了目标检测和分割功能之外，YOLOv8模型还可以对检测到的目标进行分类，即给出目标的类别信息。这对于了解目标的属性和进行更细粒度的分析非常重要。</p> </li></ol> 
<p>总而言之，YOLOv8模型综合了多种功能，包括检测、分割、姿态估计、跟踪和分类等，使其具备了更广泛的应用领域和更强大的功能。<br> <img src="https://images2.imgbox.com/95/b6/0A8Uyqlo_o.png" alt="在这里插入图片描述"><br> github地址：https://github.com/ultralytics/ultralytics</p> 
<h3><a id="v5_39"></a>v5入门示例</h3> 
<h4><a id="_40"></a>安装</h4> 
<p>克隆 repo，并要求在 Python&gt;=3.7.0 环境中安装 requirements.txt ，且要求 PyTorch&gt;=1.7 。</p> 
<pre><code>micromamba create prefix=d:/python380 python=3.8  #创建3.8的虚拟环境
micromamba activate d:/python380
git clone https://github.com/ultralytics/yolov5  # clone
cd yolov5
pip install -r requirements.txt  # install
</code></pre> 
<p>源代码目录结构</p> 
<pre><code>yolov5/
├── data/                  # 数据集配置目录
│   ├── coco.yaml            # COCO数据集配置文件，里面有数据集的下载地址和加载的python脚本
│   ├──ImageNet.yaml           # ImageNet数据集
│   ├── custom.yaml          # 自定义数据集配置文件
│   └── ...                  # 其他数据集配置文件
├── models/                # 模型定义目录
│   ├── common.py            # 通用函数和类定义
│   ├── experimental.py      # 实验性模型定义
│   ├── export.py            # 导出模型为ONNX的脚本
│   ├── models.py            # YOLOv5模型定义
│   ├── yolo.py              # YOLO类定义
│   └── ...                  # 其他模型定义文件
├── utils/                 # 实用工具目录
│   ├── autoanchor.py        # 自动锚框生成工具
│   ├── datasets.py          # 数据集处理工具
│   ├── general.py           # 通用实用函数
│   ├── google_utils.py      # Google云平台工具
│   ├── loss.py              # 损失函数定义
│   ├── metrics.py           # 评估指标定义
│   ├── torch_utils.py       # PyTorch工具
│   ├── wandb_logging.py     # WandB日志记录工具
│   └── ...                  # 其他实用工具文件
├── runs/                 # 训练和预测的结果输出目录
│   ├── detect        # 使用detect.py训练后输出目录，输出的目录是[ex自增数字]
│   ├── train        # 使用detect.py训练后输出目录，输出的目录是[ex自增数字],包含了训练好的模型和测试集效果
├── weights/               # 预训练模型权重目录
├── .gitignore             # Git忽略文件配置
├── Dockerfile             # Docker容器构建文件
├── LICENSE                # 许可证文件
├── README.md              # 项目说明文档
├── requirements.txt       # 项目依赖包列表
├── train.py               # 训练脚本
├── detect.py               # 预测脚本
├── export.py               # 导出YOLOv5 PyTorch model to 其他格式
├── hubconf.py               # hubconf.py文件是用于定义模型和数据集的Python模块
└── ...                    # 其他源代码文件
</code></pre> 
<blockquote> 
 <p>这里通过yolov5可以下载到很多常用的训练数据集，而且很轻松的找到下载地址,如ImageNet,<br> coco128等，不用自己辛苦的找了</p> 
</blockquote> 
<h4><a id="_93"></a>模型下载</h4> 
<p>下载地址：https://github.com/ultralytics/yolov5/releases</p> 
<h5><a id="v61_95"></a>v6.1</h5> 
<p>这里的版本是v6.1是yolov5的子版本号</p> 
<h6><a id="Pretrained_Checkpoints_97"></a>Pretrained Checkpoints</h6> 
<p>Pretrained Checkpoints 是预训练权重文件的一种称呼。在深度学习中，预训练权重是指在大规模数据集上通过无监督学习或有监督学习得到的模型参数。这些参数通常可以被用来初始化一个新的模型，从而加速模型训练并提高模型的性能。</p> 
<p>Pretrained Checkpoints 是指已经训练好的预训练权重文件，可以用来初始化一个新的模型，并继续训练这个模型以适应新的任务或数据集。这种方法被称为迁移学习，可以大大提高模型的训练效率和泛化能力。在计算机视觉领域，常见的预训练网络包括 VGG、ResNet、Inception、MobileNet 等。</p> 
<h6><a id="_101"></a>模型概述</h6> 
<p>以下模型列的解释</p> 
<table><thead><tr><th>列名</th><th>解释</th></tr></thead><tbody><tr><td>Model</td><td>模型的名称</td></tr><tr><td>size(pixels)</td><td>输入图像的大小（以像素为单位）</td></tr><tr><td>mAPval0.5:0.95</td><td>在验证集上的平均精确度（mean Average Precision），考虑所有IOU阈值从0.5到0.95的情况，准确率是%</td></tr><tr><td>mAPval0.5</td><td>在验证集上的平均精确度，只考虑IOU阈值为0.5的情况</td></tr><tr><td>Speed CPU b1(ms)</td><td>在CPU上使用batch size为1时的推理速度（以毫秒为单位）</td></tr><tr><td>Speed V100 b1(ms)</td><td>在NVIDIA V100 GPU上使用batch size为1时的推理速度（以毫秒为单位）</td></tr><tr><td>Speed V100 b32(ms)</td><td>在NVIDIA V100 GPU上使用batch size为32时的推理速度（以毫秒为单位）</td></tr><tr><td>params (M)</td><td>模型的参数量（以百万为单位）</td></tr><tr><td>FLOPs @640 (B)</td><td>在输入图像大小为640时，模型的浮点运算次数（以十亿为单位）</td></tr></tbody></table> 
<table><thead><tr><th>Model</th><th>size(pixels)</th><th>mAPval0.5:0.95</th><th>mAPval0.5</th><th>Speed CPU b1(ms)</th><th>Speed V100 b1(ms)</th><th>Speed V100 b32(ms)</th><th>params (M)</th><th>FLOPs @640 (B)</th></tr></thead><tbody><tr><td><a href="https://github.com/ultralytics/yolov5/releases">YOLOv5n</a></td><td>640</td><td>28.0</td><td>45.7</td><td><strong>45</strong></td><td><strong>6.3</strong></td><td><strong>0.6</strong></td><td><strong>1.9</strong></td><td><strong>4.5</strong></td></tr><tr><td><a href="https://github.com/ultralytics/yolov5/releases">YOLOv5s</a></td><td>640</td><td>37.4</td><td>56.8</td><td>98</td><td>6.4</td><td>0.9</td><td>7.2</td><td>16.5</td></tr><tr><td><a href="https://github.com/ultralytics/yolov5/releases">YOLOv5m</a></td><td>640</td><td>45.4</td><td>64.1</td><td>224</td><td>8.2</td><td>1.7</td><td>21.2</td><td>49.0</td></tr><tr><td><a href="https://github.com/ultralytics/yolov5/releases">YOLOv5l</a></td><td>640</td><td>49.0</td><td>67.3</td><td>430</td><td>10.1</td><td>2.7</td><td>46.5</td><td>109.1</td></tr><tr><td><a href="https://github.com/ultralytics/yolov5/releases">YOLOv5x</a></td><td>640</td><td>50.7</td><td>68.9</td><td>766</td><td>12.1</td><td>4.8</td><td>86.7</td><td>205.7</td></tr><tr><td><a href="https://github.com/ultralytics/yolov5/releases">YOLOv5n6</a></td><td>1280</td><td>36.0</td><td>54.4</td><td>153</td><td>8.1</td><td>2.1</td><td>3.2</td><td>4.6</td></tr><tr><td><a href="https://github.com/ultralytics/yolov5/releases">YOLOv5s6</a></td><td>1280</td><td>44.8</td><td>63.7</td><td>385</td><td>8.2</td><td>3.6</td><td>12.6</td><td>16.8</td></tr><tr><td><a href="https://github.com/ultralytics/yolov5/releases">YOLOv5m6</a></td><td>1280</td><td>51.3</td><td>69.3</td><td>887</td><td>11.1</td><td>6.8</td><td>35.7</td><td>50.0</td></tr><tr><td><a href="https://github.com/ultralytics/yolov5/releases">YOLOv5l6</a></td><td>1280</td><td>53.7</td><td>71.3</td><td>1784</td><td>15.8</td><td>10.5</td><td>76.8</td><td>111.4</td></tr></tbody></table> 
<h5><a id="v70_127"></a>v7.0</h5> 
<p>新的YOLOv5 v7.0实例分割模型是世界上最快、最准确的，超过了所有当前的SOTA基准。我们使它们非常简单易用，可以轻松进行训练、验证和部署。<br> 这个版本中的主要目标是引入与我们现有的目标检测模型类似的超级简单的YOLOv5分割工作流程。<br> 重要更新</p> 
<ul><li>分割模型 ⭐ 新增：第一次提供了SOTA YOLOv5-seg COCO预训练的分割模型（由@glenn-jocher、@AyushExel和@Laughing-q开发的#9052）</li><li>Paddle Paddle导出：使用python export.py --include paddle 可以将任何YOLOv5模型（cls、seg、det）导出为Paddle格式（由@glenn-jocher开发的#9459）</li><li>YOLOv5 AutoCache：使用python train.py --cache ram 现在会扫描可用内存并与预测的数据集RAM使用量进行比较。这降低了缓存风险，并应该有助于提高数据集缓存功能的使用率，从而显著加快训练速度。（由@glenn-jocher开发的#10027）</li><li>Comet日志记录和可视化集成：永久免费，Comet可以保存YOLOv5模型，恢复训练，并进行交互式可视化和调试预测。（由@DN6开发的#9232）</li></ul> 
<table><thead><tr><th>Model</th><th>size (pixels)</th><th>mAPbox<br>50-95</th><th>mAPmask<br>50-95</th><th>Train time 300 epochs<br>A100 (hours)</th><th>Speed ONNX CPU<br>(ms)</th><th>Speed TRT A100<br>(ms)</th><th>params (M)</th><th>FLOPs @640(B)</th></tr></thead><tbody><tr><td><a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n-seg.pt">YOLOv5n-seg</a></td><td>640</td><td>27.6</td><td>23.4</td><td>80:17</td><td><strong>62.7</strong></td><td><strong>1.2</strong></td><td><strong>2.0</strong></td><td><strong>7.1</strong></td></tr><tr><td><a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-seg.pt">YOLOv5s-seg</a></td><td>640</td><td>37.6</td><td>31.7</td><td>88:16</td><td>173.3</td><td>1.4</td><td>7.6</td><td>26.4</td></tr><tr><td><a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m-seg.pt">YOLOv5m-seg</a></td><td>640</td><td>45.0</td><td>37.1</td><td>108:36</td><td>427.0</td><td>2.2</td><td>22.0</td><td>70.8</td></tr><tr><td><a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l-seg.pt">YOLOv5l-seg</a></td><td>640</td><td>49.0</td><td>39.9</td><td>66:43 (2x)</td><td>857.4</td><td>2.9</td><td>47.9</td><td>147.7</td></tr><tr><td><a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x-seg.pt">YOLOv5x-seg</a></td><td>640</td><td><strong>50.7</strong></td><td><strong>41.4</strong></td><td>62:56 (3x)</td><td>1579.2</td><td>4.5</td><td>88.8</td><td>265.7</td></tr></tbody></table> 
<p>我这里选择一个V6.1模型<a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n6.pt">yolov5n6.pt</a><br> 将模型丢到yolov5项目根目录即可<br> <img src="https://images2.imgbox.com/28/5c/1tnT1r8O_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="_147"></a>预测</h4> 
<p>因为预训练模型，已经有检测某些类别能力，我们可以看下data/coco.yml中names可以看到总共有80个类别<br> <img src="https://images2.imgbox.com/f7/de/0fj6HOtf_o.png" alt="在这里插入图片描述"><br> 在yolov5中可以使用./detect.py脚本来进行目标物品检测。<br> 以下是对"./detect.py"脚本中常见参数的详细解释：</p> 
<ol><li> <p><code>--source</code>：指定输入源，可以是图像路径、视频文件路径或摄像头索引（默认为当前目录data/images，里面就两张图片）。</p> </li><li> <p><code>--weights</code>：指定模型权重文件的路径。可以是本地路径或PaddleHub模型中心的模型名称，默认是当前目录的yolov5s.pt。</p> </li><li> <p><code>--data</code>：指定要使用的数据集的配置文件。数据集的配置文件包含了数据集的路径、类别标签、训练集、验证集和测试集的划分等信息,默认data/coco128.yaml，选填。</p> </li><li> <p><code>--img-size</code>：指定输入图像的尺寸，格式为",“，例如"640,480”。默认为640x640。</p> </li><li> <p><code>--conf-thres</code>：目标置信度阈值，范围为0到1。超过该阈值的目标将被保留，默认为0.25。</p> </li><li> <p><code>--iou-thres</code>：NMS（非极大值抑制）的IoU（交并比）阈值，范围为0到1。重叠度大于该阈值的目标将被合并，默认为0.45。</p> </li><li> <p><code>--max-det</code>：每个图像中最多检测的目标数，默认为100。</p> </li><li> <p><code>--device</code>：指定使用的设备，可以是"cpu"或"cuda"。默认为"cpu"。</p> </li><li> <p><code>--view-img</code>：在检测过程中显示图像窗口。</p> </li><li> <p><code>--save-txt</code>：保存检测结果的txt文件。</p> </li><li> <p><code>--save-conf</code>：保存检测结果的置信度。</p> </li><li> <p><code>--save-crop</code>：保存检测结果的裁剪图像。</p> </li><li> <p><code>--half</code>：使用半精度浮点数进行推理。</p> </li></ol> 
<p>这些参数可以根据您的需求进行调整，以获得最佳的检测结果。您可以在运行脚本时使用<code>--help</code>参数查看更多参数选项和说明。</p> 
<p>执行命令预测</p> 
<pre><code>python ./detect.py --source ./data/images --weight ./yolov5n6.pt
</code></pre> 
<p>执行结果</p> 
<pre><code>(D:\condaenv\yolov5) D:\code1\yolov5-master\yolov5-master&gt;python ./detect.py --source ./data/images --weight ./yolov5n6.pt
detect: weights=['./yolov5n6.pt'], source=./data/images, data=data\coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=Fal
se, augment=False, visualize=False, update=False, project=runs\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1
YOLOv5  2023-5-30 Python-3.8.16 torch-2.0.1+cpu CPU

Fusing layers...
YOLOv5n6 summary: 280 layers, 3239884 parameters, 0 gradients
image 1/2 D:\code1\yolov5-master\yolov5-master\data\images\bus.jpg: 640x512 4 persons, 1 bus, 211.9ms
image 2/2 D:\code1\yolov5-master\yolov5-master\data\images\zidane.jpg: 384x640 3 persons, 1 tie, 152.9ms
Speed: 1.0ms pre-process, 182.4ms inference, 3.0ms NMS per image at shape (1, 3, 640, 640)
Results saved to runs\detect\exp8

</code></pre> 
<p>找到runs\detect\exp8 打开目录查看分类图片<br> <img src="https://images2.imgbox.com/0d/91/ZSX4jlrh_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/3c/74/4YNeeKWQ_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_205"></a>训练模型</h3> 
<p>参考自官网：https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/#before-you-start</p> 
<h4><a id="_207"></a>准备数据集</h4> 
<h5><a id="yaml_208"></a>创建数据集yaml</h5> 
<p>COCO128是一个小型教程数据集的例子，由COCO train2017中的前128张图像组成。这128张图像同时用于训练和验证，以验证我们的训练流程能够过拟合。data/coco128.yaml是数据集配置文件，定义了以下内容：<br> 1）数据集根目录路径以及训练/验证/测试图像目录的相对路径（或包含图像路径的*.txt文件）；<br> 2）类别名称字典。</p> 
<pre><code># Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
path: ../datasets/coco128  # dataset root dir
train: images/train2017  # train images (relative to 'path') 128 images
val: images/train2017  # val images (relative to 'path') 128 images
test:  # test images (optional)

# Classes (80 COCO classes)
names:
  0: person
  1: bicycle
  2: car
  ...
  77: teddy bear
  78: hair drier
  79: toothbrush
  # Download script/URL (optional)
download: https://ultralytics.com/assets/coco128.zip
</code></pre> 
<p>https://ultralytics.com/assets/coco128.zip下载后，目录结构如下<br> <img src="https://images2.imgbox.com/1d/c4/FoH2is9c_o.png" alt="在这里插入图片描述"><br> 我这里用来训练判断一个身份证的正反面，我在项目根目录新建一个idcard目录，下面在建一个mul目录，这个目录只是用来训练不同的身份证信息用来区分的，我们的所有数据集都在mul目录</p> 
<pre><code># Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
path: ./idcard/mul  # dataset root dir
train: images  # train images 
val: images  # val images
test: images   # test images 

# Classes
names:
  0: idcard_z  #表示身份证正面
  1: idcard_f   #表示身份证反面
</code></pre> 
<p>注意这里yolov5回自动找path下的train目录在加上你的images作为图片的目录<br> 比如真正的训练目录是：./idcard/mul/train/images，images的同级目录下会有个labels目录是标注<br> 验证集的目录是：./idcard/mul/val/images<br> 测试集的目录是：./idcard/test/val/images<br> <img src="https://images2.imgbox.com/f2/0b/JBUGsWdI_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>一般来说，常见的做法是将数据集划分为训练集、验证集和测试集，比如将数据划分为70%的训练集、15%的验证集和15%的测试集。这种比例通常适用于较小的数据集。对于较大的数据集，可以考虑增加验证集和测试集的比例。</p> 
</blockquote> 
<h5><a id="labels_255"></a>创建labels</h5> 
<p>在使用注释工具（labelme,lableimg）为图像标注后，将标签导出为YOLO格式，每个图像对应一个*.txt文件（如果图像中没有对象，则不需要*.txt文件）。*.txt文件的规范如下：</p> 
<ul><li>每个对象占据一行</li><li>每行的格式为：类别 x中心点 y中心点 宽度 高度。<br> 框的坐标必须使用归一化的xywh格式（范围在0-1之间）。如果您的框的坐标是以像素为单位的，则需要将x中心点和宽度除以图像宽度，并将y中心点和高度除以图像高度。</li><li>类别编号从零开始（索引为0），和数据集yaml的names索引对应。</li></ul> 
<p>这里建议使用labelimg标注</p> 
<pre><code>pip install labelimg -i https://pypi.tuna.tsinghua.edu.cn/simple
</code></pre> 
<p>切换到当前环境输入labelimg ，输入labelimage命令打开<br> <img src="https://images2.imgbox.com/75/25/oCTE09yb_o.png" alt="在这里插入图片描述"></p> 
<p>选择open dir选择你的需要标记的图片目录(idcard/mul/train/images目录)，Change Save Dir选择你的idcard/mul/train/labels目录,选择YOLO格式<br> 打开了图片后，需要一张一张图片的标记，常用的操作步骤是：</p> 
<ol><li>按w唤起一个矩形框，选择你要选择的目标，选择后，弹出label，注意要先标注一个data.yaml中索引为0的，然后是1的，后面在弹出是可以选择的。<br> <img src="https://images2.imgbox.com/65/ea/LbBYLEbp_o.png" alt="在这里插入图片描述"></li><li>标准完成后ctrl+s保存。</li><li>按键盘d键切换到下一张图片，继续按w矩形框标注，知道所有图片完成。</li></ol> 
<p>在你的labels目录下会有个classes.txt，看下他的顺序是否和data.yaml一致，如果不一致，不要调整classes.txt,调整data.yaml保持一致就行。</p> 
<h5><a id="_279"></a>训练</h5> 
<p>我这里准备了差不多350个标注好的图片，训练后识别率98%。<br> 使用train.py执行</p> 
<pre><code>#  --weight是指定初始的权重，可以用它来fine tuning调整训练你自己的模型。
python train.py --batch-size 4 --epochs 10 --data .\idcard\mul\idcard.yaml --weight .\yolov5n6.pt
</code></pre> 
<p>执行完成后，runs\trains\expn\weights\best.pt就是训练好的模型，可以使用之前的detect.py指定这个模型来预测下</p> 
<pre><code>python ./detect.py --source .\idcard\mul\test\images --weight .\runs\train\exp3\weights\best.pt
</code></pre> 
<p>查看runs\detect\expn\下的预测图片<br> <img src="https://images2.imgbox.com/8d/89/B5PYt5Kg_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/8e/93/HGSCNwMx_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/0e/3d/vjonjSu7_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_297"></a>模型应用</h3> 
<p>我们需要在我们的应用使用生成好的best.pt模型可以使用torch.hub</p> 
<pre><code>#使用我们本地之前用于训练的yolov5-master，我有把best.pt拷贝到当前目录
model = torch.hub.load('D:\\code1\\yolov5-master\\yolov5-master', 'custom', path='./best.pt', source='local')  # local repo
#print(model)
# 读取图像
img = cv2.imread('../images/zm.jpg')
# 进行预测
results = model(img)
resultLabel=[]
# 解析预测结果
for result in results.xyxy[0]:
    x1, y1, x2, y2, conf, cls = result.tolist()
    if conf &gt; 0.5:
        # 绘制边框和标签
        cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
        cv2.putText(img, f"{model.names[int(cls)]} {conf:.2f}", (int(x1), int(y1 - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        resultLabel.append(model.names[int(cls)])
# 显示图像
print("预测的结果是",resultLabel)
plt.imshow(img)
plt.show()
</code></pre> 
<p><img src="https://images2.imgbox.com/d0/98/sV0yChQ6_o.png" alt="在这里插入图片描述"></p> 
<p>这是官方提供在线的版本调用，但是程序会自动去下载ultralytics/yolov5包和yolov5s模型，速度很慢</p> 
<pre><code>import torch
# Model
model = torch.hub.load("ultralytics/yolov5", "yolov5s")  # or yolov5n - yolov5x6, custom
# Images
img = "https://ultralytics.com/images/zidane.jpg"  # or file, Path, PIL, OpenCV, numpy, list
# Inference
results = model(img)
# Results
results.print()  # or .show(), .save(), .crop(), .pandas(), etc.
</code></pre>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7e1caf30fde32bc6d40226372fe09f7e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Flink SQL之Interval Joins</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8816bc5fd2c66f6f05e8bf21718249b3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Java策略模式</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>