<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Observability：如何使用 Elastic Agents 把定制的日志摄入到 Elasticsearch 中 - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Observability：如何使用 Elastic Agents 把定制的日志摄入到 Elasticsearch 中" />
<meta property="og:description" content="在我之前的文章 “Observability：使用 Elastic Agent 来摄入日志及指标 - Elastic Stack 8.0”，我详细地描述了如何安装 Elasticsearch，Stack 及 Elastic Agents 来采集系统日志及指标。很多开发者可能会有疑问，在我们的实际使用中，我们更多的可能是需要采集定制的应用日志，而不是系统日志。那么在这个时候，我们该如何使用 Elastic Agents 来把这些日志摄入呢？在以前的系统中，我们可以使用如下的几种方式来采集日志：
我们可以直接使用 Beats 把数据传入到 Elasticsearch 中。对数据的处理，我们可以使用 Beats 的 processors 来处理数据，或者通过 Elasticsearch 集群的 ingest nodes 来处理数据。我们可以通过 Beats =&gt; Logstash =&gt; Elasticsearch。针对这种情况，我们可以分别在 Beats，Logstash 或者 Elasticsearch 集群的 ingest nodes 来处理数据。我们可以直接使用各种编程语言来直接向 Elasticsearch 集群进行写入。我们可以使用 Elasticsearch 集群的 ingest nodes 来处理数据。 在今天的文章里，我们来详细地描述如何使用 Elastic Agents 把应用中的定制日志摄入到 Elasticsearch 中并进行分析。在今天的演示中，我将使用如下测试环境：
我将使用 Elastic Stack 8.3 来进行安装并展示。
如何使用 Elastic Agents 把定制的日志摄入到 Elasticsearch 中
准备日志 为了方便，我们使用我之前的一个教程写的文章里的例子来生成日志。我使用 Python 应用来生成日志。请参考文章 “Beats: 使用 Filebeat 进行日志结构化 - Python”。我们按照如下的步骤在 Ubuntu OS 的机器上来运行应用：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/0de0f24751c216921bedab62cfb4eed9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-08-15T11:42:48+08:00" />
<meta property="article:modified_time" content="2022-08-15T11:42:48+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Observability：如何使用 Elastic Agents 把定制的日志摄入到 Elasticsearch 中</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>在我之前的文章 “<a class="link-info" href="https://elasticstack.blog.csdn.net/article/details/122956622" rel="nofollow" title="Observability：使用 Elastic Agent 来摄入日志及指标 - Elastic Stack 8.0">Observability：使用 Elastic Agent 来摄入日志及指标 - Elastic Stack 8.0</a>”，我详细地描述了如何安装 Elasticsearch，Stack 及 Elastic Agents 来采集系统日志及指标。很多开发者可能会有疑问，在我们的实际使用中，我们更多的可能是需要采集定制的应用日志，而不是系统日志。那么在这个时候，我们该如何使用 Elastic Agents 来把这些日志摄入呢？在以前的系统中，我们可以使用如下的几种方式来采集日志：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/a4/e9/CR6TYqrx_o.png"></p> 
<ol><li> 我们可以直接使用 Beats 把数据传入到 Elasticsearch 中。对数据的处理，我们可以使用 Beats 的 processors 来处理数据，或者通过 Elasticsearch 集群的 ingest nodes 来处理数据。</li><li>我们可以通过 Beats =&gt; Logstash =&gt; Elasticsearch。针对这种情况，我们可以分别在 Beats，Logstash 或者 Elasticsearch 集群的 ingest nodes 来处理数据。</li><li>我们可以直接使用各种编程语言来直接向 Elasticsearch 集群进行写入。我们可以使用 Elasticsearch 集群的 ingest nodes 来处理数据。</li></ol> 
<p>在今天的文章里，我们来详细地描述如何使用 Elastic Agents 把应用中的定制日志摄入到 Elasticsearch 中并进行分析。在今天的演示中，我将使用如下测试环境：</p> 
<p class="img-center"><img alt="" height="409" src="https://images2.imgbox.com/07/48/u9OHzJQu_o.png" width="600"></p> 
<p>我将使用 Elastic Stack 8.3 来进行安装并展示。</p> 
<div class="csdn-video-box"> 
 <iframe id="HdnLtANH-1657242607873" frameborder="0" src="https://player.bilibili.com/player.html?aid=215723314" allowfullscreen="true" data-mediaembed="bilibili"></iframe> 
 <p>如何使用 Elastic Agents 把定制的日志摄入到 Elasticsearch 中</p> 
</div> 
<h2>准备日志</h2> 
<p>为了方便，我们使用我之前的一个教程写的文章里的例子来生成日志。我使用 Python 应用来生成日志。请参考文章 “<a class="link-info" href="https://elasticstack.blog.csdn.net/article/details/106688240" rel="nofollow" title="Beats: 使用 Filebeat 进行日志结构化 - Python">Beats: 使用 Filebeat 进行日志结构化 - Python</a>”。我们按照如下的步骤在 Ubuntu OS 的机器上来运行应用：</p> 
<pre><code>liuxg@liuxgu:~/python/logs$ pwd
/home/liuxg/python/logs
liuxg@liuxgu:~/python/logs$ ls
createlogs.py  createlogs_1.py  createlogs_2.py  filebeat_json.yml  json_logs  test.log
liuxg@liuxgu:~/python/logs$ python createlogs_2.py 
liuxg@liuxgu:~/python/logs$ cat json_logs 
{"user_name": "arthur", "id": 42, "verified": false, "event": "logged_in"}
{"user_name": "arthur", "id": 42, "verified": true, "event": "changed_state"}</code></pre> 
<p>可以看出来在我的应用目录里会生成一个叫做 json_logs 的文件。它的内容如上所示。上面的文档路径及文件名将在下面的配置中要用到。我们的日志路径是：</p> 
<pre><code>/home/liuxg/python/logs/json_logs</code></pre> 
<p></p> 
<h2>安装</h2> 
<p> 在进行下面的练习之前，我们必须安装好 Elasticsearch 及 Kibana。我们可以参考之前的文章：</p> 
<ul><li><a href="https://elasticstack.blog.csdn.net/article/details/99413578" rel="nofollow" title="如何在 Linux，MacOS 及 Windows 上进行安装 Elasticsearch">如何在 Linux，MacOS 及 Windows 上进行安装 Elasticsearch</a></li><li> <p id="articleContentId"><a href="https://elasticstack.blog.csdn.net/article/details/122936411" rel="nofollow" title="Elastic：使用 Docker 安装 Elastic Stack 8.0 并开始使用">Elastic：使用 Docker 安装 Elastic Stack 8.0 并开始使用</a></p> </li></ul> 
<p>我们按照上面的要求进行安装 Elasticsearch 及 Kibana。为了能够让 fleet 正常工作，<a href="https://www.elastic.co/guide/en/elasticsearch/reference/8.0/security-settings.html#api-key-service-settings" rel="nofollow" title="内置的 API service">内置的 API service</a> 必须启动。我们必须为 Elasticsearch 的配置文件 config/elasticsearch.yml 文件配置：</p> 
<div> 
 <pre><code>xpack.security.authc.api_key.enabled: true</code></pre> 
</div> 
<p> 配置完后，我们再重新启动 Elasticsearch。针对 Kibana，我们也需要做一个额外的配置。我们需要修改 config/kibana.yml 文件。在这个文件的最后面，添加如下的一行：</p> 
<div> 
 <pre><code>
xpack.encryptedSavedObjects.encryptionKey: 'fhjskloppd678ehkdfdlliverpoolfcr'</code></pre> 
</div> 
<p>如果你不想使用上面的这个设置，你可以使用如下的方式来获得：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/52/db/Kye81b9z_o.png" width="1200"></p> 
<p>从上面的输出中，我们可以看出来，有三个输出的 key。我们可以把这三个同时拷贝，并添加到 config/kibana.yml 文件的后面。当然，我们也可以只拷贝其中的一个也可。我们再重新启动 Kibana。</p> 
<p>这样我们对 Elasticsearch 及 Kibana 的配置就完成。 针对 Elastic Stack 8.0 以前的版本安装，请阅读我之前的文章 “<a href="https://elasticstack.blog.csdn.net/article/details/122678925" rel="nofollow" title="Observability：如何在最新的 Elastic Stack 中使用 Fleet 摄入 system 日志及指标">Observability：如何在最新的 Elastic Stack 中使用 Fleet 摄入 system 日志及指标</a>”。 </p> 
<p>除此之外，Kibana 需要 Internet 连接才能从 Elastic Package Registry 下载集成包。 确保 Kibana 服务器可以连接到<a href="https://epr.elastic.co/" rel="nofollow" title="https://epr.elastic.co">https://epr.elastic.co</a> 的端口 443 上 。如果你的环境有网络流量限制，有一些方法可以解决此要求。 有关详细信息，请参阅<a href="https://www.elastic.co/guide/en/fleet/8.0/air-gapped.html" rel="nofollow" title="气隙环境">气隙环境</a>。</p> 
<p>目前，Fleet 只能被具有 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/8.0/built-in-roles.html" rel="nofollow" title="superuser role">superuser role</a> 的用户所使用。</p> 
<p></p> 
<h2>配置 Fleet</h2> 
<p>使用 Kibana 中的 Fleet 将日志、指标和安全数据导入 Elastic Stack。第一次使用 Fleet 时，你可能需要对其进行设置并添加 Fleet Server。在做配置之前，我们首先来查看一下有没有任何的 integration 被安装：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/fd/d9/QTR2939i_o.png" width="1200"></p> 
<p><img alt="" height="1152" src="https://images2.imgbox.com/79/2c/zqxRXiSa_o.png" width="1200">从上面我们可以看出来没有任何安装的 integrations。</p> 
<p>我们打开 Fleet 页面：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/48/9d/VkKgmbZJ_o.png" width="1200"></p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/d4/78/9raIuOty_o.png" width="1200"></p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/df/e2/clSccNMA_o.png" width="1200"></p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/3c/36/R4GsWMpG_o.png" width="1200"></p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/fd/04/6fWRFT0D_o.png" width="1200">  </p> 
<p>我们接下来添加 Agent：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/3b/6b/BokRpd7O_o.png" width="1200"></p> 
<p> </p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/c4/5f/7uh6AY5F_o.png" width="1200"> 上面显示我们的 Fleet Sever policy 被成功地创建了。我们需要把我们的 Fleet Server 安装到 Ubuntu OS 机器上。 </p> 
<p> <img alt="" height="478" src="https://images2.imgbox.com/61/1b/wKdCsjly_o.png" width="1200"></p> 
<p>我们的目标机器是 Linux OS。我们点击上面的拷贝按钮，并在 Linux OS 上进行安装：</p> 
<pre><code>curl -L -O https://artifacts.elastic.co/downloads/beats/elastic-agent/elastic-agent-8.3.0-linux-x86_64.tar.gz
tar xzvf elastic-agent-8.3.0-linux-x86_64.tar.gz
cd elastic-agent-8.3.0-linux-x86_64
sudo ./elastic-agent install \
  --fleet-server-es=https://192.168.0.3:9200 \
  --fleet-server-service-token=AAEAAWVsYXN0aWMvZmxlZXQtc2VydmVyL3Rva2VuLTE2NTY1Njg5MTE1NTk6clBaX1pidXNTdTZXc2Fvb0ROcXVhUQ \
  --fleet-server-policy=fleet-server-policy \
  --fleet-server-es-ca-trusted-fingerprint=764021beb30446365d829986a362ffba82d03f4ff7861839a60f7951b8e83e7a</code></pre> 
<p>我们按照 Kibana 中的提示来安装：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/a3/31/SrzwFSTi_o.png" width="1200"></p> 
<p> <img alt="" height="1200" src="https://images2.imgbox.com/9c/4b/sHAVnYdM_o.png" width="1200"></p> 
<p>等过一段时间，我们可以看到这个运用于 192.168.0.4 机器上的 Agents 的状态也变为 healthy：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/05/3e/lM6HTdyd_o.png" width="1200"></p> 
<p>由于我们的 Elastic Agent 和 Fleet Server 是在一个服务器上运行的，所以，我们直接在 Fleet Server Policy 里添加我们想要的 integration。如果你的 Elastic Agent 可以运行于另外的一个机器上，而不和 Fleet Server 在同一个机器上，你可以创建一个新的 policy，比如 logs。然后让 agent 赋予给这个 新创建的 policy。</p> 
<p>我们直接在这个 Fleet Server Policy 里添加一个叫做 custom log 的集成：</p> 
<p><img alt="" height="1172" src="https://images2.imgbox.com/8b/2b/QrZsjck7_o.png" width="1200"></p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/20/6e/weGAmtHL_o.png" width="1200"></p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/d6/da/GYwExrcp_o.png" width="1200"></p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/90/73/Lacay9zC_o.png" width="1200"></p> 
<p>在上面，我们把 Ubuntu OS 上的日志的路径添加进去：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/36/88/Qeawr9Od_o.png" width="1200"></p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/e8/d4/7WFYy96Q_o.png" width="1200"></p> 
<p>在上面，我们可以看到新增加的 log-1 集成。 </p> 
<p>如果你之前已经生成过 json_logs 日志文件，我们可以删除当前目录的文件，并再次生成该文件：</p> 
<pre><code>liuxg@liuxgu:~/python/logs$ pwd
/home/liuxg/python/logs
liuxg@liuxgu:~/python/logs$ ls
createlogs.py  createlogs_1.py  createlogs_2.py  filebeat_json.yml  json_logs  test.log
liuxg@liuxgu:~/python/logs$ rm json_logs 
liuxg@liuxgu:~/python/logs$ python createlogs_2.py </code></pre> 
<p>我们或者使用如下的命令来追加一写文档到 json_logs 文件中：</p> 
<pre><code>liuxg@liuxgu:~/python/logs$ pwd
/home/liuxg/python/logs
liuxg@liuxgu:~/python/logs$ cp json_logs temp
liuxg@liuxgu:~/python/logs$ cat temp &gt;&gt; json_logs 
liuxg@liuxgu:~/python/logs$ cat json_logs 
{"user_name": "arthur", "id": 42, "verified": false, "event": "logged_in"}
{"user_name": "arthur", "id": 42, "verified": true, "event": "changed_state"}
{"user_name": "arthur", "id": 42, "verified": false, "event": "logged_in"}
{"user_name": "arthur", "id": 42, "verified": true, "event": "changed_state"}</code></pre> 
<p>在上面，我们追加了两个文档到 json_logs 里去了。</p> 
<p>我们接下来回到 Discover 去查看：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/1e/78/1kdoEHJ3_o.png" width="1200"></p> 
<p> 在搜索框中输入 json_logs，我们发现在过去 15分钟之内有两个新摄入的文档。我们再次查看 message 的内容：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/60/48/0cUWycMA_o.png" width="1200"></p> 
<p>显然，我们可以看到 message 字段显示的就是我们之前在日志中的信息。它是一个 JSON 格式的信息。这个虽然好，但是它不是结构化的日志信息。我们想要的是 user_name 为文档的一个字段，id 为另外一个字段这样的结构化信息。在之前的 Filebeat 中，我们可以轻松地使用 Filebeat 所提供的 processors 或者就像如同在文章 “<a class="link-info" href="https://elasticstack.blog.csdn.net/article/details/106688240" rel="nofollow" title="Beats: 使用 Filebeat 进行日志结构化 - Python">Beats: 使用 Filebeat 进行日志结构化 - Python</a>” 使用的那样。我们可以使用 Filebeat input type 所提供的固有功能来完成。再者，我们还可以使用 Elasticsearch 集群的 ingest node 来完成。</p> 
<p>那么针对我们目前的 Elastic Agent 摄入方式，我们该如何结构化这个 message 信息呢？答案是使用 ingest pipeline。</p> 
<p>我们在 Kibana 的 Dev Tools 中创建如下的 ingest pipeline：</p> 
<pre><code>POST _ingest/pipeline/_simulate
{
  "pipeline": {
    "description": "structure a JSON format message",
    "processors": [
      {
        "json": {
          "field": "message",
          "target_field": "json_fields"
        }
      }
    ]
  },
  "docs": [
    {
      "_source": {
        "message": "{\"user_name\": \"arthur\", \"id\": 42, \"verified\": false, \"event\": \"logged_in\"}"
      }
    }
  ]
}</code></pre> 
<p>在上面，我们通过 _simulate 来测试我们的 pipeline：</p> 
<pre><code>{
  "docs": [
    {
      "doc": {
        "_index": "_index",
        "_id": "_id",
        "_source": {
          "json_fields": {
            "verified": false,
            "id": 42,
            "event": "logged_in",
            "user_name": "arthur"
          },
          "message": """{"user_name": "arthur", "id": 42, "verified": false, "event": "logged_in"}"""
        },
        "_ingest": {
          "timestamp": "2022-07-01T00:03:30.040008Z"
        }
      }
    }
  ]
}</code></pre> 
<p>如上所示，我们的 josn processor 能够非常出色地完成 message 的结构化，并把结构化的信息保存于一个叫做 json_fields 的字段中。在完成上面的模拟后，我们使用如下的命令来创建一个 pipeline：</p> 
<pre><code>PUT _ingest/pipeline/message_structure
{
  "description": "structure a JSON format message",
  "processors": [
    {
      "json": {
        "field": "message",
        "target_field": "json_fields"
      }
    }
  ]
}</code></pre> 
<p>在上面，我们创建了一个叫做 message_structure 的 ingest pipeline。</p> 
<p>我们接下来展示如何在 custom logs 里来使用这个 ingest pipeline。我们重新打开 log-1 集成：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/50/56/vhLL83rI_o.png" width="1200"></p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/7c/79/hU4gTIqI_o.png" width="1200"></p> 
<p>在上面，选定默认的 generic 为 dataset 的名称，那么最终形成的索引名称将会是 logs-generic-*。我们也可以设定自己想要的 dataset，比如 my-app。如上所示，我们在 Custom congfiguration 里填写 pipeline 的定义。点击上面的 Save integration： </p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/c1/d0/I938mLgt_o.png" width="1200"></p> 
<p><img alt="" height="1142" src="https://images2.imgbox.com/42/68/TmyjPmEs_o.png" width="1200"></p> 
<p>上面显示，我们的更新完毕。</p> 
<p>我们接下来再次删除在日志目录下的 json_logs 文件，并再次运行 python 应用：</p> 
<pre><code>liuxg@liuxgu:~/python/logs$ pwd
/home/liuxg/python/logs
liuxg@liuxgu:~/python/logs$ ls
createlogs.py  createlogs_1.py  createlogs_2.py  filebeat_json.yml  json_logs  test.log
liuxg@liuxgu:~/python/logs$ rm json_logs 
liuxg@liuxgu:~/python/logs$ python createlogs_2.py </code></pre> 
<p> 我们再次回到 Discover 界面来进行查看：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/4f/b9/4bqHvpQ6_o.png" width="1200"></p> 
<p>我们可以看到最新的日志信息被收集起来了。我们展开该文档进行查看：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/4a/e5/JgVTXug7_o.png" width="1200"></p> 
<p>我们可以看到 message 的信息被结构化了，并且保存于一个叫做 json_fields 的字段中。 </p> 
<p>好了，今天我的分享就写到这里。希望对大家从 Beats 转换到 Elastic Agents 的使用提供一个平滑的过度。在未来，Elastic 更推崇 Elastic Agents 的使用虽然之前的 Beats 方式还可以继续使用。使用 Elastic Agents 可以使我们的 Agents 更容易集中管理。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d024445b7a8936620bd268b6d1faf20e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【MXNet报错】OSError: libnccl.so.2: cannot open shared object file: No such file or directory</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d153ff03aa184098b8001da2a6a68f2e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">EasyExcel单字段自定义转换@ExcelProperty::converter无效</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>