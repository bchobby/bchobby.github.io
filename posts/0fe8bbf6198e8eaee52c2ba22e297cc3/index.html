<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Keras学习笔记3——keras.layers - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Keras学习笔记3——keras.layers" />
<meta property="og:description" content="目录 0. 函数1. 全连接层2. 激活层3. Dropout层4. Flatten层5. Reshape层6. 卷积层Conv2DLocallyConnected2D 7. 池化层8. 循环层RNNSimpleRNNGRULSTMConvLSTM2DSimpleRNNCellGRUCellLSTMCellCuDNNGRUCuDNNLSTM 9. 嵌入层10. 融合层 MergeAddSubtractMultiplyAverageMaximumConcatenateDotaddsubtractmultiplyaveragemaximumconcatenatedot 11. 高级激活层 Advanced ActivationsReLUSoftmaxThresholdedReLULeakyReLUPReLUELU 12. 标准化层 NormalizationBatchNormalization 13. 噪声层 NoiseGaussianNoiseGaussianDropoutAlphaDropout 14. 层封装器 wrappersTimeDistributedBidirectional 15. 其他InputPermuteRepeatVectorLambdaActivityRegularizationMaskingSpatialDropout1DSpatialDropout2DSpatialDropout3D 参考:
Keras入门简介
&gt;&gt;&gt; import keras &gt;&gt;&gt; dir(keras.layers) [&#39;Activation&#39;, &#39;ActivityRegularization&#39;, &#39;Add&#39;, &#39;AlphaDropout&#39;, &#39;AtrousConvolution1D&#39;, &#39;AtrousConvolution2D&#39;, &#39;Average&#39;, &#39;AveragePooling1D&#39;, &#39;AveragePooling2D&#39;, &#39;AveragePooling3D&#39;, &#39;AvgPool1D&#39;, &#39;AvgPool2D&#39;, &#39;AvgPool3D&#39;, &#39;BatchNormalization&#39;, &#39;Bidirectional&#39;, &#39;Concatenate&#39;, &#39;Conv1D&#39;, &#39;Conv2D&#39;, &#39;Conv2DTranspose&#39;, &#39;Conv3D&#39;, &#39;Conv3DTranspose&#39;, &#39;ConvLSTM2D&#39;, &#39;ConvLSTM2DCell&#39;, &#39;ConvRecurrent2D&#39;, &#39;Convolution1D&#39;, &#39;Convolution2D&#39;, &#39;Convolution3D&#39;, &#39;Cropping1D&#39;, &#39;Cropping2D&#39;, &#39;Cropping3D&#39;, &#39;CuDNNGRU&#39;, &#39;CuDNNLSTM&#39;, &#39;Deconvolution2D&#39;, &#39;Deconvolution3D&#39;, &#39;Dense&#39;, &#39;DepthwiseConv2D&#39;, &#39;Dot&#39;, &#39;Dropout&#39;, &#39;ELU&#39;, &#39;Embedding&#39;, &#39;Flatten&#39;, &#39;GRU&#39;, &#39;GRUCell&#39;, &#39;GaussianDropout&#39;, &#39;GaussianNoise&#39;, &#39;GlobalAveragePooling1D&#39;, &#39;GlobalAveragePooling2D&#39;, &#39;GlobalAveragePooling3D&#39;, &#39;GlobalAvgPool1D&#39;, &#39;GlobalAvgPool2D&#39;, &#39;GlobalAvgPool3D&#39;, &#39;GlobalMaxPool1D&#39;, &#39;GlobalMaxPool2D&#39;, &#39;GlobalMaxPool3D&#39;, &#39;GlobalMaxPooling1D&#39;, &#39;GlobalMaxPooling2D&#39;, &#39;GlobalMaxPooling3D&#39;, &#39;Highway&#39;, &#39;Input&#39;, &#39;InputLayer&#39;, &#39;InputSpec&#39;, &#39;LSTM&#39;, &#39;LSTMCell&#39;, &#39;Lambda&#39;, &#39;Layer&#39;, &#39;LeakyReLU&#39;, &#39;LocallyConnected1D&#39;, &#39;LocallyConnected2D&#39;, &#39;Masking&#39;, &#39;MaxPool1D&#39;, &#39;MaxPool2D&#39;, &#39;MaxPool3D&#39;, &#39;MaxPooling1D&#39;, &#39;MaxPooling2D&#39;, &#39;MaxPooling3D&#39;, &#39;Maximum&#39;, &#39;MaxoutDense&#39;, &#39;Minimum&#39;, &#39;Multiply&#39;, &#39;PReLU&#39;, &#39;Permute&#39;, &#39;RNN&#39;, &#39;ReLU&#39;, &#39;Recurrent&#39;, &#39;RepeatVector&#39;, &#39;Reshape&#39;, &#39;SeparableConv1D&#39;, &#39;SeparableConv2D&#39;, &#39;SimpleRNN&#39;, &#39;SimpleRNNCell&#39;, &#39;Softmax&#39;, &#39;SpatialDropout1D&#39;, &#39;SpatialDropout2D&#39;, &#39;SpatialDropout3D&#39;, &#39;StackedRNNCells&#39;, &#39;Subtract&#39;, &#39;ThresholdedReLU&#39;, &#39;TimeDistributed&#39;, &#39;UpSampling1D&#39;, &#39;UpSampling2D&#39;, &#39;UpSampling3D&#39;, &#39;ZeroPadding1D&#39;, &#39;ZeroPadding2D&#39;, &#39;ZeroPadding3D&#39;, &#39;__builtins__&#39;, &#39;__cached__&#39;, &#39;__doc__&#39;, &#39;__file__&#39;, &#39;__loader__&#39;, &#39;__name__&#39;, &#39;__package__&#39;, &#39;__path__&#39;, &#39;__spec__&#39;, &#39;absolute_import&#39;, &#39;add&#39;, &#39;advanced_activations&#39;, &#39;average&#39;, &#39;concatenate&#39;, &#39;convolutional&#39;, &#39;convolutional_recurrent&#39;, &#39;core&#39;, &#39;cudnn_recurrent&#39;, &#39;deserialize&#39;, &#39;deserialize_keras_object&#39;, &#39;dot&#39;, &#39;embeddings&#39;, &#39;local&#39;, &#39;maximum&#39;, &#39;merge&#39;, &#39;minimum&#39;, &#39;multiply&#39;, &#39;noise&#39;, &#39;normalization&#39;, &#39;pooling&#39;, &#39;recurrent&#39;, &#39;serialize&#39;, &#39;subtract&#39;, &#39;wrappers&#39;] 0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/0fe8bbf6198e8eaee52c2ba22e297cc3/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-09-15T23:56:16+08:00" />
<meta property="article:modified_time" content="2020-09-15T23:56:16+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Keras学习笔记3——keras.layers</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#0__9" rel="nofollow">0. 函数</a></li><li><a href="#1__38" rel="nofollow">1. 全连接层</a></li><li><a href="#2__70" rel="nofollow">2. 激活层</a></li><li><a href="#3_Dropout_84" rel="nofollow">3. Dropout层</a></li><li><a href="#4_Flatten_95" rel="nofollow">4. Flatten层</a></li><li><a href="#5_Reshape_112" rel="nofollow">5. Reshape层</a></li><li><a href="#6__144" rel="nofollow">6. 卷积层</a></li><li><ul><li><a href="#Conv2D_147" rel="nofollow">Conv2D</a></li><li><a href="#LocallyConnected2D_205" rel="nofollow">LocallyConnected2D</a></li></ul> 
  </li><li><a href="#7__209" rel="nofollow">7. 池化层</a></li><li><a href="#8__243" rel="nofollow">8. 循环层</a></li><li><ul><li><a href="#RNN_246" rel="nofollow">RNN</a></li><li><a href="#SimpleRNN_342" rel="nofollow">SimpleRNN</a></li><li><a href="#GRU_348" rel="nofollow">GRU</a></li><li><a href="#LSTM_353" rel="nofollow">LSTM</a></li><li><a href="#ConvLSTM2D_397" rel="nofollow">ConvLSTM2D</a></li><li><a href="#SimpleRNNCell_402" rel="nofollow">SimpleRNNCell</a></li><li><a href="#GRUCell_408" rel="nofollow">GRUCell</a></li><li><a href="#LSTMCell_414" rel="nofollow">LSTMCell</a></li><li><a href="#CuDNNGRU_420" rel="nofollow">CuDNNGRU</a></li><li><a href="#CuDNNLSTM_427" rel="nofollow">CuDNNLSTM</a></li></ul> 
  </li><li><a href="#9__433" rel="nofollow">9. 嵌入层</a></li><li><a href="#10__Merge_473" rel="nofollow">10. 融合层 Merge</a></li><li><ul><li><a href="#Add_474" rel="nofollow">Add</a></li><li><a href="#Subtract_496" rel="nofollow">Subtract</a></li><li><a href="#Multiply_521" rel="nofollow">Multiply</a></li><li><a href="#Average_530" rel="nofollow">Average</a></li><li><a href="#Maximum_540" rel="nofollow">Maximum</a></li><li><a href="#Concatenate_549" rel="nofollow">Concatenate</a></li><li><a href="#Dot_563" rel="nofollow">Dot</a></li><li><a href="#add_578" rel="nofollow">add</a></li><li><a href="#subtract_607" rel="nofollow">subtract</a></li><li><a href="#multiply_635" rel="nofollow">multiply</a></li><li><a href="#average_650" rel="nofollow">average</a></li><li><a href="#maximum_665" rel="nofollow">maximum</a></li><li><a href="#concatenate_680" rel="nofollow">concatenate</a></li><li><a href="#dot_695" rel="nofollow">dot</a></li></ul> 
  </li><li><a href="#11__Advanced_Activations_712" rel="nofollow">11. 高级激活层 Advanced Activations</a></li><li><ul><li><a href="#ReLU_714" rel="nofollow">ReLU</a></li><li><a href="#Softmax_736" rel="nofollow">Softmax</a></li><li><a href="#ThresholdedReLU_750" rel="nofollow">ThresholdedReLU</a></li><li><a href="#LeakyReLU_767" rel="nofollow">LeakyReLU</a></li><li><a href="#PReLU_785" rel="nofollow">PReLU</a></li><li><a href="#ELU_805" rel="nofollow">ELU</a></li></ul> 
  </li><li><a href="#12__Normalization_827" rel="nofollow">12. 标准化层 Normalization</a></li><li><ul><li><a href="#BatchNormalization_828" rel="nofollow">BatchNormalization</a></li></ul> 
  </li><li><a href="#13__Noise_861" rel="nofollow">13. 噪声层 Noise</a></li><li><ul><li><a href="#GaussianNoise_863" rel="nofollow">GaussianNoise</a></li><li><a href="#GaussianDropout_881" rel="nofollow">GaussianDropout</a></li><li><a href="#AlphaDropout_893" rel="nofollow">AlphaDropout</a></li></ul> 
  </li><li><a href="#14__wrappers_907" rel="nofollow">14. 层封装器 wrappers</a></li><li><ul><li><a href="#TimeDistributed_908" rel="nofollow">TimeDistributed</a></li><li><a href="#Bidirectional_944" rel="nofollow">Bidirectional</a></li></ul> 
  </li><li><a href="#15__964" rel="nofollow">15. 其他</a></li><li><ul><li><a href="#Input_965" rel="nofollow">Input</a></li><li><a href="#Permute_995" rel="nofollow">Permute</a></li><li><a href="#RepeatVector_1014" rel="nofollow">RepeatVector</a></li><li><a href="#Lambda_1038" rel="nofollow">Lambda</a></li><li><a href="#ActivityRegularization_1077" rel="nofollow">ActivityRegularization</a></li><li><a href="#Masking_1093" rel="nofollow">Masking</a></li><li><a href="#SpatialDropout1D_1116" rel="nofollow">SpatialDropout1D</a></li><li><a href="#SpatialDropout2D_1135" rel="nofollow">SpatialDropout2D</a></li><li><a href="#SpatialDropout3D_1156" rel="nofollow">SpatialDropout3D</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<p>参考:<br> <a href="https://blog.csdn.net/vivian_ll/article/details/80795139">Keras入门简介</a></p> 
<pre><code class="prism language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">import</span> keras
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token builtin">dir</span><span class="token punctuation">(</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token string">'Activation'</span><span class="token punctuation">,</span> <span class="token string">'ActivityRegularization'</span><span class="token punctuation">,</span> <span class="token string">'Add'</span><span class="token punctuation">,</span> <span class="token string">'AlphaDropout'</span><span class="token punctuation">,</span> <span class="token string">'AtrousConvolution1D'</span><span class="token punctuation">,</span> <span class="token string">'AtrousConvolution2D'</span><span class="token punctuation">,</span> <span class="token string">'Average'</span><span class="token punctuation">,</span> <span class="token string">'AveragePooling1D'</span><span class="token punctuation">,</span> <span class="token string">'AveragePooling2D'</span><span class="token punctuation">,</span> <span class="token string">'AveragePooling3D'</span><span class="token punctuation">,</span> <span class="token string">'AvgPool1D'</span><span class="token punctuation">,</span> <span class="token string">'AvgPool2D'</span><span class="token punctuation">,</span> <span class="token string">'AvgPool3D'</span><span class="token punctuation">,</span> <span class="token string">'BatchNormalization'</span><span class="token punctuation">,</span> <span class="token string">'Bidirectional'</span><span class="token punctuation">,</span> <span class="token string">'Concatenate'</span><span class="token punctuation">,</span> <span class="token string">'Conv1D'</span><span class="token punctuation">,</span> <span class="token string">'Conv2D'</span><span class="token punctuation">,</span> <span class="token string">'Conv2DTranspose'</span><span class="token punctuation">,</span> <span class="token string">'Conv3D'</span><span class="token punctuation">,</span> <span class="token string">'Conv3DTranspose'</span><span class="token punctuation">,</span> <span class="token string">'ConvLSTM2D'</span><span class="token punctuation">,</span> <span class="token string">'ConvLSTM2DCell'</span><span class="token punctuation">,</span> <span class="token string">'ConvRecurrent2D'</span><span class="token punctuation">,</span> <span class="token string">'Convolution1D'</span><span class="token punctuation">,</span> <span class="token string">'Convolution2D'</span><span class="token punctuation">,</span> <span class="token string">'Convolution3D'</span><span class="token punctuation">,</span> <span class="token string">'Cropping1D'</span><span class="token punctuation">,</span> <span class="token string">'Cropping2D'</span><span class="token punctuation">,</span> <span class="token string">'Cropping3D'</span><span class="token punctuation">,</span> <span class="token string">'CuDNNGRU'</span><span class="token punctuation">,</span> <span class="token string">'CuDNNLSTM'</span><span class="token punctuation">,</span> <span class="token string">'Deconvolution2D'</span><span class="token punctuation">,</span> <span class="token string">'Deconvolution3D'</span><span class="token punctuation">,</span> <span class="token string">'Dense'</span><span class="token punctuation">,</span> <span class="token string">'DepthwiseConv2D'</span><span class="token punctuation">,</span> <span class="token string">'Dot'</span><span class="token punctuation">,</span> <span class="token string">'Dropout'</span><span class="token punctuation">,</span> <span class="token string">'ELU'</span><span class="token punctuation">,</span> <span class="token string">'Embedding'</span><span class="token punctuation">,</span> <span class="token string">'Flatten'</span><span class="token punctuation">,</span> <span class="token string">'GRU'</span><span class="token punctuation">,</span> <span class="token string">'GRUCell'</span><span class="token punctuation">,</span> <span class="token string">'GaussianDropout'</span><span class="token punctuation">,</span> <span class="token string">'GaussianNoise'</span><span class="token punctuation">,</span> <span class="token string">'GlobalAveragePooling1D'</span><span class="token punctuation">,</span> <span class="token string">'GlobalAveragePooling2D'</span><span class="token punctuation">,</span> <span class="token string">'GlobalAveragePooling3D'</span><span class="token punctuation">,</span> <span class="token string">'GlobalAvgPool1D'</span><span class="token punctuation">,</span> <span class="token string">'GlobalAvgPool2D'</span><span class="token punctuation">,</span> <span class="token string">'GlobalAvgPool3D'</span><span class="token punctuation">,</span> <span class="token string">'GlobalMaxPool1D'</span><span class="token punctuation">,</span> <span class="token string">'GlobalMaxPool2D'</span><span class="token punctuation">,</span> <span class="token string">'GlobalMaxPool3D'</span><span class="token punctuation">,</span> <span class="token string">'GlobalMaxPooling1D'</span><span class="token punctuation">,</span> <span class="token string">'GlobalMaxPooling2D'</span><span class="token punctuation">,</span> <span class="token string">'GlobalMaxPooling3D'</span><span class="token punctuation">,</span> <span class="token string">'Highway'</span><span class="token punctuation">,</span> <span class="token string">'Input'</span><span class="token punctuation">,</span> <span class="token string">'InputLayer'</span><span class="token punctuation">,</span> <span class="token string">'InputSpec'</span><span class="token punctuation">,</span> <span class="token string">'LSTM'</span><span class="token punctuation">,</span> <span class="token string">'LSTMCell'</span><span class="token punctuation">,</span> <span class="token string">'Lambda'</span><span class="token punctuation">,</span> <span class="token string">'Layer'</span><span class="token punctuation">,</span> <span class="token string">'LeakyReLU'</span><span class="token punctuation">,</span> <span class="token string">'LocallyConnected1D'</span><span class="token punctuation">,</span> <span class="token string">'LocallyConnected2D'</span><span class="token punctuation">,</span> <span class="token string">'Masking'</span><span class="token punctuation">,</span> <span class="token string">'MaxPool1D'</span><span class="token punctuation">,</span> <span class="token string">'MaxPool2D'</span><span class="token punctuation">,</span> <span class="token string">'MaxPool3D'</span><span class="token punctuation">,</span> <span class="token string">'MaxPooling1D'</span><span class="token punctuation">,</span> <span class="token string">'MaxPooling2D'</span><span class="token punctuation">,</span> <span class="token string">'MaxPooling3D'</span><span class="token punctuation">,</span> <span class="token string">'Maximum'</span><span class="token punctuation">,</span> <span class="token string">'MaxoutDense'</span><span class="token punctuation">,</span> <span class="token string">'Minimum'</span><span class="token punctuation">,</span> <span class="token string">'Multiply'</span><span class="token punctuation">,</span> <span class="token string">'PReLU'</span><span class="token punctuation">,</span> <span class="token string">'Permute'</span><span class="token punctuation">,</span> <span class="token string">'RNN'</span><span class="token punctuation">,</span> <span class="token string">'ReLU'</span><span class="token punctuation">,</span> <span class="token string">'Recurrent'</span><span class="token punctuation">,</span> <span class="token string">'RepeatVector'</span><span class="token punctuation">,</span> <span class="token string">'Reshape'</span><span class="token punctuation">,</span> <span class="token string">'SeparableConv1D'</span><span class="token punctuation">,</span> <span class="token string">'SeparableConv2D'</span><span class="token punctuation">,</span> <span class="token string">'SimpleRNN'</span><span class="token punctuation">,</span> <span class="token string">'SimpleRNNCell'</span><span class="token punctuation">,</span> <span class="token string">'Softmax'</span><span class="token punctuation">,</span> <span class="token string">'SpatialDropout1D'</span><span class="token punctuation">,</span> <span class="token string">'SpatialDropout2D'</span><span class="token punctuation">,</span> <span class="token string">'SpatialDropout3D'</span><span class="token punctuation">,</span> <span class="token string">'StackedRNNCells'</span><span class="token punctuation">,</span> <span class="token string">'Subtract'</span><span class="token punctuation">,</span> <span class="token string">'ThresholdedReLU'</span><span class="token punctuation">,</span> <span class="token string">'TimeDistributed'</span><span class="token punctuation">,</span> <span class="token string">'UpSampling1D'</span><span class="token punctuation">,</span> <span class="token string">'UpSampling2D'</span><span class="token punctuation">,</span> <span class="token string">'UpSampling3D'</span><span class="token punctuation">,</span> <span class="token string">'ZeroPadding1D'</span><span class="token punctuation">,</span> <span class="token string">'ZeroPadding2D'</span><span class="token punctuation">,</span> <span class="token string">'ZeroPadding3D'</span><span class="token punctuation">,</span> <span class="token string">'__builtins__'</span><span class="token punctuation">,</span> <span class="token string">'__cached__'</span><span class="token punctuation">,</span> <span class="token string">'__doc__'</span><span class="token punctuation">,</span> <span class="token string">'__file__'</span><span class="token punctuation">,</span> <span class="token string">'__loader__'</span><span class="token punctuation">,</span> <span class="token string">'__name__'</span><span class="token punctuation">,</span> <span class="token string">'__package__'</span><span class="token punctuation">,</span> <span class="token string">'__path__'</span><span class="token punctuation">,</span> <span class="token string">'__spec__'</span><span class="token punctuation">,</span> <span class="token string">'absolute_import'</span><span class="token punctuation">,</span> <span class="token string">'add'</span><span class="token punctuation">,</span> <span class="token string">'advanced_activations'</span><span class="token punctuation">,</span> <span class="token string">'average'</span><span class="token punctuation">,</span> <span class="token string">'concatenate'</span><span class="token punctuation">,</span> <span class="token string">'convolutional'</span><span class="token punctuation">,</span> <span class="token string">'convolutional_recurrent'</span><span class="token punctuation">,</span> <span class="token string">'core'</span><span class="token punctuation">,</span> <span class="token string">'cudnn_recurrent'</span><span class="token punctuation">,</span> <span class="token string">'deserialize'</span><span class="token punctuation">,</span> <span class="token string">'deserialize_keras_object'</span><span class="token punctuation">,</span> <span class="token string">'dot'</span><span class="token punctuation">,</span> <span class="token string">'embeddings'</span><span class="token punctuation">,</span> <span class="token string">'local'</span><span class="token punctuation">,</span> <span class="token string">'maximum'</span><span class="token punctuation">,</span> <span class="token string">'merge'</span><span class="token punctuation">,</span> <span class="token string">'minimum'</span><span class="token punctuation">,</span> <span class="token string">'multiply'</span><span class="token punctuation">,</span> <span class="token string">'noise'</span><span class="token punctuation">,</span> <span class="token string">'normalization'</span><span class="token punctuation">,</span> <span class="token string">'pooling'</span><span class="token punctuation">,</span> <span class="token string">'recurrent'</span><span class="token punctuation">,</span> <span class="token string">'serialize'</span><span class="token punctuation">,</span> <span class="token string">'subtract'</span><span class="token punctuation">,</span> <span class="token string">'wrappers'</span><span class="token punctuation">]</span>
</code></pre> 
<h2><a id="0__9"></a>0. 函数</h2> 
<p>layer.get_weights(): 以含有Numpy矩阵的列表形式返回层的权重。<br> layer.set_weights(weights): 从含有Numpy矩阵的列表中设置层的权重（与get_weights的输出形状相同）。<br> layer.get_config(): 返回包含层配置的字典。此图层可以通过以下方式重置：</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> keras <span class="token keyword">import</span> layers
layer <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span>
config <span class="token operator">=</span> layer<span class="token punctuation">.</span>get_config<span class="token punctuation">(</span><span class="token punctuation">)</span>
reconstructed_layer <span class="token operator">=</span> Dense<span class="token punctuation">.</span>from_config<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
</code></pre> 
<p>或:</p> 
<pre><code class="prism language-python">config <span class="token operator">=</span> layer<span class="token punctuation">.</span>get_config<span class="token punctuation">(</span><span class="token punctuation">)</span>
layer <span class="token operator">=</span> layers<span class="token punctuation">.</span>deserialize<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">'class_name'</span><span class="token punctuation">:</span> layer<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__<span class="token punctuation">,</span>
                            <span class="token string">'config'</span><span class="token punctuation">:</span> config<span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> 
<p>如果一个层具有单个节点 (i.e. 如果它不是共享层), 你可以得到它的输入张量、输出张量、输入尺寸和输出尺寸:<br> layer.input<br> layer.output<br> layer.input_shape<br> layer.output_shape</p> 
<p>如果层有多个节点，可以使用以下函数:<br> layer.get_input_at(node_index)<br> layer.get_output_at(node_index)<br> layer.get_input_shape_at(node_index)<br> layer.get_output_shape_at(node_index)</p> 
<h2><a id="1__38"></a>1. 全连接层</h2> 
<p><strong>output = activation( dot(input, kernel) + bias )</strong></p> 
<p>其中，</p> 
<ul><li>activation 是按逐个元素计算的激活函数，</li><li>kernel 是由网络层创建的权重矩阵，</li><li>bias 是其创建的偏置向量 (只在 use_bias 为 True 时才有用)。</li></ul> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span>units<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> use_bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> 
kernel_initializer<span class="token operator">=</span><span class="token string">'glorot_uniform'</span><span class="token punctuation">,</span> bias_initializer<span class="token operator">=</span><span class="token string">'zeros'</span><span class="token punctuation">,</span> 
kernel_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> bias_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> 
activity_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> kernel_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> bias_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>参数说明：</strong></p> 
<ul><li>units: 正整数，全连接层输出的维度，即下一层神经元的个数。</li><li>activation：激活函数，若不指定，则不使用激活函数 (即，「线性」激活: a(x) = x)。</li><li>use_bias: 布尔值，该层是否使用bias偏置向量。</li><li>kernel_initializer: kernel 权值矩阵的初始化器。</li><li>bias_initializer: 偏置向量的初始化器 。</li><li>kernel_regularizer: 运用到 kernel 权值矩阵的正则化函数。</li><li>bias_regularizer: 运用到偏置向的的正则化函数。</li><li>activity_regularizer: 运用到层的输出的正则化函数 (它的 “activation”)。</li><li>kernel_constraint: 运用到 kernel 权值矩阵的约束函数。</li><li>bias_constraint: 运用到偏置向量的约束函数。</li></ul> 
<p><strong>输入尺寸:</strong><br> nD 张量，尺寸: (batch_size, …, input_dim)。 最常见的情况是一个尺寸为 (batch_size, input_dim) 的 2D 输入。</p> 
<p><strong>输出尺寸:</strong><br> nD 张量，尺寸: (batch_size, …, units)。 例如，对于尺寸为 (batch_size, input_dim) 的 2D 输入， 输出的尺寸为 (batch_size, units)。</p> 
<h2><a id="2__70"></a>2. 激活层</h2> 
<p>对上一层的输出应用激活函数。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Activation<span class="token punctuation">(</span>activation<span class="token punctuation">)</span>
</code></pre> 
<p><strong>参数说明：</strong><br> activation：想要使用的激活函数，如：’relu’、’tanh’、‘sigmoid’等。</p> 
<p><strong>输入尺寸:</strong><br> 任意尺寸。 当使用此层作为模型中的第一层时， 使用参数 input_shape （整数元组，不包括样本数的轴）。</p> 
<p><strong>输出尺寸:</strong><br> 与输入相同。</p> 
<h2><a id="3_Dropout_84"></a>3. Dropout层</h2> 
<p>Dropout 应用于输入。Dropout 包括在训练中每次更新时， 对输入单元随机选取一定比例的失活，不更新，但是权重仍然保留，这有助于防止过拟合。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>rate<span class="token punctuation">,</span> noise_shape<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> seed<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>参数说明:</strong></p> 
<ul><li>rate：在 0 和 1 之间的浮点数，需要丢弃的输入比例。</li><li>noise_shape: 1D 整数张量， 表示将与输入相乘的二进制 dropout 掩层的形状。 例如，如果你的输入尺寸为 (batch_size, timesteps, features)，然后 你希望 dropout 掩层在所有时间步都是一样的， 你可以使用 noise_shape=(batch_size, 1, features)。</li><li>seed: 一个作为随机种子的 Python 整数。</li></ul> 
<h2><a id="4_Flatten_95"></a>4. Flatten层</h2> 
<p>将一个维度大于或等于3的高维矩阵，“压扁”为一个二维矩阵。即保留第一个维度（如：batch的个数），然后将剩下维度的值相乘作为“压扁”矩阵的第二个维度。将输入展平。不影响批量大小。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span>data_format<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>参数：</strong></p> 
<ul><li>data_format：一个字符串，其值为 channels_last（默认值）或者 channels_first。它表明输入的维度的顺序。此参数的目的是当模型从一种数据格式切换到另一种数据格式时保留权重顺序。channels_last 对应着尺寸为 (batch, …, channels) 的输入，而 channels_first 对应着尺寸为 (batch, channels, …) 的输入。默认为 image_data_format 的值，你可以在 Keras 的配置文件 ~/.keras/keras.json 中找到它。如果你从未设置过它，那么它将是 channels_last。</li></ul> 
<p><strong>例：</strong></p> 
<pre><code class="prism language-python">model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 现在：model.output_shape == (None, 64, 32, 32)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> input_shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 现在：model.output_shape == (None, 65536)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<h2><a id="5_Reshape_112"></a>5. Reshape层</h2> 
<p>将输入重新调整为特定的尺寸。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Reshape<span class="token punctuation">(</span>target_shape<span class="token punctuation">)</span>
</code></pre> 
<p><strong>参数说明：</strong><br> target_shape：目标尺寸。整数元组。 不包含表示批量的轴。</p> 
<p><strong>输入尺寸：</strong><br> 任意，尽管输入尺寸中的所有维度必须是固定的。 <strong>当使用此层作为模型中的第一层时， 使用参数 input_shape （整数元组，不包括样本数的轴）。</strong></p> 
<p><strong>输出尺寸：</strong><br> (batch_size,) + target_shape</p> 
<p><strong>例：</strong></p> 
<pre><code class="prism language-python"><span class="token comment"># 作为 Sequential 模型的第一层</span>
model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> input_shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 现在：model.output_shape == (None, 3, 4)</span>
<span class="token comment"># 注意： `None` 是批表示的维度</span>

<span class="token comment"># 作为 Sequential 模型的中间层</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 现在： model.output_shape == (None, 6, 2)</span>

<span class="token comment"># 还支持使用 `-1` 表示维度的尺寸推断</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 现在： model.output_shape == (None, 3, 2, 2)</span>
</code></pre> 
<h2><a id="6__144"></a>6. 卷积层</h2> 
<p><a href="https://keras.io/zh/layers/convolutional/" rel="nofollow">来源</a><br> 卷积操作分为一维、二维、三维，分别为Conv1D、Conv2D、Conv3D。一维卷积主要应用于以时间序列数据或文本数据，二维卷积通常应用于图像数据。由于这三种的使用和参数都基本相同，所以主要以处理图像数据的Conv2D进行说明。</p> 
<h3><a id="Conv2D_147"></a>Conv2D</h3> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Conv2D<span class="token punctuation">(</span>filters<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'valid'</span><span class="token punctuation">,</span> 
data_format<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dilation_rate<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> use_bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> 
kernel_initializer<span class="token operator">=</span><span class="token string">'glorot_uniform'</span><span class="token punctuation">,</span> bias_initializer<span class="token operator">=</span><span class="token string">'zeros'</span><span class="token punctuation">,</span> 
kernel_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> bias_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> activity_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> 
kernel_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> bias_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>参数说明：</strong></p> 
<ul><li>filters: 整数，输出空间的维度 （即卷积中滤波器的输出数量）。</li><li>kernel_size: 一个整数，或者 2 个整数表示的元组或列表， 指明 2D 卷积窗口的宽度和高度。 可以是一个整数，为所有空间维度指定相同的值。</li><li>strides: 一个整数，或者 2 个整数表示的元组或列表， 指明卷积沿宽度和高度方向的步长。 可以是一个整数，为所有空间维度指定相同的值。 指定任何 stride 值 != 1 与指定 dilation_rate 值 != 1 两者不兼容。</li><li>padding: “valid” 或 “same” (大小写敏感)。补“0”策略，’valid‘指卷积后的大小与原来的大小可以不同，’same‘则卷积后大小与原来大小一致。<code>"valid" 表示「不填充」。 "same" 表示填充输入以使输出具有与原始输入相同的长度。</code> “causal” 表示因果（膨胀）卷积， 例如，output[t] 不依赖于 input[t+1:]， 在模型不应违反时间顺序的时间数据建模时非常有用。</li><li>data_format: 字符串， channels_last (默认) 或 channels_first 之一，表示输入中维度的顺序。 <strong>channels_last</strong> 对应输入尺寸为 (batch, height, width, <strong>channels</strong>)， <strong>channels_first</strong> 对应输入尺寸为 (batch, <strong>channels</strong>, height, width)。 它默认为从 Keras 配置文件 ~/.keras/keras.json 中 找到的 image_data_format 值。 如果你从未设置它，将使用 channels_last。</li><li>dilation_rate: 一个整数或 2 个整数的元组或列表， 指定膨胀卷积的膨胀率。 可以是一个整数，为所有空间维度指定相同的值。 当前，指定任何 dilation_rate 值 != 1 与 指定 stride 值 != 1 两者不兼容。</li><li>activation: 要使用的激活函数。 如果你不指定，则不使用激活函数 (即线性激活： a(x) = x)。</li><li>use_bias: 布尔值，该层是否使用偏置向量。</li><li>kernel_initializer: kernel 权值矩阵的初始化器。</li><li>bias_initializer: 偏置向量的初始化器 。</li><li>kernel_regularizer: 运用到 kernel 权值矩阵的正则化函数。</li><li>bias_regularizer: 运用到偏置向量的正则化函数。</li><li>activity_regularizer: 运用到层输出（它的激活值）的正则化函数。</li><li>kernel_constraint: 运用到 kernel 权值矩阵的约束函数。</li><li>bias_constraint: 运用到偏置向量的约束函数。</li></ul> 
<table><thead><tr><th>data_format</th><th>输入尺寸</th><th>输出尺寸</th></tr></thead><tbody><tr><td>‘channels_first’</td><td>(samples, channels, rows, cols)</td><td>(samples, filters, new_rows, new_cols)</td></tr><tr><td>‘channels_last’</td><td>(samples, rows, cols, channels)</td><td>(samples, new_rows, new_cols, filters)</td></tr></tbody></table> 
<p>由于填充的原因， rows 和 cols 值可能已更改。</p> 
<p><strong>卷积核的运算过程</strong><br> 例如输入224x224x3（rgb三通道，3张feature map），输出是32位深度（filters=32），卷积核尺寸为5x5（kernel_size=(5, 5)）。</p> 
<pre><code class="prism language-python">model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'same'</span><span class="token punctuation">,</span> input_shape<span class="token operator">=</span>x_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>那么我们需要32个卷积核，每一个的尺寸为5x5x3（最后的3就是原图的rgb位深3），每一个卷积核的每一层是5x5（共3层）分别与原图的每层224x224卷积，然后<strong>将得到的三张新图叠加（算术求和）</strong>，变成一张新的feature map。 每一个卷积核都这样操作，就可以得到32张新的feature map了。 也就是说：<strong>不管输入图像的深度为多少，经过一个卷积核（filter），最后都变成一个深度为1的特征图</strong>。不同的filter可以卷积得到不同的特征，也就是得到不同的feature map。</p> 
<table><thead><tr><th>函数</th><th>卷积层</th></tr></thead><tbody><tr><td>Conv1D</td><td>1D 卷积层 (例如时序卷积)</td></tr><tr><td>Conv2D</td><td>2D 卷积层 (例如对图像的空间卷积)</td></tr><tr><td>SeparableConv1D</td><td>深度方向的可分离 1D 卷积</td></tr><tr><td>SeparableConv2D</td><td>深度方向的可分离 2D 卷积</td></tr><tr><td>DepthwiseConv2D</td><td>深度可分离 2D 卷积</td></tr><tr><td>Conv2DTranspose</td><td>转置卷积层 (有时被成为反卷积)</td></tr><tr><td>Conv3D</td><td>3D 卷积层 (例如立体空间卷积)</td></tr><tr><td>Conv3DTranspose</td><td>转置卷积层 (有时被成为反卷积)</td></tr><tr><td>Cropping1D</td><td>1D 输入的裁剪层（例如时间序列）</td></tr><tr><td>Cropping2D</td><td>2D 输入的裁剪层（例如图像）</td></tr><tr><td>Cropping3D</td><td>3D 数据的裁剪层（例如空间或时空）</td></tr><tr><td>UpSampling1D</td><td>1D 输入的上采样层</td></tr><tr><td>UpSampling2D</td><td>2D 输入的上采样层</td></tr><tr><td>UpSampling3D</td><td>3D 输入的上采样层</td></tr><tr><td>ZeroPadding1D</td><td>1D 输入的零填充层（例如，时间序列）</td></tr><tr><td>ZeroPadding2D</td><td>2D 输入的零填充层（例如图像）</td></tr><tr><td>ZeroPadding3D</td><td>3D 数据的零填充层（空间或时空）</td></tr></tbody></table> 
<h3><a id="LocallyConnected2D_205"></a>LocallyConnected2D</h3> 
<p>2D 输入的局部连接层。<br> LocallyConnected2D 层与 Conv2D 层的工作方式相同，<strong>除了权值不共享外</strong>， 也就是说，<strong>在输入的每个不同部分应用不同的一组过滤器</strong>。</p> 
<h2><a id="7__209"></a>7. 池化层</h2> 
<p><a href="https://keras.io/zh/layers/pooling/" rel="nofollow">来源</a><br> 与卷积层一样，最大统计量池化和平均统计量池化也有三种，分别为MaxPooling1D、MaxPooling2D、MaxPooling3D和AveragePooling1D、AveragePooling2D、AveragePooling3D，由于使用和参数基本相同，所以主要以MaxPooling2D进行说明。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>MaxPooling2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> 
padding<span class="token operator">=</span><span class="token string">'valid'</span><span class="token punctuation">,</span> data_format<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>参数说明：</strong></p> 
<ul><li>pool_size: 整数，或者 2 个整数表示的元组， 沿（垂直，水平）方向缩小比例的因数。 （2，2）会把输入张量的两个维度都缩小一半。 如果只使用一个整数，那么两个维度都会使用同样的窗口长度。</li><li>strides: 整数，2 个整数表示的元组，或者是 None。 表示步长值。 如果是 None，那么默认值是 pool_size。</li><li>padding: “valid” 或者 “same” （区分大小写）。</li><li>data_format: 字符串，channels_last (默认)或 channels_first 之一。 表示输入各维度的顺序。 channels_last 代表尺寸是 (batch, height, width, channels) 的输入张量， 而 channels_first 代表尺寸是 (batch, channels, height, width) 的输入张量。 默认值根据 Keras 配置文件 ~/.keras/keras.json 中的 image_data_format 值来设置。 如果还没有设置过，那么默认值就是 “channels_last”。</li></ul> 
<table><thead><tr><th>data_format</th><th>输入尺寸</th><th>输出尺寸</th></tr></thead><tbody><tr><td>‘channels_first’</td><td>(batch_size, channels, rows, cols)</td><td>(batch_size, channels, pooled_rows, pooled_cols)</td></tr><tr><td>‘channels_last’</td><td>(batch_size, rows, cols, channels)</td><td>(batch_size, pooled_rows, pooled_cols, channels)</td></tr></tbody></table> 
<table><thead><tr><th>函数</th><th>池化层</th></tr></thead><tbody><tr><td>MaxPooling1D</td><td>对于时序数据的最大池化</td></tr><tr><td>MaxPooling2D</td><td>对于空间数据的最大池化</td></tr><tr><td>MaxPooling3D</td><td>对于 3D（空间，或时空间）数据的最大池化</td></tr><tr><td>AveragePooling1D</td><td>对于时序数据的平均池化</td></tr><tr><td>AveragePooling2D</td><td>对于空间数据的平均池化</td></tr><tr><td>AveragePooling3D</td><td>对于3D（空间，或时空间）数据的平均池化</td></tr><tr><td>GlobalMaxPooling1D</td><td>对于时序数据的全局最大池化</td></tr><tr><td>GlobalMaxPooling2D</td><td>对于空间数据的全局最大池化</td></tr><tr><td>GlobalMaxPooling3D</td><td>对于3D数据的全局最大池化</td></tr><tr><td>GlobalAveragePooling1D</td><td>对于时序数据的全局平均池化</td></tr><tr><td>GlobalAveragePooling2D</td><td>对于空间数据的全局平均池化</td></tr><tr><td>GlobalAveragePooling3D</td><td>对于3D数据的全局平均池化</td></tr></tbody></table> 
<h2><a id="8__243"></a>8. 循环层</h2> 
<p><a href="https://keras.io/zh/layers/recurrent/" rel="nofollow">来源</a><br> 循环神经网络中的RNN、LSTM和GRU都继承本层，所以该父类的参数同样使用于对应的子类SimpleRNN、LSTM和GRU。</p> 
<h3><a id="RNN_246"></a>RNN</h3> 
<p>循环神经网络层基类。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>RNN<span class="token punctuation">(</span>cell<span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> return_state<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> 
go_backwards<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> stateful<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> unroll<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>参数：</strong></p> 
<ul><li> <p>cell: 一个 RNN 单元实例。RNN 单元是一个具有以下几项的类：</p> 
  <ul><li>一个 call(input_at_t, states_at_t) 方法， 它返回 (output_at_t, states_at_t_plus_1)。 单元的调用方法也可以采引入可选参数 constants， 详见下面的小节「关于给 RNN 传递外部常量的说明」。</li><li>一个 state_size 属性。这可以是单个整数（单个状态）， 在这种情况下，它是循环层状态的大小（应该与单元输出的大小相同）。 这也可以是整数表示的列表/元组（每个状态一个大小）。</li><li>一个 output_size 属性。 这可以是单个整数或者是一个 TensorShape， 它表示输出的尺寸。出于向后兼容的原因，如果此属性对于当前单元不可用， 则该值将由 state_size 的第一个元素推断。<br> cell 也可能是 RNN 单元实例的列表，在这种情况下，RNN 的单元将堆叠在另一个单元上，实现高效的堆叠 RNN。</li></ul> </li><li> <p>return_sequences：控制返回的类型，“False”返回输出序列的最后一个输出，“True”则返回整个序列。当我们要搭建多层神经网络（如深层LSTM）时，若不是最后一层，则需要将该参数设为True。</p> </li><li> <p>return_sequences: 布尔值。是返回输出序列中的最后一个输出，还是全部序列。</p> </li><li> <p>return_state: 布尔值。除了输出之外是否返回最后一个状态。</p> </li><li> <p>go_backwards: 布尔值 (默认 False)。 如果为 True，则向后处理输入序列并返回相反的序列。</p> </li><li> <p>stateful: 布尔值 (默认 False)。 如果为 True，则批次中索引 i 处的每个样品的最后状态将用作下一批次中索引 i 样品的初始状态。</p> </li><li> <p>unroll: 布尔值 (默认 False)。 如果为 True，则网络将展开，否则将使用符号循环。 展开可以加速 RNN，但它往往会占用更多的内存。 展开只适用于短序列。</p> </li><li> <p>input_dim: 输入的维度（整数）。 将此层用作模型中的第一层时，此参数（或者，关键字参数 input_shape）是必需的。</p> </li><li> <p>input_length: 输入序列的长度，在恒定时指定。 如果你要在上游连接 Flatten 和 Dense 层， 则需要此参数（如果没有它，无法计算全连接输出的尺寸）。 请注意，如果循环神经网络层不是模型中的第一层， 则需要在第一层的层级指定输入长度（例如，通过 input_shape 参数）。</p> </li></ul> 
<p><strong>输入尺寸</strong><br> 3D 张量，尺寸为 (batch_size, timesteps, input_dim)。</p> 
<p><strong>输出尺寸</strong><br> 如果 return_state：返回张量列表。 第一个张量为输出。剩余的张量为最后的状态， 每个张量的尺寸为 (batch_size, units)。<br> 如果 return_sequences：返回 3D 张量， 尺寸为 (batch_size, timesteps, units)。<br> 否则，返回尺寸为 (batch_size, units) 的 2D 张量。</p> 
<p><strong>Masking</strong><br> 该层支持以可变数量的时间步对输入数据进行 masking。 要将 masking 引入数据，需使用 Embedding 层， 并将 mask_zero 参数设置为 True。</p> 
<p><strong>关于在 RNN 中使用「状态（statefulness）」的说明</strong><br> 将 RNN 层设置为 stateful（有状态的）， 这意味着针对一个批次的样本计算的状态将被重新用作下一批样本的初始状态。 这假定在不同连续批次的样品之间有一对一的映射。</p> 
<p>为了使状态有效：</p> 
<ul><li>在层构造器中指定 stateful=True。</li><li>为你的模型指定一个固定的批次大小， 如果是顺序模型，为你的模型的第一层传递一个 batch_input_shape=(…) 参数。</li><li>为你的模型指定一个固定的批次大小， 如果是顺序模型，为你的模型的第一层传递一个 batch_input_shape=(…)。 如果是带有 1 个或多个 Input 层的函数式模型，为你的模型的所有第一层传递一个 batch_shape=(…)。 这是你的输入的预期尺寸，包括批量维度。 它应该是整数的元组，例如 (32, 10, 100)。</li><li>在调用 fit() 是指定 shuffle=False。</li></ul> 
<p>要重置模型的状态，请在特定图层或整个模型上调用 .reset_states()。</p> 
<p><strong>关于指定 RNN 初始状态的说明</strong><br> 可以通过使用关键字参数 initial_state 调用它们来符号化地指定 RNN 层的初始状态。 initial_state 的值应该是表示 RNN 层初始状态的张量或张量列表。</p> 
<p>可以通过调用带有关键字参数 states 的 reset_states 方法来数字化地指定 RNN 层的初始状态。 states 的值应该是一个代表 RNN 层初始状态的 Numpy 数组或者 Numpy 数组列表。</p> 
<p><strong>关于给 RNN 传递外部常量的说明</strong><br> 可以使用 RNN.<strong>call</strong>（以及 RNN.call）的 constants 关键字参数将「外部」常量传递给单元。 这要求 cell.call 方法接受相同的关键字参数 constants。 这些常数可用于调节附加静态输入（不随时间变化）上的单元转换，也可用于注意力机制。</p> 
<p><strong>例子：</strong></p> 
<pre><code class="prism language-python"><span class="token comment"># 首先，让我们定义一个 RNN 单元，作为网络层子类。</span>

<span class="token keyword">class</span> <span class="token class-name">MinimalRNNCell</span><span class="token punctuation">(</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Layer<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> units<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>units <span class="token operator">=</span> units
        self<span class="token punctuation">.</span>state_size <span class="token operator">=</span> units
        <span class="token builtin">super</span><span class="token punctuation">(</span>MinimalRNNCell<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">build</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>kernel <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>input_shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>units<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                      initializer<span class="token operator">=</span><span class="token string">'uniform'</span><span class="token punctuation">,</span>
                                      name<span class="token operator">=</span><span class="token string">'kernel'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>recurrent_kernel <span class="token operator">=</span> self<span class="token punctuation">.</span>add_weight<span class="token punctuation">(</span>
            shape<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>units<span class="token punctuation">,</span> self<span class="token punctuation">.</span>units<span class="token punctuation">)</span><span class="token punctuation">,</span>
            initializer<span class="token operator">=</span><span class="token string">'uniform'</span><span class="token punctuation">,</span>
            name<span class="token operator">=</span><span class="token string">'recurrent_kernel'</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>built <span class="token operator">=</span> <span class="token boolean">True</span>

    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> states<span class="token punctuation">)</span><span class="token punctuation">:</span>
        prev_output <span class="token operator">=</span> states<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        h <span class="token operator">=</span> K<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>kernel<span class="token punctuation">)</span>
        output <span class="token operator">=</span> h <span class="token operator">+</span> K<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>prev_output<span class="token punctuation">,</span> self<span class="token punctuation">.</span>recurrent_kernel<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output<span class="token punctuation">,</span> <span class="token punctuation">[</span>output<span class="token punctuation">]</span>

<span class="token comment"># 让我们在 RNN 层使用这个单元：</span>

cell <span class="token operator">=</span> MinimalRNNCell<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span>
x <span class="token operator">=</span> keras<span class="token punctuation">.</span>Input<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
layer <span class="token operator">=</span> RNN<span class="token punctuation">(</span>cell<span class="token punctuation">)</span>
y <span class="token operator">=</span> layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

<span class="token comment"># 以下是如何使用单元格构建堆叠的 RNN的方法：</span>

cells <span class="token operator">=</span> <span class="token punctuation">[</span>MinimalRNNCell<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span> MinimalRNNCell<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
x <span class="token operator">=</span> keras<span class="token punctuation">.</span>Input<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
layer <span class="token operator">=</span> RNN<span class="token punctuation">(</span>cells<span class="token punctuation">)</span>
y <span class="token operator">=</span> layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

</code></pre> 
<h3><a id="SimpleRNN_342"></a>SimpleRNN</h3> 
<p>全连接的 RNN，其输出将被反馈到输入。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>SimpleRNN<span class="token punctuation">(</span>units<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'tanh'</span><span class="token punctuation">,</span> use_bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> kernel_initializer<span class="token operator">=</span><span class="token string">'glorot_uniform'</span><span class="token punctuation">,</span> recurrent_initializer<span class="token operator">=</span><span class="token string">'orthogonal'</span><span class="token punctuation">,</span> bias_initializer<span class="token operator">=</span><span class="token string">'zeros'</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> recurrent_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> bias_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> activity_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> kernel_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> recurrent_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> bias_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> recurrent_dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> return_state<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> go_backwards<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> stateful<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> unroll<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="GRU_348"></a>GRU</h3> 
<p>门限循环单元网络（Gated Recurrent Unit） - Cho et al. 2014.</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>units<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'tanh'</span><span class="token punctuation">,</span> recurrent_activation<span class="token operator">=</span><span class="token string">'hard_sigmoid'</span><span class="token punctuation">,</span> use_bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> kernel_initializer<span class="token operator">=</span><span class="token string">'glorot_uniform'</span><span class="token punctuation">,</span> recurrent_initializer<span class="token operator">=</span><span class="token string">'orthogonal'</span><span class="token punctuation">,</span> bias_initializer<span class="token operator">=</span><span class="token string">'zeros'</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> recurrent_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> bias_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> activity_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> kernel_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> recurrent_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> bias_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> recurrent_dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> implementation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> return_state<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> go_backwards<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> stateful<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> unroll<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> reset_after<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="LSTM_353"></a>LSTM</h3> 
<p>长短期记忆网络层（Long Short-Term Memory） - Hochreiter 1997.</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>units<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'tanh'</span><span class="token punctuation">,</span> recurrent_activation<span class="token operator">=</span><span class="token string">'hard_sigmoid'</span><span class="token punctuation">,</span> use_bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> kernel_initializer<span class="token operator">=</span><span class="token string">'glorot_uniform'</span><span class="token punctuation">,</span> recurrent_initializer<span class="token operator">=</span><span class="token string">'orthogonal'</span><span class="token punctuation">,</span> bias_initializer<span class="token operator">=</span><span class="token string">'zeros'</span><span class="token punctuation">,</span> unit_forget_bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> recurrent_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> bias_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> activity_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> kernel_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> recurrent_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> bias_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> recurrent_dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> implementation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> return_state<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> go_backwards<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> stateful<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> unroll<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>参数：</strong></p> 
<ul><li>units: 正整数，输出空间的维度。</li><li>activation: 要使用的激活函数。 如果传入 None，则不使用激活函数 (即 线性激活：a(x) = x)。</li><li>recurrent_activation: 用于循环时间步的激活函数。 默认：分段线性近似 sigmoid (hard_sigmoid)。 如果传入 None，则不使用激活函数 (即 线性激活：a(x) = x)。</li><li>use_bias: 布尔值，该层是否使用偏置向量。</li><li>kernel_initializer: kernel 权值矩阵的初始化器， 用于输入的线性转换。</li><li>recurrent_initializer: recurrent_kernel 权值矩阵 的初始化器，用于循环层状态的线性转换。</li><li>bias_initializer:偏置向量的初始化器.</li><li>unit_forget_bias: 布尔值。 如果为 True，初始化时，将忘记门的偏置加 1。 将其设置为 True 同时还会强制bias_initializer=“zeros”。 这个建议来自 Jozefowicz et al.。</li><li>kernel_regularizer: 运用到 kernel 权值矩阵的正则化函数。</li><li>recurrent_regularizer: 运用到 recurrent_kernel 权值矩阵的正则化函数。</li><li>bias_regularizer: 运用到偏置向量的正则化函数。</li><li>activity_regularizer: 运用到层输出的正则化函数。</li><li>kernel_constraint: 运用到 kernel 权值矩阵的约束函数。</li><li>recurrent_constraint: 运用到 recurrent_kernel 权值矩阵的约束函数。</li><li>bias_constraint: 运用到偏置向量的约束函数。</li><li>dropout: 在 0 和 1 之间的浮点数。 单元的丢弃比例，用于输入的线性转换。</li><li>recurrent_dropout: 在 0 和 1 之间的浮点数。 单元的丢弃比例，用于循环层状态的线性转换。</li><li>implementation: 实现模式，1 或 2。 模式 1 将把它的操作结构化为更多的小的点积和加法操作， 而模式 2 将把它们分批到更少，更大的操作中。 这些模式在不同的硬件和不同的应用中具有不同的性能配置文件。</li><li>return_sequences: 布尔值。是返回输出序列中的最后一个输出，还是全部序列。</li><li>return_state: 布尔值。除了输出之外是否返回最后一个状态。</li><li>go_backwards: 布尔值 (默认 False)。 如果为 True，则向后处理输入序列并返回相反的序列。</li><li>stateful: 布尔值 (默认 False)。 如果为 True，则批次中索引 i 处的每个样品的最后状态 将用作下一批次中索引 i 样品的初始状态。</li><li>unroll: 布尔值 (默认 False)。 如果为 True，则网络将展开，否则将使用符号循环。 展开可以加速 RNN，但它往往会占用更多的内存。 展开只适用于短序列。</li></ul> 
<p><strong>输入尺寸</strong><br> 如果 data_format=‘channels_first’， 输入 5D 张量，尺寸为： (samples,time, channels, rows, cols)。<br> 如果 data_format=‘channels_last’， 输入 5D 张量，尺寸为： (samples,time, rows, cols, channels)。</p> 
<p><strong>输出尺寸</strong><br> 如果 return_sequences，<br> 如果 data_format=‘channels_first’，返回 5D 张量，尺寸为：(samples, <strong>time</strong>, filters, output_row, output_col)。<br> 如果 data_format=‘channels_last’，返回 5D 张量，尺寸为：(samples, <strong>time</strong>, output_row, output_col, filters)。<br> 否则，<br> 如果 data_format =‘channels_first’，返回 4D 张量，尺寸为：(samples, filters, output_row, output_col)。<br> 如果 data_format=‘channels_last’，返回 4D 张量，尺寸为：(samples, output_row, output_col, filters)。<br> o_row 和 o_col 取决于 filter 和 padding 的尺寸。</p> 
<h3><a id="ConvLSTM2D_397"></a>ConvLSTM2D</h3> 
<p>卷积 LSTM。它类似于 LSTM 层，但输入变换和循环变换都是卷积的。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>ConvLSTM2D<span class="token punctuation">(</span>filters<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'valid'</span><span class="token punctuation">,</span> data_format<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dilation_rate<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'tanh'</span><span class="token punctuation">,</span> recurrent_activation<span class="token operator">=</span><span class="token string">'hard_sigmoid'</span><span class="token punctuation">,</span> use_bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> kernel_initializer<span class="token operator">=</span><span class="token string">'glorot_uniform'</span><span class="token punctuation">,</span> recurrent_initializer<span class="token operator">=</span><span class="token string">'orthogonal'</span><span class="token punctuation">,</span> bias_initializer<span class="token operator">=</span><span class="token string">'zeros'</span><span class="token punctuation">,</span> unit_forget_bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> recurrent_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> bias_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> activity_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> kernel_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> recurrent_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> bias_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> go_backwards<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> stateful<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> recurrent_dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="SimpleRNNCell_402"></a>SimpleRNNCell</h3> 
<p>SimpleRNN 的单元类。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>SimpleRNNCell<span class="token punctuation">(</span>units<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'tanh'</span><span class="token punctuation">,</span> use_bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> kernel_initializer<span class="token operator">=</span><span class="token string">'glorot_uniform'</span><span class="token punctuation">,</span> recurrent_initializer<span class="token operator">=</span><span class="token string">'orthogonal'</span><span class="token punctuation">,</span> bias_initializer<span class="token operator">=</span><span class="token string">'zeros'</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> recurrent_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> bias_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> kernel_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> recurrent_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> bias_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> recurrent_dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="GRUCell_408"></a>GRUCell</h3> 
<p>GRU 层的单元类。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>GRUCell<span class="token punctuation">(</span>units<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'tanh'</span><span class="token punctuation">,</span> recurrent_activation<span class="token operator">=</span><span class="token string">'hard_sigmoid'</span><span class="token punctuation">,</span> use_bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> kernel_initializer<span class="token operator">=</span><span class="token string">'glorot_uniform'</span><span class="token punctuation">,</span> recurrent_initializer<span class="token operator">=</span><span class="token string">'orthogonal'</span><span class="token punctuation">,</span> bias_initializer<span class="token operator">=</span><span class="token string">'zeros'</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> recurrent_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> bias_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> kernel_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> recurrent_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> bias_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> recurrent_dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> implementation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> reset_after<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="LSTMCell_414"></a>LSTMCell</h3> 
<p>LSTM 层的单元类。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LSTMCell<span class="token punctuation">(</span>units<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'tanh'</span><span class="token punctuation">,</span> recurrent_activation<span class="token operator">=</span><span class="token string">'hard_sigmoid'</span><span class="token punctuation">,</span> use_bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> kernel_initializer<span class="token operator">=</span><span class="token string">'glorot_uniform'</span><span class="token punctuation">,</span> recurrent_initializer<span class="token operator">=</span><span class="token string">'orthogonal'</span><span class="token punctuation">,</span> bias_initializer<span class="token operator">=</span><span class="token string">'zeros'</span><span class="token punctuation">,</span> unit_forget_bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> recurrent_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> bias_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> kernel_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> recurrent_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> bias_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> recurrent_dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> implementation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="CuDNNGRU_420"></a>CuDNNGRU</h3> 
<p>由 CuDNN 支持的快速 GRU 实现。只能以 TensorFlow 后端运行在 GPU 上。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>CuDNNGRU<span class="token punctuation">(</span>units<span class="token punctuation">,</span> kernel_initializer<span class="token operator">=</span><span class="token string">'glorot_uniform'</span><span class="token punctuation">,</span> recurrent_initializer<span class="token operator">=</span><span class="token string">'orthogonal'</span><span class="token punctuation">,</span> bias_initializer<span class="token operator">=</span><span class="token string">'zeros'</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> recurrent_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> bias_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> activity_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> kernel_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> recurrent_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> bias_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> return_state<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> stateful<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="CuDNNLSTM_427"></a>CuDNNLSTM</h3> 
<p>由 CuDNN 支持的快速 LSTM 实现。只能以 TensorFlow 后端运行在 GPU 上。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>CuDNNLSTM<span class="token punctuation">(</span>units<span class="token punctuation">,</span> kernel_initializer<span class="token operator">=</span><span class="token string">'glorot_uniform'</span><span class="token punctuation">,</span> recurrent_initializer<span class="token operator">=</span><span class="token string">'orthogonal'</span><span class="token punctuation">,</span> bias_initializer<span class="token operator">=</span><span class="token string">'zeros'</span><span class="token punctuation">,</span> unit_forget_bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> recurrent_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> bias_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> activity_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> kernel_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> recurrent_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> bias_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> return_state<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> stateful<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<h2><a id="9__433"></a>9. 嵌入层</h2> 
<p>该层只能用在模型的第一层，是将所有索引标号的稀疏矩阵映射到致密的低维矩阵。如我们对文本数据进行处理时，我们对每个词编号后，我们希望将词编号变成词向量就可以使用嵌入层。将正整数（索引值）转换为固定尺寸的稠密向量。 例如： [[4], [20]] -&gt; [[0.25, 0.1], [0.6, -0.2]]</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">,</span> embeddings_initializer<span class="token operator">=</span><span class="token string">'uniform'</span><span class="token punctuation">,</span> embeddings_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> activity_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> embeddings_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> mask_zero<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> input_length<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>参数说明：</strong></p> 
<ul><li>input_dim: int &gt; 0。词汇表大小， 即，最大整数 index + 1。</li><li>output_dim: int &gt;= 0。词向量的维度。</li><li>embeddings_initializer: embeddings 矩阵的初始化方法 (详见 initializers)。</li><li>embeddings_regularizer: embeddings matrix 的正则化方法 (详见 regularizer)。</li><li>embeddings_constraint: embeddings matrix 的约束函数 (详见 constraints)。</li><li>mask_zero: 是否把 0 看作为一个应该被遮蔽的特殊的 “padding” 值。 这对于可变长的 循环神经网络层 十分有用。 如果设定为 True，那么接下来的所有层都必须支持 masking，否则就会抛出异常。 如果 mask_zero 为 True，作为结果，索引 0 就不能被用于词汇表中 （input_dim 应该与 vocabulary + 1 大小相同）。</li><li>input_length: 输入序列的长度，当它是固定的时。 如果你需要连接 Flatten 和 Dense 层，则这个参数是必须的 （没有它，dense 层的输出尺寸就无法计算）。</li></ul> 
<p><strong>输入尺寸</strong><br> 尺寸为 (batch_size, sequence_length) 的 2D 张量。</p> 
<p><strong>输出尺寸</strong><br> 尺寸为 (batch_size, sequence_length, output_dim) 的 3D 张量。</p> 
<p>该层可能有点费解，举个例子，当我们有一个文本，该文本有100句话，我们已经通过一系列操作，使得文本变成一个(100,32)矩阵，每行代表一句话，每个元素代表一个词，我们希望将该词变为64维的词向量：<br> Embedding(100, 64, input_length=32)<br> 则输出的矩阵的shape变为(100, 32, 64)：即每个词已经变成一个64维的词向量。<br> 可以发现Keras在搭建模型比起Tensorflow等简单太多了，如Tensorflow需要定义每一层的权重矩阵，输入用占位符等，这些在Keras中都不需要，我们只要在第一层定义输入维度，其他层定义输出维度就可以搭建起模型，通俗易懂，方便高效，这是Keras的一个显著的优势。</p> 
<pre><code class="prism language-python">model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Embedding<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> input_length<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 模型将输入一个大小为 (batch, input_length) 的整数矩阵。</span>
<span class="token comment"># 输入中最大的整数（即词索引）不应该大于 999 （词汇表大小）</span>
<span class="token comment"># 现在 model.output_shape == (None, 10, 64)，其中 None 是 batch 的维度。</span>

input_array <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span><span class="token string">'rmsprop'</span><span class="token punctuation">,</span> <span class="token string">'mse'</span><span class="token punctuation">)</span>
output_array <span class="token operator">=</span> model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>input_array<span class="token punctuation">)</span>
<span class="token keyword">assert</span> output_array<span class="token punctuation">.</span>shape <span class="token operator">==</span> <span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span>
</code></pre> 
<h2><a id="10__Merge_473"></a>10. 融合层 Merge</h2> 
<h3><a id="Add_474"></a>Add</h3> 
<p>计算输入张量列表的和。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Add<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>它接受一个张量的列表， 所有的张量必须有相同的输入尺寸， 然后返回一个张量（和输入张量尺寸相同）。</p> 
<p><strong>例子</strong></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> keras

input1 <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
x1 <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>input1<span class="token punctuation">)</span>
input2 <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
x2 <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>input2<span class="token punctuation">)</span>
<span class="token comment"># 相当于 added = keras.layers.add([x1, x2])</span>
added <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Add<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">[</span>x1<span class="token punctuation">,</span> x2<span class="token punctuation">]</span><span class="token punctuation">)</span>  

out <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">(</span>added<span class="token punctuation">)</span>
model <span class="token operator">=</span> keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Model<span class="token punctuation">(</span>inputs<span class="token operator">=</span><span class="token punctuation">[</span>input1<span class="token punctuation">,</span> input2<span class="token punctuation">]</span><span class="token punctuation">,</span> outputs<span class="token operator">=</span>out<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="Subtract_496"></a>Subtract</h3> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Subtract<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>计算两个输入张量的差。</p> 
<p>它接受一个长度为 2 的张量列表， 两个张量必须有相同的尺寸，然后返回一个值为 (inputs[0] - inputs[1]) 的张量， 输出张量和输入张量尺寸相同。</p> 
<p><strong>例子</strong></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> keras

input1 <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
x1 <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>input1<span class="token punctuation">)</span>
input2 <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
x2 <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>input2<span class="token punctuation">)</span>
<span class="token comment"># 相当于 subtracted = keras.layers.subtract([x1, x2])</span>
subtracted <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Subtract<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">[</span>x1<span class="token punctuation">,</span> x2<span class="token punctuation">]</span><span class="token punctuation">)</span>

out <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">(</span>subtracted<span class="token punctuation">)</span>
model <span class="token operator">=</span> keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Model<span class="token punctuation">(</span>inputs<span class="token operator">=</span><span class="token punctuation">[</span>input1<span class="token punctuation">,</span> input2<span class="token punctuation">]</span><span class="token punctuation">,</span> outputs<span class="token operator">=</span>out<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="Multiply_521"></a>Multiply</h3> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Multiply<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>计算输入张量列表的（逐元素间的）乘积。<br> 它接受一个张量的列表， 所有的张量必须有相同的输入尺寸， 然后返回一个张量（和输入张量尺寸相同）。</p> 
<h3><a id="Average_530"></a>Average</h3> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Average<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>计算输入张量列表的平均值。<br> 它接受一个张量的列表， 所有的张量必须有相同的输入尺寸， 然后返回一个张量（和输入张量尺寸相同）。</p> 
<h3><a id="Maximum_540"></a>Maximum</h3> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Maximum<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>计算输入张量列表的（逐元素间的）最大值。<br> 它接受一个张量的列表， 所有的张量必须有相同的输入尺寸， 然后返回一个张量（和输入张量尺寸相同）。</p> 
<h3><a id="Concatenate_549"></a>Concatenate</h3> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Concatenate<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<p>连接一个输入张量的列表。</p> 
<p>它接受一个张量的列表， 除了连接轴之外，其他的尺寸都必须相同， 然后返回一个由所有输入张量连接起来的输出张量。</p> 
<p><strong>参数</strong><br> axis: 连接的轴。<br> **kwargs: 层关键字参数。</p> 
<h3><a id="Dot_563"></a>Dot</h3> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dot<span class="token punctuation">(</span>axes<span class="token punctuation">,</span> normalize<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<p>计算两个张量之间样本的点积。</p> 
<p>例如，如果作用于输入尺寸为 (batch_size, n) 的两个张量 a 和 b， 那么输出结果就会是尺寸为 (batch_size, 1) 的一个张量。 在这个张量中，每一个条目 i 是 a[i] 和 b[i] 之间的点积。</p> 
<p><strong>参数</strong><br> axes: 整数或者整数元组， 一个或者几个进行点积的轴。<br> normalize: 是否在点积之前对即将进行点积的轴进行 L2 标准化。 如果设置成 True，那么输出两个样本之间的余弦相似值。<br> **kwargs: 层关键字参数。</p> 
<h3><a id="add_578"></a>add</h3> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>add<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
</code></pre> 
<p>Add 层的函数式接口。</p> 
<p><strong>参数</strong><br> inputs: 一个输入张量的列表（列表大小至少为 2）。<br> **kwargs: 层关键字参数。</p> 
<p><strong>返回</strong><br> 一个张量，所有输入张量的和。</p> 
<p><strong>例子</strong></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> keras

input1 <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
x1 <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>input1<span class="token punctuation">)</span>
input2 <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
x2 <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>input2<span class="token punctuation">)</span>
added <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token punctuation">[</span>x1<span class="token punctuation">,</span> x2<span class="token punctuation">]</span><span class="token punctuation">)</span>

out <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">(</span>added<span class="token punctuation">)</span>
model <span class="token operator">=</span> keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Model<span class="token punctuation">(</span>inputs<span class="token operator">=</span><span class="token punctuation">[</span>input1<span class="token punctuation">,</span> input2<span class="token punctuation">]</span><span class="token punctuation">,</span> outputs<span class="token operator">=</span>out<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="subtract_607"></a>subtract</h3> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>subtract<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
</code></pre> 
<p>Subtract 层的函数式接口。</p> 
<p><strong>参数</strong><br> inputs: 一个列表的输入张量（列表大小准确为 2）。<br> **kwargs: 层的关键字参数。</p> 
<p><strong>返回</strong><br> 一个张量，两个输入张量的差。</p> 
<p><strong>例子</strong></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> keras

input1 <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
x1 <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>input1<span class="token punctuation">)</span>
input2 <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
x2 <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>input2<span class="token punctuation">)</span>
subtracted <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>subtract<span class="token punctuation">(</span><span class="token punctuation">[</span>x1<span class="token punctuation">,</span> x2<span class="token punctuation">]</span><span class="token punctuation">)</span>

out <span class="token operator">=</span> keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">(</span>subtracted<span class="token punctuation">)</span>
model <span class="token operator">=</span> keras<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Model<span class="token punctuation">(</span>inputs<span class="token operator">=</span><span class="token punctuation">[</span>input1<span class="token punctuation">,</span> input2<span class="token punctuation">]</span><span class="token punctuation">,</span> outputs<span class="token operator">=</span>out<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="multiply_635"></a>multiply</h3> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
</code></pre> 
<p>Multiply 层的函数式接口。</p> 
<p><strong>参数</strong><br> inputs: 一个列表的输入张量（列表大小至少为 2）。<br> **kwargs: 层的关键字参数。</p> 
<p><strong>返回</strong><br> 一个张量，所有输入张量的逐元素乘积。</p> 
<h3><a id="average_650"></a>average</h3> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>average<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
</code></pre> 
<p>Average 层的函数式接口。</p> 
<p><strong>参数</strong><br> inputs: 一个列表的输入张量（列表大小至少为 2）。<br> **kwargs: 层的关键字参数。</p> 
<p><strong>返回</strong><br> 一个张量，所有输入张量的平均值。</p> 
<h3><a id="maximum_665"></a>maximum</h3> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>maximum<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
</code></pre> 
<p>Maximum 层的函数式接口。</p> 
<p><strong>参数</strong><br> inputs: 一个列表的输入张量（列表大小至少为 2）。<br> **kwargs: 层的关键字参数。</p> 
<p><strong>返回</strong><br> 一个张量，所有张量的逐元素的最大值。</p> 
<h3><a id="concatenate_680"></a>concatenate</h3> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<p>Concatenate 层的函数式接口。</p> 
<p><strong>参数</strong><br> inputs: 一个列表的输入张量（列表大小至少为 2）。<br> axis: 串联的轴。<br> **kwargs: 层的关键字参数。</p> 
<p><strong>返回</strong><br> 一个张量，所有输入张量通过 axis 轴串联起来的输出张量。</p> 
<h3><a id="dot_695"></a>dot</h3> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> axes<span class="token punctuation">,</span> normalize<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<p>Dot 层的函数式接口。</p> 
<p><strong>参数</strong><br> inputs: 一个列表的输入张量（列表大小至少为 2）。<br> axes: 整数或者整数元组， 一个或者几个进行点积的轴。<br> normalize: 是否在点积之前对即将进行点积的轴进行 L2 标准化。 如果设置成 True，那么输出两个样本之间的余弦相似值。<br> **kwargs: 层的关键字参数。</p> 
<p><strong>返回</strong><br> 一个张量，所有输入张量样本之间的点积。</p> 
<h2><a id="11__Advanced_Activations_712"></a>11. 高级激活层 Advanced Activations</h2> 
<p><a href="https://keras.io/zh/layers/advanced-activations/" rel="nofollow">来源</a></p> 
<h3><a id="ReLU_714"></a>ReLU</h3> 
<p>ReLU 激活函数。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>max_value<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> negative_slope<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> threshold<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span>
</code></pre> 
<p>使用默认值时（max_value=None, negative_slope=0.0, threshold=0.0），它返回逐个元素的 max(x，0)。<br> 否则：<br> 如果 x &gt;= max_value，返回 f(x) = max_value，<br> 如果 threshold &lt;= x &lt; max_value，返回 f(x) = x,<br> 否则，返回 f(x) = negative_slope * (x - threshold)。</p> 
<p><strong>输入尺寸</strong><br> 可以是任意的。如果将这一层作为模型的第一层， 则需要指定 input_shape 参数 （整数元组，不包含样本数量的维度）。</p> 
<p><strong>输出尺寸</strong><br> 与输入相同。</p> 
<p><strong>参数</strong><br> max_value: 浮点数，最大的输出值。<br> negative_slope: float &gt;= 0. 负斜率系数。<br> threshold: float。“thresholded activation” 的阈值。</p> 
<h3><a id="Softmax_736"></a>Softmax</h3> 
<p>Softmax 激活函数。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>输入尺寸</strong><br> 可以是任意的。如果将这一层作为模型的第一层， 则需要指定 input_shape 参数 （整数元组，不包含样本数量的维度）。</p> 
<p><strong>输出尺寸</strong><br> 与输入相同。</p> 
<p><strong>参数</strong><br> axis: 整数，应用 softmax 标准化的轴。</p> 
<h3><a id="ThresholdedReLU_750"></a>ThresholdedReLU</h3> 
<p>带阈值的修正线性单元。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>ThresholdedReLU<span class="token punctuation">(</span>theta<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span>
</code></pre> 
<p>形式： f(x) = x for x &gt; theta, f(x) = 0 otherwise.</p> 
<p><strong>输入尺寸</strong><br> 可以是任意的。如果将这一层作为模型的第一层， 则需要指定 input_shape 参数 （整数元组，不包含样本数量的维度）。</p> 
<p><strong>输出尺寸</strong><br> 与输入相同。</p> 
<p><strong>参数</strong><br> theta: float &gt;= 0。<strong>激活的阈值位。</strong></p> 
<h3><a id="LeakyReLU_767"></a>LeakyReLU</h3> 
<p>带泄漏的 ReLU。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span>alpha<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>
</code></pre> 
<p>当神经元未激活时，它仍允许赋予一个很小的梯度： f(x) = alpha * x for x &lt; 0, f(x) = x for x &gt;= 0.</p> 
<p><strong>输入尺寸</strong><br> 可以是任意的。如果将该层作为模型的第一层， 则需要指定 input_shape 参数 （整数元组，不包含样本数量的维度）。</p> 
<p><strong>输出尺寸</strong><br> 与输入相同。</p> 
<p><strong>参数</strong><br> alpha: float &gt;= 0。负斜率系数。</p> 
<h3><a id="PReLU_785"></a>PReLU</h3> 
<p>参数化的 ReLU。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>PReLU<span class="token punctuation">(</span>alpha_initializer<span class="token operator">=</span><span class="token string">'zeros'</span><span class="token punctuation">,</span> alpha_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> alpha_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> shared_axes<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p>形式： f(x) = alpha * x for x &lt; 0, f(x) = x for x &gt;= 0, 其中 <strong>alpha 是一个可学习的数组</strong>，尺寸与 x 相同。</p> 
<p><strong>输入尺寸</strong><br> 可以是任意的。如果将这一层作为模型的第一层， 则需要指定 input_shape 参数 （整数元组，不包含样本数量的维度）。</p> 
<p><strong>输出尺寸</strong><br> 与输入相同。</p> 
<p><strong>参数</strong></p> 
<ul><li>alpha_initializer: 权重的初始化函数。</li><li>alpha_regularizer: 权重的正则化方法。</li><li>alpha_constraint: 权重的约束。</li><li>shared_axes: 激活函数共享可学习参数的轴。 例如，如果输入特征图来自输出形状为 (batch, height, width, channels) 的 2D 卷积层，而且你希望跨空间共享参数，以便每个滤波器只有一组参数， 可设置 shared_axes=[1, 2]。</li></ul> 
<h3><a id="ELU_805"></a>ELU</h3> 
<p>指数线性单元。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>ELU<span class="token punctuation">(</span>alpha<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span>
</code></pre> 
<p>形式： f(x) = alpha * (exp(x) - 1.) for x &lt; 0, f(x) = x for x &gt;= 0.</p> 
<p><strong>输入尺寸</strong><br> 可以是任意的。如果将这一层作为模型的第一层， 则需要指定 input_shape 参数 （整数元组，不包含样本数量的维度）。</p> 
<p><strong>输出尺寸</strong><br> 与输入相同。</p> 
<p><strong>参数</strong><br> alpha: 负因子的尺度。</p> 
<h2><a id="12__Normalization_827"></a>12. 标准化层 Normalization</h2> 
<h3><a id="BatchNormalization_828"></a>BatchNormalization</h3> 
<p>批量标准化层 (Ioffe and Szegedy, 2014)。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>BatchNormalization<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.99</span><span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> 
center<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> scale<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> beta_initializer<span class="token operator">=</span><span class="token string">'zeros'</span><span class="token punctuation">,</span> 
gamma_initializer<span class="token operator">=</span><span class="token string">'ones'</span><span class="token punctuation">,</span> moving_mean_initializer<span class="token operator">=</span><span class="token string">'zeros'</span><span class="token punctuation">,</span> 
moving_variance_initializer<span class="token operator">=</span><span class="token string">'ones'</span><span class="token punctuation">,</span> beta_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> 
gamma_regularizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> beta_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> gamma_constraint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p>在每一个批次的数据中标准化前一层的激活项， 即，应用一个维持激活项平均值接近 0，标准差接近 1 的转换。</p> 
<p><strong>参数</strong></p> 
<ul><li>axis: 整数，需要标准化的轴 （通常是特征轴）。 例如，在 data_format=“channels_first” 的 Conv2D 层之后， 在 BatchNormalization 中设置 axis=1。</li><li>momentum: 移动均值和移动方差的动量。</li><li>epsilon: 增加到方差的小的浮点数，以避免除以零。</li><li>center: 如果为 True，把 beta 的偏移量加到标准化的张量上。 如果为 False， beta 被忽略。</li><li>scale: 如果为 True，乘以 gamma。 如果为 False，gamma 不使用。 当下一层为线性层（或者例如 nn.relu）， 这可以被禁用，因为缩放将由下一层完成。</li><li>beta_initializer: beta 权重的初始化方法。</li><li>gamma_initializer: gamma 权重的初始化方法。</li><li>moving_mean_initializer: 移动均值的初始化方法。</li><li>moving_variance_initializer: 移动方差的初始化方法。</li><li>beta_regularizer: 可选的 beta 权重的正则化方法。</li><li>gamma_regularizer: 可选的 gamma 权重的正则化方法。</li><li>beta_constraint: 可选的 beta 权重的约束方法。</li><li>gamma_constraint: 可选的 gamma 权重的约束方法。</li></ul> 
<p><strong>输入尺寸</strong><br> 可以是任意的。如果将这一层作为模型的第一层， 则需要指定 input_shape 参数 （整数元组，不包含样本数量的维度）。</p> 
<p><strong>输出尺寸</strong><br> 与输入相同。</p> 
<h2><a id="13__Noise_861"></a>13. 噪声层 Noise</h2> 
<p><a href="https://keras.io/zh/layers/noise/" rel="nofollow">来源</a></p> 
<h3><a id="GaussianNoise_863"></a>GaussianNoise</h3> 
<p>应用以 0 为中心的加性高斯噪声。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>GaussianNoise<span class="token punctuation">(</span>stddev<span class="token punctuation">)</span>
</code></pre> 
<p>这对缓解过拟合很有用 （你可以将其视为随机数据增强的一种形式）。 高斯噪声（GS）是对真实输入的腐蚀过程的自然选择。</p> 
<p>由于它是一个正则化层，因此它只在训练时才被激活。</p> 
<p><strong>参数</strong><br> stddev: float，噪声分布的标准差。</p> 
<p><strong>输入尺寸</strong><br> 可以是任意的。 如果将该层作为模型的第一层，则需要指定 input_shape 参数 （整数元组，不包含样本数量的维度）。</p> 
<p><strong>输出尺寸</strong><br> 与输入相同。</p> 
<h3><a id="GaussianDropout_881"></a>GaussianDropout</h3> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>GaussianDropout<span class="token punctuation">(</span>rate<span class="token punctuation">)</span>
</code></pre> 
<p>应用以 1 为中心的 乘性高斯噪声。<br> 由于它是一个正则化层，因此它只在训练时才被激活。</p> 
<p><strong>参数</strong><br> rate: float，丢弃概率（与 Dropout 相同）。 这个乘性噪声的标准差为 sqrt(rate / (1 - rate))。</p> 
<h3><a id="AlphaDropout_893"></a>AlphaDropout</h3> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>AlphaDropout<span class="token punctuation">(</span>rate<span class="token punctuation">,</span> noise_shape<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> seed<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p>将 Alpha Dropout 应用到输入。</p> 
<p>Alpha Dropout 是一种 Dropout， 它保持输入的平均值和方差与原来的值不变， 以确保即使在 dropout 后也能实现自我归一化。 通过随机将激活设置为负饱和值， Alpha Dropout 非常适合按比例缩放的指数线性单元（SELU）。</p> 
<p><strong>参数</strong></p> 
<ul><li>rate: float，丢弃概率（与 Dropout 相同）。 这个乘性噪声的标准差为 sqrt(rate / (1 - rate))。</li><li>seed: 用作随机种子的 Python 整数。</li></ul> 
<h2><a id="14__wrappers_907"></a>14. 层封装器 wrappers</h2> 
<h3><a id="TimeDistributed_908"></a>TimeDistributed</h3> 
<p>这个封装器将一个层应用于输入的每个时间片。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>TimeDistributed<span class="token punctuation">(</span>layer<span class="token punctuation">)</span>
</code></pre> 
<p>输入至少为 3D，且第一个维度应该是时间所表示的维度。考虑 32 个样本的一个 batch， 其中每个样本是 10 个 16 维向量的序列。 那么这个 batch 的输入尺寸为 (32, 10, 16)， 而 input_shape 不包含样本数量的维度，为 (10, 16)。</p> 
<p>你可以使用 TimeDistributed 来将 Dense 层独立地应用到 这 10 个时间步的每一个：</p> 
<pre><code class="prism language-python"><span class="token comment"># 作为模型第一层</span>
model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>TimeDistributed<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">,</span> input_shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 现在 model.output_shape == (None, 10, 8)</span>
</code></pre> 
<p>输出的尺寸为 (32, 10, 8)。<br> 在后续的层中，将不再需要 input_shape：</p> 
<pre><code class="prism language-python">model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>TimeDistributed<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 现在 model.output_shape == (None, 10, 32)</span>
</code></pre> 
<p>输出的尺寸为 (32, 10, 32)。</p> 
<p>TimeDistributed 可以应用于任意层，不仅仅是 Dense， 例如运用于 Conv2D 层：</p> 
<pre><code class="prism language-python">model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>TimeDistributed<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                          input_shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">299</span><span class="token punctuation">,</span> <span class="token number">299</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>参数<br> layer: 一个网络层实例。</p> 
<h3><a id="Bidirectional_944"></a>Bidirectional</h3> 
<p>RNN 的双向封装器，对序列进行前向和后向计算。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Bidirectional<span class="token punctuation">(</span>layer<span class="token punctuation">,</span> merge_mode<span class="token operator">=</span><span class="token string">'concat'</span><span class="token punctuation">,</span> weights<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>参数</strong></p> 
<ul><li>layer: Recurrent 实例。</li><li>merge_mode: 前向和后向 RNN 的输出的结合模式。 为 {‘sum’, ‘mul’, ‘concat’, ‘ave’, None} 其中之一。 如果是 None，输出不会被结合，而是作为一个列表被返回。</li></ul> 
<p>例</p> 
<pre><code class="prism language-python">model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Bidirectional<span class="token punctuation">(</span>LSTM<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                        input_shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Bidirectional<span class="token punctuation">(</span>LSTM<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Activation<span class="token punctuation">(</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token string">'categorical_crossentropy'</span><span class="token punctuation">,</span> optimizer<span class="token operator">=</span><span class="token string">'rmsprop'</span><span class="token punctuation">)</span>
</code></pre> 
<h2><a id="15__964"></a>15. 其他</h2> 
<h3><a id="Input_965"></a>Input</h3> 
<p>用于实例化 Keras 张量。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> batch_shape<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> sparse<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> tensor<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p>Keras 张量是底层后端 的张量对象，我们增加了一些特性，使得能够通过模型的输入和输出来构建 Keras 模型。例如，如果 a, b 和 c 都是 Keras 张量， 那么以下操作是可行的： <code>model = keras.models.Model(input=[a, b], output=c)</code></p> 
<p>添加的 Keras 属性是：</p> 
<ul><li>_keras_shape: 通过 Keras端的尺寸推理 进行传播的整数尺寸元组。</li><li>_keras_history: 应用于张量的最后一层。 整个网络层计算图可以递归地从该层中检索。</li></ul> 
<p><strong>参数:</strong></p> 
<ul><li>shape: 一个尺寸元组（整数），<strong>不包含批量大小</strong>。 例如，shape=(32,) 表明期望的输入是按批次的 32 维向量。</li><li>batch_shape: 一个尺寸元组（整数），包含批量大小。 例如，batch_shape=(10, 32) 表明期望的输入是 10 个 32 维向量。 batch_shape=(None, 32) 表明任意批次大小的 32 维向量。</li><li>name: 一个可选的层的名称的字符串。 在一个模型中应该是唯一的（不可以重用一个名字两次）。 如未提供，将自动生成。</li><li>dtype: 输入所期望的数据类型，字符串表示 (float32, float64, int32…)</li><li>sparse: 一个布尔值，指明需要创建的占位符是否是稀疏的。</li><li>tensor: 可选的可封装到 Input 层的现有张量。 如果设定了，那么这个层将不会创建占位符张量。</li></ul> 
<p><strong>返回:</strong><br> 一个张量。</p> 
<p><strong>例:</strong></p> 
<pre><code class="prism language-python"><span class="token comment"># 这是 Keras 中的一个逻辑回归</span>
x <span class="token operator">=</span> Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">'float32'</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> Dense<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
model <span class="token operator">=</span> Model<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="Permute_995"></a>Permute</h3> 
<p>根据给定的模式置换输入的维度。<strong>相当于2维矩阵的转置。</strong></p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Permute<span class="token punctuation">(</span>dims<span class="token punctuation">)</span>
</code></pre> 
<p><strong>参数：</strong><br> dims: 整数元组。置换模式，不包含样本维度。 索引从 1 开始。 例如, (2, 1) 置换输入的第一和第二个维度。</p> 
<pre><code class="prism language-python">model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Permute<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> input_shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 现在： model.output_shape == (None, 64, 10)</span>
<span class="token comment"># 注意： `None` 是批表示的维度</span>
</code></pre> 
<p><strong>输入尺寸</strong><br> 任意。当使用此层作为模型中的第一层时， 使用参数 input_shape （整数元组，不包括样本数的轴）。</p> 
<p><strong>输出尺寸</strong><br> 与输入尺寸相同，但是维度根据指定的模式重新排列。</p> 
<h3><a id="RepeatVector_1014"></a>RepeatVector</h3> 
<p>将输入重复 n 次。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>RepeatVector<span class="token punctuation">(</span>n<span class="token punctuation">)</span>
</code></pre> 
<p><strong>参数：</strong><br> n: 整数，重复次数。</p> 
<p><strong>输入尺寸</strong><br> 2D 张量，尺寸为 (num_samples, features)。</p> 
<p><strong>输出尺寸</strong><br> 3D 张量，尺寸为 (num_samples, n, features)。</p> 
<p><strong>例：</strong></p> 
<pre><code class="prism language-python">model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> input_dim<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 现在： model.output_shape == (None, 32)</span>
<span class="token comment"># 注意： `None` 是批表示的维度</span>

model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>RepeatVector<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 现在： model.output_shape == (None, 3, 32)</span>
</code></pre> 
<h3><a id="Lambda_1038"></a>Lambda</h3> 
<p>将任意表达式封装为 Layer 对象。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Lambda<span class="token punctuation">(</span>function<span class="token punctuation">,</span> output_shape<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> arguments<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>参数</strong></p> 
<ul><li>function: 需要封装的函数。 将输入张量作为第一个参数。</li><li>output_shape: 预期的函数输出尺寸。 只在使用 Theano 时有意义。 可以是元组或者函数。 如果是元组，它只指定第一个维度； 样本维度假设与输入相同： output_shape = (input_shape[0], ) + output_shape 或者，输入是 None 且样本维度也是 None： output_shape = (None, ) + output_shape 如果是函数，它指定整个尺寸为输入尺寸的一个函数： output_shape = f(input_shape)</li><li>arguments: 可选的需要传递给函数的关键字参数。</li></ul> 
<p><strong>输入尺寸</strong><br> 任意。当使用此层作为模型中的第一层时， 使用参数 input_shape （整数元组，不包括样本数的轴）。</p> 
<p><strong>输出尺寸</strong><br> 由 output_shape 参数指定 (或者在使用 TensorFlow 时，自动推理得到)。</p> 
<p><strong>例：</strong></p> 
<pre><code class="prism language-python"><span class="token comment"># 添加一个 x -&gt; x^2 层</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Lambda<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token comment"># 添加一个网络层，返回输入的正数部分与负数部分的反面的连接</span>
<span class="token keyword">def</span> <span class="token function">antirectifier</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x <span class="token operator">-=</span> K<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>x<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    x <span class="token operator">=</span> K<span class="token punctuation">.</span>l2_normalize<span class="token punctuation">(</span>x<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
    pos <span class="token operator">=</span> K<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    neg <span class="token operator">=</span> K<span class="token punctuation">.</span>relu<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span>
    <span class="token keyword">return</span> K<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">[</span>pos<span class="token punctuation">,</span> neg<span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">antirectifier_output_shape</span><span class="token punctuation">(</span>input_shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
    shape <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>input_shape<span class="token punctuation">)</span>
    <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span>  <span class="token comment"># only valid for 2D tensors</span>
    shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*=</span> <span class="token number">2</span>
    <span class="token keyword">return</span> <span class="token builtin">tuple</span><span class="token punctuation">(</span>shape<span class="token punctuation">)</span>

model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Lambda<span class="token punctuation">(</span>antirectifier<span class="token punctuation">,</span> output_shape<span class="token operator">=</span>antirectifier_output_shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="ActivityRegularization_1077"></a>ActivityRegularization</h3> 
<p>网络层，对基于代价函数的输入活动应用一个更新。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>ActivityRegularization<span class="token punctuation">(</span>l1<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> l2<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>参数</strong><br> l1: L1 正则化因子 (正数浮点型)。<br> l2: L2 正则化因子 (正数浮点型)。</p> 
<p><strong>输入尺寸</strong><br> 任意。当使用此层作为模型中的第一层时， 使用参数 input_shape （整数元组，不包括样本数的轴）。</p> 
<p><strong>输出尺寸</strong><br> 与输入相同。</p> 
<h3><a id="Masking_1093"></a>Masking</h3> 
<p>使用覆盖值覆盖序列，以跳过时间步。</p> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Masking<span class="token punctuation">(</span>mask_value<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span>
</code></pre> 
<p>对于输入张量的每一个时间步（张量的第一个维度）， 如果所有时间步中输入张量的值与 mask_value 相等， 那么这个时间步将在所有下游层被覆盖 (跳过) （只要它们支持覆盖）。</p> 
<p>如果任何下游层不支持覆盖但仍然收到此类输入覆盖信息，会引发异常。</p> 
<p><strong>例</strong></p> 
<p>考虑将要喂入一个 LSTM 层的 Numpy 矩阵 x， 尺寸为 (samples, timesteps, features)。 你想要覆盖时间步 #3 和 #5，因为你缺乏这几个 时间步的数据。你可以：</p> 
<p>设置 x[:, 3, :] = 0. 以及 x[:, 5, :] = 0.<br> 在 LSTM 层之前，插入一个 mask_value=0 的 Masking 层：</p> 
<pre><code class="prism language-python">model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Masking<span class="token punctuation">(</span>mask_value<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> input_shape<span class="token operator">=</span><span class="token punctuation">(</span>timesteps<span class="token punctuation">,</span> features<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>LSTM<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="SpatialDropout1D_1116"></a>SpatialDropout1D</h3> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>SpatialDropout1D<span class="token punctuation">(</span>rate<span class="token punctuation">)</span>
</code></pre> 
<p>Dropout 的 Spatial 1D 版本</p> 
<p>此版本的功能与 Dropout 相同，但它会丢弃整个 1D 的特征图而不是丢弃单个元素。如果特征图中相邻的帧是强相关的（通常是靠前的卷积层中的情况），那么常规的 dropout 将无法使激活正则化，且导致有效的学习速率降低。在这种情况下，SpatialDropout1D 将有助于提高特征图之间的独立性，应该使用它来代替 Dropout。</p> 
<p><strong>参数</strong><br> rate: 0 到 1 之间的浮点数。需要丢弃的输入比例。</p> 
<p><strong>输入尺寸</strong><br> 3D 张量，尺寸为：(samples, timesteps, channels)</p> 
<p><strong>输出尺寸</strong><br> 与输入相同。</p> 
<h3><a id="SpatialDropout2D_1135"></a>SpatialDropout2D</h3> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>SpatialDropout2D<span class="token punctuation">(</span>rate<span class="token punctuation">,</span> data_format<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p>Dropout 的 Spatial 2D 版本</p> 
<p>此版本的功能与 Dropout 相同，但它会丢弃整个 2D 的特征图而不是丢弃单个元素。如果特征图中相邻的像素是强相关的（通常是靠前的卷积层中的情况），那么常规的 dropout 将无法使激活正则化，且导致有效的学习速率降低。在这种情况下，SpatialDropout2D 将有助于提高特征图之间的独立性，应该使用它来代替 dropout。</p> 
<p><strong>参数</strong></p> 
<ul><li>rate: 0 到 1 之间的浮点数。需要丢弃的输入比例。</li><li>data_format：channels_first 或者 channels_last。在 channels_first 模式中，通道维度（即深度）位于索引 1，在 channels_last 模式中，通道维度位于索引 3。默认为 image_data_format 的值，你可以在 Keras 的配置文件 ~/.keras/keras.json 中找到它。如果你从未设置过它，那么它将是 channels_last</li></ul> 
<p><strong>输入尺寸</strong><br> 4D 张量，如果 data_format＝channels_first，尺寸为 (samples, channels, rows, cols)，如果 data_format＝channels_last，尺寸为 (samples, rows, cols, channels)</p> 
<p><strong>输出尺寸</strong><br> 与输入相同。</p> 
<h3><a id="SpatialDropout3D_1156"></a>SpatialDropout3D</h3> 
<pre><code class="prism language-python">keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>SpatialDropout3D<span class="token punctuation">(</span>rate<span class="token punctuation">,</span> data_format<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p>Dropout 的 Spatial 3D 版本</p> 
<p>此版本的功能与 Dropout 相同，但它会丢弃整个 3D 的特征图而不是丢弃单个元素。如果特征图中相邻的体素是强相关的（通常是靠前的卷积层中的情况），那么常规的 dropout 将无法使激活正则化，且导致有效的学习速率降低。在这种情况下，SpatialDropout3D 将有助于提高特征图之间的独立性，应该使用它来代替 dropout。</p> 
<p><strong>参数</strong></p> 
<ul><li>rate: 0 到 1 之间的浮点数。需要丢弃的输入比例。</li><li>data_format：channels_first 或者 channels_last。在 channels_first 模式中，通道维度（即深度）位于索引 1，在 channels_last 模式中，通道维度位于索引 4。默认为 image_data_format 的值，你可以在 Keras 的配置文件 ~/.keras/keras.json 中找到它。如果你从未设置过它，那么它将是 channels_last</li></ul> 
<p><strong>输入尺寸</strong><br> 5D 张量，如果 data_format＝channels_first，尺寸为 (samples, channels, dim1, dim2, dim3)，如果 data_format＝channels_last，尺寸为 (samples, dim1, dim2, dim3, channels)</p> 
<p><strong>输出尺寸</strong><br> 与输入相同。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4c6356d04b299123b838a887fbd4813e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">电商后台管理系统——首页内容定制</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/581ee339cea15a58a418c5cc9e55c593/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">谷歌firebase_我使用Google Firebase学到的东西</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>