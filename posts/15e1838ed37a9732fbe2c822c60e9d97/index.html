<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度强化学习记录 - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度强化学习记录" />
<meta property="og:description" content="增强学习是什么 增强学习
与环境交互获取反馈 agent 目标是最大化累积奖励，称为期望值回馈
增强学习框架 RL Process RL process叫做Markov Decision Process (MDP)
The reward hypothesis RL基于奖励假设，目标函数是最大化期望回归，maximize the expected cumulative reward
the Markov Property the Markov Property 暗示agent只需要最近的状态去学习下一步采取什么动作，不考虑历史状态和行为
Observations/States Space agent从环境中学到的信息，
State s
一个完整的这个世界状态的描述Observation o
一个部分的这个世界状态的描述 Action Space 环境中所有可能动作的集合
环境来自离散或者连续空间
Rewards and the discounting agent知道采取行动是否是好的
累积奖励：
即时奖励更有可能发生，因为比起长期奖励他们更容易预测
定义一个discount rate叫gamma,0-1之间。大多时候是0.99-0.95（gamma越大，discount越小，agent更在乎长期回归。gamma越小，discount越大，agent更关系短期回报）每一个奖励会被gamma计算到时间指数步骤，未来期望奖励发生概率降低 任务类型 两种任务： episodic ，continuing
Episodic task：
有一个开始点和结束点（终止状态），an episode: a list of States, Actions, Rewards, and new States ，比如超级玛丽
Continuing tasks：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/15e1838ed37a9732fbe2c822c60e9d97/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-13T14:42:24+08:00" />
<meta property="article:modified_time" content="2023-12-13T14:42:24+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度强化学习记录</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="_1"></a>增强学习是什么</h3> 
<p>增强学习</p> 
<ol><li>与环境交互</li><li>获取反馈</li></ol> 
<p>agent 目标是最大化累积奖励，称为期望值回馈</p> 
<h3><a id="_11"></a>增强学习框架</h3> 
<h4><a id="RL_Process_12"></a>RL Process</h4> 
<p>RL process叫做Markov Decision Process (MDP)<br> <img src="https://images2.imgbox.com/20/70/QYM70EII_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="The_reward_hypothesis_16"></a>The reward hypothesis</h4> 
<p>RL基于奖励假设，目标函数是最大化期望回归，maximize the expected cumulative reward</p> 
<h4><a id="the_Markov_Property_18"></a>the Markov Property</h4> 
<p>the Markov Property 暗示agent只需要最近的状态去学习下一步采取什么动作，不考虑历史状态和行为</p> 
<h4><a id="ObservationsStates_Space_23"></a>Observations/States Space</h4> 
<p>agent从环境中学到的信息，</p> 
<ul><li>State s<br> 一个完整的这个世界状态的描述</li><li>Observation o<br> 一个部分的这个世界状态的描述</li></ul> 
<h4><a id="Action_Space_30"></a>Action Space</h4> 
<p>环境中所有可能动作的集合</p> 
<p>环境来自离散或者连续空间</p> 
<h4><a id="Rewards_and_the_discounting_37"></a>Rewards and the discounting</h4> 
<p>agent知道采取行动是否是好的</p> 
<p>累积奖励：<br> <img src="https://images2.imgbox.com/a6/df/q0a9aExH_o.png" alt="在这里插入图片描述"><br> 即时奖励更有可能发生，因为比起长期奖励他们更容易预测</p> 
<ol><li>定义一个discount rate叫gamma,0-1之间。大多时候是0.99-0.95（gamma越大，discount越小，agent更在乎长期回归。gamma越小，discount越大，agent更关系短期回报）</li><li>每一个奖励会被gamma计算到时间指数步骤，未来期望奖励发生概率降低</li></ol> 
<p><img src="https://images2.imgbox.com/35/24/uk5eGH4y_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_50"></a>任务类型</h3> 
<p>两种任务： episodic ，continuing</p> 
<p>Episodic task：<br> 有一个开始点和结束点（终止状态），an episode: a list of States, Actions, Rewards, and new States ，比如超级玛丽</p> 
<p>Continuing tasks：<br> 任务会一直继续，学会选择最好的策略，同时与环境交互。比如自动化股市交易，agent一直运行直到我们决定停止</p> 
<h3><a id="The_ExplorationExploitation_tradeoff_60"></a>The Exploration/Exploitation trade-off</h3> 
<ul><li>Exploration 采取随机行动探索环境，发现更多信息</li><li>Exploitation 利用已知信息最大化奖励</li></ul> 
<p>我们需要平衡 多少时候去探索环境，多少时候去利用环境信息获取奖励。</p> 
<p>必须定义一个规则帮助我们折衷</p> 
<h3><a id="RL__68"></a>两个解决RL 问题的主要方法</h3> 
<p>换一个说法，如何建立一个RL agent最大化累积期望</p> 
<h4><a id="The_Policy__the_agents_brain_70"></a>The Policy π: the agent’s brain</h4> 
<p>Policy π 是Agent大脑，这个函数告诉我们，在当前状态下去采取什么行动<br> <img src="https://images2.imgbox.com/c1/db/QKUr1Qyi_o.png" alt="在这里插入图片描述"></p> 
<p>Policy 是我们想学习的函数，我们的目标是找到最优的policy π* (最大化期望回归)，通过训练找到。两个训练方法：</p> 
<ul><li>直接，教给agent学习采取哪个行动，根据当前状态：Policy-Based Methods</li><li>间接，教给agent学习哪一个状态更有价值，然后采取行动通向更多有价值状态 ：Value-Based Methods.</li></ul> 
<h4><a id="PolicyBased_Methods_78"></a>Policy-Based Methods</h4> 
<p>这个函数会定义每个状态到最一致行动的映射。或者说，该函数在该状态下所有可能的行动定义一个概率分布</p> 
<p>有两种类型policies：</p> 
<ul><li>确定的 。给定状态下一直返回相同的行动的policy<br> <img src="https://images2.imgbox.com/92/2a/rdgp1cVz_o.png" alt="在这里插入图片描述"></li><li>随机的：在行动上输出一个概率分布<br> <img src="https://images2.imgbox.com/03/9c/dtoyfEmK_o.png" alt="在这里插入图片描述"></li></ul> 
<h4><a id="ValueBased_Methods_86"></a>Value-Based Methods</h4> 
<p>学习一个从状态映射到该状态期望值映射的价值函数</p> 
<p>一个状态的价值是如果agent从那个状态开始，并且按照policy（带着最高价值）行动，可以获取的期望折扣<br> <img src="https://images2.imgbox.com/23/5a/axg37JEq_o.png" alt="在这里插入图片描述"></p> 
<p>每一步选择价值函数定义的最大值，-7, then -6, then -5<br> <img src="https://images2.imgbox.com/92/72/Uc0vEEB9_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="UNIT0_deep_96"></a>UNIT0 增强学习中的deep</h3> 
<p>“deep"指的是深度学习中的网络</p> 
<p>两种基于值的算法， Q-Learning和 Deep Q-Learning.</p> 
<p>Q-Learning使用传统算法，创建一个Q表找到每一个状态对应的行动。</p> 
<p>Deep Q-Learning 使用神经网络最大化Q值<br> <img src="https://images2.imgbox.com/c6/17/vqPUalVM_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="_106"></a>总结</h4> 
<ul><li>增强学习从行动中学习的计算方式，建立一个agent，通过试错与环境交互，收到环境奖励（正、负）作为回馈</li><li>RL agent目标是最大化累计期望奖励</li><li>RL 过程是一个循环，它会输出:state,action,reward,next state</li><li>计算累积期望奖励，需要为奖励做折扣：即时奖励更有可能发生，因为它比长期奖励更可以预测</li><li>解决RL问题，找到一个合适policy.</li><li>有两种找到最优policy方法：直接训练policy，或者训练一个价值函数可以计算每一个state期望回馈，使用函数定义policy</li><li>最后，说到Deep RL,因为我们引入了深度神经网络评估采取的行动（policy-based）或者评估状态价值（value-based)），所以叫做"deep"</li></ul> 
<h3><a id="UNIT2_QLearning_115"></a>UNIT2 介绍Q-Learning</h3> 
<h4><a id="The_Bellman_Equation_simplify_our_value_estimation_117"></a>The Bellman Equation: simplify our value estimation</h4> 
<p>Bellman 公式思想：计算每一个value作为期望回归的总和过程太长，所以我们计算即时奖励+状态的discounted value 总和作为value</p> 
<p><img src="https://images2.imgbox.com/6f/d9/7pSIne2v_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Monte_Carlo_vs_Temporal_Difference_Learning_123"></a>Monte Carlo vs Temporal Difference Learning</h4> 
<p>两种学习策略</p> 
<p>Monte Carlo 在学习前，利用完整的一个episode(表示一个智能体从开始到结束的一次交互过程，包括了环境状态、智能体的决策和行动以及最终的奖励或惩罚)</p> 
<p>Temporal Difference 只使用一步（St,At，Rt+1,St+1）学习</p> 
<p>Monte Carlo:在episode最后学习<br> <img src="https://images2.imgbox.com/49/e0/mixSIyEo_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/81/52/2vgswxzv_o.png" alt="在这里插入图片描述"></p> 
<p>Temporal Difference Learning:每一步都在学习</p> 
<p><img src="https://images2.imgbox.com/5c/7e/M9N6pt8W_o.png" alt="在这里插入图片描述"></p> 
<p>Q-Learning是什么？<br> <img src="https://images2.imgbox.com/9b/9d/ebA47W9l_o.png" alt="在这里插入图片描述"></p> 
<p>Q-Learning is an off-policy value-based method that uses a TD approach to train its action-value function</p> 
<p>Q-Learning是学训练Q函数方法，一个决定在特殊状态下的值，并且在按个状态下采取具体行动的函数<br> <img src="https://images2.imgbox.com/68/ae/tqN43POM_o.png" alt="在这里插入图片描述"></p> 
<p>Q来自Quality（在state下的value）<br> Q-function内部有一个Q-table</p> 
<p>Off-policy vs On-policy<br> Off-policy: using a different policy for acting (inference) and updating (training).</p> 
<p>For instance, with Q-Learning, the epsilon-greedy policy (acting policy), is different from the greedy policy that is used to select the best next-state action value to update our Q-value (updating policy).</p> 
<p>On-policy: using the same policy for acting and updating.<br> For instance, with Sarsa, another value-based algorithm, the epsilon-greedy policy selects the next state-action pair, not a greedy policy.</p> 
<p><img src="https://images2.imgbox.com/37/2d/AfZF5KE4_o.png" alt="在这里插入图片描述"></p> 
<p>总结：</p> 
<p>找到optimal policy两种方法：Policy-based methods. +Value-based method（The state-value function结束时返回值+The action-value function每一步返回值）</p> 
<p><img src="https://images2.imgbox.com/e2/68/8e2Fdt5G_o.png" alt="在这里插入图片描述"><br> Epsilon-greedy strategy:用来平衡exploration 和 exploitation.1-epsilon概率选择最高期望值的行动，epsilon概率随机选择炫动，epsilon随着时间降低exploitation</p> 
<p>Greedy strategy：总是选择给予当前环境的了解下最高期望值的行动，没有exploration，在未知情况下表现很差</p> 
<h4><a id="Unit3_176"></a>Unit3</h4> 
<p>Deep Q-Learning 使用深度神经网络而不是Q-table采取下一步状态评估每一步的Q-values，</p> 
<p>Q-Learning是训练Q-Function（action-value function决定了在特殊状态下的值，并且采取具体行动）的算法。</p> 
<p><img src="https://images2.imgbox.com/4b/de/7p6zP1hN_o.png" alt="在这里插入图片描述"></p> 
<p>Deep Q-Learning training 可能局限于不稳定，因为结合了一个非线性Q-value函数，bootstrappong,</p> 
<p>为了训练稳定：</p> 
<ol><li>Experience Replay to make more efficient use of experiences.（use a replay buffer that saves experience samples that we can reuse during the training，By randomly sampling the experiences, we remove correlation in the observation sequences and avoid action values from oscillating or diverging catastrophically）</li><li>Fixed Q-Target to stabilize the training.</li><li>Double Deep Q-Learning, to handle the problem of the overestimation of Q-values.</li></ol> 
<p><img src="https://images2.imgbox.com/25/d1/n4XDPCvH_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/88/f4/9vJKWuFo_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Policy__197"></a>Policy 梯度</h4> 
<p>找到最优 policy π ∗，</p> 
<ul><li>value-based methods，学习价值函数</li><li>policy-based methods</li></ul> 
<h3><a id="_203"></a>动手实践</h3> 
<p><img src="https://images2.imgbox.com/b3/ce/VEetJ3IJ_o.png" alt="在这里插入图片描述"></p> 
<p><a href="https://www.gymlibrary.dev/environments/box2d/lunar_lander/" rel="nofollow">Gym</a> : Box2D 环境的部分。环境是一个经典的火箭轨迹优化问题</p> 
<p>Gym 库提供了：创建RL环境接口+环境的集合 (gym-control, atari, box2D…)</p> 
<p><a href="https://stable-baselines3.readthedocs.io/en/master/" rel="nofollow">Stable-Baselines3</a>：是PyTorch增强学习算法的一系列实现，https://github.com/DLR-RM/stable-baselines3</p> 
<h4><a id="UNIT1_214"></a>UNIT1报错解决</h4> 
<pre><code class="prism language-cpp"><span class="token operator">!</span>sudo apt<span class="token operator">-</span>get update
<span class="token operator">!</span>apt install python<span class="token operator">-</span>opengl
<span class="token operator">!</span>apt install ffmpeg
<span class="token operator">!</span>apt install xvfb
<span class="token operator">!</span>pip3 install pyvirtualdisplay
</code></pre> 
<p>apt 改成 yum</p> 
<p>安装xvfb报错<br> <img src="https://images2.imgbox.com/8a/be/VflNaXTQ_o.png" alt="在这里插入图片描述"></p> 
<p>报错要注册，参考<a href="https://blog.csdn.net/maibaizhou/article/details/121047462">https://blog.csdn.net/maibaizhou/article/details/121047462</a><br> vim /etc/yum/pluginconf.d/subscription-manager.conf</p> 
<pre><code class="prism language-python">yum search xvfb
</code></pre> 
<p><img src="https://images2.imgbox.com/a1/f0/icUaxodw_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-cpp"> yum install python<span class="token operator">-</span>xvfbwrapper<span class="token punctuation">.</span>noarch
</code></pre> 
<p>安装Box2D失败，gcc: error trying to exec ‘cc1plus’: execvp: No such file or directory</p> 
<p>解决办法 ：<br> 使用命令</p> 
<pre><code class="prism language-cpp">yum install gcc gcc<span class="token operator">-</span>c<span class="token operator">++</span>
</code></pre> 
<p>参考：<a href="https://blog.csdn.net/qq_39240270/article/details/85287599">https://blog.csdn.net/qq_39240270/article/details/85287599</a></p> 
<p>env = gym.make(“LunarLander-v2”)执行报错executing.executing.NotOneValueFound: Expected one value, found 0</p> 
<pre><code class="prism language-cpp">pip3 install box2d box2d<span class="token operator">-</span>kengz
</code></pre> 
<p>参考：<a href="https://stackoverflow.com/questions/50037674/attributeerror-module-box2d-has-no-attribute-rand-limit-swigconstant" rel="nofollow">https://stackoverflow.com/questions/50037674/attributeerror-module-box2d-has-no-attribute-rand-limit-swigconstant</a></p> 
<p>报错：参数数量错误<br> <img src="https://images2.imgbox.com/68/19/h6Z7U9Gl_o.png" alt="在这里插入图片描述"></p> 
<p>解决：更改参数数量还是报错，重新进入python环境莫名其妙就对了</p> 
<p>参考手册：<a href="https://www.gymlibrary.dev/environments/box2d/lunar_lander/" rel="nofollow">https://www.gymlibrary.dev/environments/box2d/lunar_lander/</a></p> 
<p>模型上传时报错</p> 
<pre><code class="prism language-cpp"><span class="token keyword">import</span> <span class="token module">gym</span>

from stable_baselines3 <span class="token keyword">import</span> <span class="token module">PPO</span>
from stable_baselines3<span class="token punctuation">.</span>common<span class="token punctuation">.</span>vec_env <span class="token keyword">import</span> <span class="token module">DummyVecEnv</span>
from stable_baselines3<span class="token punctuation">.</span>common<span class="token punctuation">.</span>env_util <span class="token keyword">import</span> <span class="token module">make_vec_env</span>

from huggingface_sb3 <span class="token keyword">import</span> <span class="token module">package_to_hub</span>

<span class="token macro property"><span class="token directive-hash">#</span> <span class="token expression">PLACE the variables you<span class="token number">'</span>ve just defined two cells above</span></span>
<span class="token macro property"><span class="token directive-hash">#</span> <span class="token expression">Define </span><span class="token macro-name">the</span> <span class="token expression">name of the environment</span></span>
env_id <span class="token operator">=</span> <span class="token string">"LunarLander-v2"</span>

<span class="token macro property"><span class="token directive-hash">#</span> <span class="token expression">TODO<span class="token operator">:</span> Define the model architecture we used</span></span>
model_architecture <span class="token operator">=</span> <span class="token string">"PPO"</span>

## Define a repo_id
## repo_id is the id of the model repository from the Hugging Face <span class="token function">Hub</span> <span class="token punctuation">(</span>repo_id <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>organization<span class="token punctuation">}</span><span class="token operator">/</span><span class="token punctuation">{<!-- --></span>repo_name<span class="token punctuation">}</span> <span class="token keyword">for</span> instance ThomasSimonini<span class="token operator">/</span>ppo<span class="token operator">-</span>LunarLander<span class="token operator">-</span>v2
## CHANGE WITH YOUR REPO ID
repo_id <span class="token operator">=</span> <span class="token string">"DiracUniverse/ppo-LunarLander-v2"</span>  # Change with your repo id<span class="token punctuation">,</span> you can<span class="token number">'</span>t push with mine 😄

## Define the commit message
commit_message <span class="token operator">=</span> <span class="token string">"Upload PPO LunarLander-v2 trained agent"</span>

<span class="token macro property"><span class="token directive-hash">#</span> <span class="token expression">Create the evaluation env</span></span>
eval_env <span class="token operator">=</span> <span class="token function">DummyVecEnv</span><span class="token punctuation">(</span><span class="token punctuation">[</span>lambda<span class="token operator">:</span> gym<span class="token punctuation">.</span><span class="token function">make</span><span class="token punctuation">(</span>env_id<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token macro property"><span class="token directive-hash">#</span> <span class="token expression">PLACE the package_to_hub function you<span class="token number">'</span>ve just filled here</span></span>
<span class="token function">package_to_hub</span><span class="token punctuation">(</span>
    model<span class="token operator">=</span>model<span class="token punctuation">,</span>  # Our trained model
    model_name<span class="token operator">=</span>model_name<span class="token punctuation">,</span>  # The name of our trained model
    model_architecture<span class="token operator">=</span>model_architecture<span class="token punctuation">,</span>  # The model architecture we used<span class="token operator">:</span> in our <span class="token keyword">case</span> PPO
    env_id<span class="token operator">=</span>env_id<span class="token punctuation">,</span>  # Name of the environment
    eval_env<span class="token operator">=</span>eval_env<span class="token punctuation">,</span>  # Evaluation Environment
    repo_id<span class="token operator">=</span>repo_id<span class="token punctuation">,</span>  # id of the model repository from the Hugging Face <span class="token function">Hub</span> <span class="token punctuation">(</span>repo_id <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>organization<span class="token punctuation">}</span><span class="token operator">/</span><span class="token punctuation">{<!-- --></span>repo_name<span class="token punctuation">}</span> <span class="token keyword">for</span> instance ThomasSimonini<span class="token operator">/</span>ppo<span class="token operator">-</span>LunarLander<span class="token operator">-</span>v2
    commit_message<span class="token operator">=</span>commit_message<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/53/0e/XuWregr4_o.png" alt="在这里插入图片描述"></p> 
<p>报错，需要图形化界面<br> <a href="https://cloud.tencent.com/developer/article/1721673" rel="nofollow">CentOS7安装GUI界面及远程连接的实现</a><br> <a href="https://blog.csdn.net/qq_39335514/article/details/88998651">如何利用Windows 10连接远程服务器桌面</a><br> <a href="https://blog.csdn.net/qq_44507335/article/details/94874973">windows10远程连接centos桌面</a><br> xhost:unable to open display""的问题， 设置环境变量 export DISPLAY=:0.0</p> 
<p>pytorch 版本不对 报错： CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling <code>cublasSgemm( handle, opa, opb, m, n, k, &amp;alpha, a, lda, b, ldb, &amp;beta, c, ldc)</code><br> <img src="https://images2.imgbox.com/c1/c3/GyxjKv1J_o.png" alt="在这里插入图片描述"><br> 解决：<code>pip install torch==2.0.0</code></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/bfa395f494634c07129aaa6c6aefae99/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">互联网加竞赛 python opencv 深度学习 指纹识别算法实现</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0c3a243deb82a9aef828e83034fa8c7b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Centos7安装nacos教程</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>