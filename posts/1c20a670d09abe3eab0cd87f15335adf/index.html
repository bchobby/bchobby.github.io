<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ICDAR 2019表格识别论文与竞赛综述（上） - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="ICDAR 2019表格识别论文与竞赛综述（上）" />
<meta property="og:description" content="ICDAR 2019表格识别论文与竞赛综述（上）
表格作为一种有效的数据组织与展现方法被广泛应用，也成为各类文档中常见的页面对象。随着文档数目的爆炸性增长，如何高效地从文档中找到表格并获取内容与结构信息即表格识别，成为了一个亟待解决的问题。ICDAR是一个专注于文档分析与识别问题的国际学术会议，已经连续多届设置了表格识别专题。在今年的ICDAR 2019会议上，有不少研究者在表格检测与结构识别等领域做出了新的贡献，使其有了新的进展。本课题组梳理了该会议中有关表格识别的16篇论文，总结该领域当前的研究进展与挑战。同时，值得注意的是，该会议也举办了关于表格检测与结构识别的比赛，我们对参赛队伍使用的方法与结果进行了一些讨论。
一、研究背景 随着文档尤其是通过扫描、拍照等方式生成的文档的快速增长，自动地进行文档识别（Document Recognition）并从中提取有用的数据成为了一个备受关注的研究问题。这其中，表格作为一种高效的数据组织和展现的方式，是文档页面中最重要的数据对象之一。表格识别包括表格检测与结构识别，作为文档识别一个重要的子任务，一直是该领域研究者关注的研究问题。
图1 表格检测与结构识别示例
文档页面往往由不同的页面对象（Page Object）组成，包括文本行、图像、表格、公式等。表格检测（Table Detection）任务是从一个页面中检测出表格所在的区域，表格结构识别（Table Structure Recognition）任务则是在检测到的表格区域的基础上，进一步将表格的内容与逻辑结构识别出来。如图1所示，若输入为页面图像，表格检测任务就是在页面图像中定位出表格的位置，这与计算机视觉中的目标检测任务类似；而表格结构识别任务则是对检测出的表格区域进行分析，得到表格的行列、层次等逻辑结构。由于表格大小、种类与样式复杂多样，例如表格中存在不同的背景填充，不同的行列合并方法，不同的内容文本类型等，并且现有文档既包括现代的、电子的文档，也有历史的、扫描的手写文档，它们的文档样式、所处光照环境以及纹理等都有比较大的差异，正如图2中表格示例所展示的那样，表格识别一直是文档识别领域的研究难点。
早期的表格识别研究主要是基于启发式规则的方法[16-20]，既有基于图像文档的方法，也有基于PDF文档的方法。例如由Kieninger等人提出的T-Rect[16]系统使用自底向上的方法对文档图像进行连通分支分析，然后按照定义的规则进行合并，得到逻辑文本块。而之后由Yildiz等人提出的pdf2table[18]则是第一个在PDF文档上进行表格识别的方法，它利用了PDF文件的一些特有信息（例如文字、绘制路径等图像文档中难以获取的信息）来协助表格识别。而在最近的工作中，Koci等人[22]将页面中的布局区域表示为图（Graph）的形式，然后使用了Remove and Conquer(RAC)算法从中将表格作为一个子图识别出来。
近年来飞速发展的深度学习技术在计算机视觉、自然语言处理、语音处理等领域取得了很多可观的成果。研究者们也将深度学习技术应用到了场景文字识别、文档分析、表格识别等领域，取得了不错的效果。由于表格检测任务与计算机视觉中的目标检测任务比较类似，这个任务优先得到了很多性能上的提升，在ICDAR2013表格竞赛[23]和ICDAR2017页面对象识别竞赛[24]里，各个队伍的方法与结果已经显示出了表格识别方法的转变与相应的性能提升。而表格结构识别任务则与其他领域的任务有着不小的区别，所以之前还没出现特别有效的深度学习方法。另一方面，由于深度学习方法是数据驱动的方法，需要大量的标注数据对模型进行训练，所以现有的数据集规模偏小也是一个重要的制约因素。
图2 表格检测与结构识别示例
左上：有颜色背景的全线表，右上：少线表，左中：无线表，左下：有复杂表格线条样式的表格，右下：拍照得到的手写历史文档。
在表格识别领域，国际上最相关的会议之一就是文档分析与识别会议ICDAR（International Conference on Document Analysis and Recognition），它是由国际模式识别学会（IAPR）组织的会议之一，每两年举办一次，是文档识别领域最重要的学术会议。今年，ICDAR2019于9月20日-25日在澳大利亚悉尼举办，多篇有关表格识别的论文也在会议上作了口头或者海报报告。这些论文有的针对表格的特性解决了以往研究的一些问题，有的提出一些新的目标检测方法应用到表格识别领域并取得了不错的效果，有的则是发布了包含大量表格的数据集，为该领域提供数据支持。同时，ICDAR2019也举办了一个针对表格识别任务的比赛：ICDAR2019 表格检测与识别比赛（ICDAR2019 Competition on Table Detection and Recognition, cTDaR），来自工业界和学术界的十余支队伍在这个比赛中提交了有效结果。
我们针对ICDAR2019会议中有关表格识别，即表格检测与结构识别的论文和竞赛做一个综述，主要包括以下内容：
对ICDAR2019中关于表格识别的论文的一个整体探讨。内容包括这些论文的主要方法，方法的有效性与新颖性，实验情况等，并对这些论文做一个整体的对比分析，以期让读者对当前该领域的研究进展有一个整体认识。
对ICDAR2019表格检测与识别比赛的情况概述。参赛队伍提交方法的讨论与比赛结果的分析。参赛队伍中既有来自于工业界的同行，也有来自于学术界的学者，我们可以从中看到整个领域的方法趋势和性能进展状况。
对于表格识别领域目前仍然存在的问题和未来研究方向的探讨。表格本身的特性导致某些识别问题和挑战是非常难以解决的，我们对这些问题加以分析，并尝试针对这些问题给出一些未来可能的研究方向。
二、论文方法
在今年的ICDAR2019会议中，共有16篇与表格识别相关的论文，其中有5篇针对表格检测任务，有8篇针对表格结构识别任务，有1篇在他们的方法里同时进行了表格检测与结构识别的任务，还有2篇则是发布了新的表格识别相关的数据集。在接下来3个子章节中，我们分别介绍表格检测、表格结构识别和表格数据集三部分的论文情况，以探讨目前该领域的最新研究进展。在此之前，这些论文的整体情况概述展示在了表1中。
表1 ICDAR2019表格识别领域论文整体情况
注：“论文名称”一列中，论文名称前标有*号的论文表示其未使用基于深度学习的方法。每一种方法的“效果”一列中展示的均为论文中的最好效果。
a) 表格检测任务
随着深度学习方法的迅速发展，越来越多研究者倾向于使用深度学习的方法来进行表格检测任务。对于文档页面图像中的表格检测任务来说，研究者通常将其视为目标检测（Object Detection）或者图像语义分割（Semantic Segmentation）任务，而近些年来，随着图神经网络（Graph Neural Network）的兴起，也有一些研究者尝试将图神经网络应用到文档识别或表格识别领域中来。本次ICDAR2019会议中，共有5篇论文尝试解决表格检测中的一些已知问题，除了其中1篇使用了基于规则的传统方法，其余4篇均使用了深度学习方法，并且其中一篇创新性地将图神经网络应用到发票中的表格检测中来。
图3 论文[1]中提出的open table和closed table的检测框架
《Parameter-free Table Detection Method》[1]一文使用了传统的规则方法，将多高斯分析应用到了表格检测任务中来，使得无参数无训练集的表格检测成为可能，从而规避了该领域缺少用于训练的大量数据的问题。作者将表格检测任务分为了Open Table和Closed Table两个类别来处理。整体框架如图3所示。对于Closed Table，作者先删除表格中的所有行和列的线，之后使用距离方法确定每个单元格的边界，然后对单元格的边界进行水平和竖直方向的投影来获得行和列的分割间隙，最后使用HPP（Horizontal Projection Profile）来确认表格检测结果。对于Open Table，由于缺少表框和水平与竖直线，文本单元格的内容可能会被误识别为常规的段落文本，因此作者通过将多高斯分布模型扩展到文本块的高度和宽度直方图来解决该问题。宽度直方图显示了两种类型文本块对应的两种不同的区域。高低直方图原点附近的峰表明，有一些高度较小的块可能对应于表格的单元格文本。最终作者使用GMM-EM方法来拟合平滑的数据，用加权高斯方法将数据分为三组，分别是小、中和大尺寸的表格。经过处理，对文档中的表格和段落分类为表格块和文本块。将相邻的表格块进行合并，将没有左邻居或右邻居的块，作为表格结束单元格，以得到表格区域。本方法在ICDAR2013数据集上进行了测试，取得了Open Table F1-score=91.3%，Closed Table F1-score=92." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/1c20a670d09abe3eab0cd87f15335adf/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-11-28T21:05:00+08:00" />
<meta property="article:modified_time" content="2019-11-28T21:05:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ICDAR 2019表格识别论文与竞赛综述（上）</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p style="text-align: center"><strong>ICDAR 2019表格识别论文与竞赛综述</strong><strong>（上）</strong></p> 
 <p>表格作为一种有效的数据组织与展现方法被广泛应用，也成为各类文档中常见的页面对象。随着文档数目的爆炸性增长，如何高效地从文档中找到表格并获取内容与结构信息即表格识别，成为了一个亟待解决的问题。ICDAR是一个专注于文档分析与识别问题的国际学术会议，已经连续多届设置了表格识别专题。在今年的ICDAR 2019会议上，有不少研究者在表格检测与结构识别等领域做出了新的贡献，使其有了新的进展。本课题组梳理了该会议中有关表格识别的16篇论文，总结该领域当前的研究进展与挑战。同时，值得注意的是，该会议也举办了关于表格检测与结构识别的比赛，我们对参赛队伍使用的方法与结果进行了一些讨论。</p> 
 <p><strong>一、研究背景</strong> </p> 
 <p>随着文档尤其是通过扫描、拍照等方式生成的文档的快速增长，自动地进行文档识别（Document Recognition）并从中提取有用的数据成为了一个备受关注的研究问题。这其中，表格作为一种高效的数据组织和展现的方式，是文档页面中最重要的数据对象之一。表格识别包括表格检测与结构识别，作为文档识别一个重要的子任务，一直是该领域研究者关注的研究问题。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/d9/b5/iR1qQ49g_o.png"></p> 
 <p style="text-align: center">图1 表格检测与结构识别示例</p> 
 <p>文档页面往往由不同的页面对象（Page Object）组成，包括文本行、图像、表格、公式等。表格检测（Table Detection）任务是从一个页面中检测出表格所在的区域，表格结构识别（Table Structure Recognition）任务则是在检测到的表格区域的基础上，进一步将表格的内容与逻辑结构识别出来。如图1所示，若输入为页面图像，表格检测任务就是在页面图像中定位出表格的位置，这与计算机视觉中的目标检测任务类似；而表格结构识别任务则是对检测出的表格区域进行分析，得到表格的行列、层次等逻辑结构。由于表格大小、种类与样式复杂多样，例如表格中存在不同的背景填充，不同的行列合并方法，不同的内容文本类型等，并且现有文档既包括现代的、电子的文档，也有历史的、扫描的手写文档，它们的文档样式、所处光照环境以及纹理等都有比较大的差异，正如图2中表格示例所展示的那样，表格识别一直是文档识别领域的研究难点。</p> 
 <p>早期的表格识别研究主要是基于启发式规则的方法[16-20]，既有基于图像文档的方法，也有基于PDF文档的方法。例如由Kieninger等人提出的T-Rect[16]系统使用自底向上的方法对文档图像进行连通分支分析，然后按照定义的规则进行合并，得到逻辑文本块。而之后由Yildiz等人提出的pdf2table[18]则是第一个在PDF文档上进行表格识别的方法，它利用了PDF文件的一些特有信息（例如文字、绘制路径等图像文档中难以获取的信息）来协助表格识别。而在最近的工作中，Koci等人[22]将页面中的布局区域表示为图（Graph）的形式，然后使用了Remove and Conquer(RAC)算法从中将表格作为一个子图识别出来。</p> 
 <p>近年来飞速发展的深度学习技术在计算机视觉、自然语言处理、语音处理等领域取得了很多可观的成果。研究者们也将深度学习技术应用到了场景文字识别、文档分析、表格识别等领域，取得了不错的效果。由于表格检测任务与计算机视觉中的目标检测任务比较类似，这个任务优先得到了很多性能上的提升，在ICDAR2013表格竞赛[23]和ICDAR2017页面对象识别竞赛[24]里，各个队伍的方法与结果已经显示出了表格识别方法的转变与相应的性能提升。而表格结构识别任务则与其他领域的任务有着不小的区别，所以之前还没出现特别有效的深度学习方法。另一方面，由于深度学习方法是数据驱动的方法，需要大量的标注数据对模型进行训练，所以现有的数据集规模偏小也是一个重要的制约因素。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/9f/a3/iPQTAago_o.png"></p> 
 <p style="text-align: center">图2 表格检测与结构识别示例</p> 
 <p style="text-align: center">左上：有颜色背景的全线表，右上：少线表，左中：无线表，左下：有复杂表格线条样式的表格，右下：拍照得到的手写历史文档。</p> 
 <p>在表格识别领域，国际上最相关的会议之一就是文档分析与识别会议ICDAR（International Conference on Document Analysis and Recognition），它是由国际模式识别学会（IAPR）组织的会议之一，每两年举办一次，是文档识别领域最重要的学术会议。今年，ICDAR2019于9月20日-25日在澳大利亚悉尼举办，多篇有关表格识别的论文也在会议上作了口头或者海报报告。这些论文有的针对表格的特性解决了以往研究的一些问题，有的提出一些新的目标检测方法应用到表格识别领域并取得了不错的效果，有的则是发布了包含大量表格的数据集，为该领域提供数据支持。同时，ICDAR2019也举办了一个针对表格识别任务的比赛：ICDAR2019 表格检测与识别比赛（ICDAR2019 Competition on Table Detection and Recognition, cTDaR），来自工业界和学术界的十余支队伍在这个比赛中提交了有效结果。</p> 
 <p>我们针对ICDAR2019会议中有关表格识别，即表格检测与结构识别的论文和竞赛做一个综述，主要包括以下内容：</p> 
 <ol><li><p>对ICDAR2019中关于表格识别的论文的一个整体探讨。内容包括这些论文的主要方法，方法的有效性与新颖性，实验情况等，并对这些论文做一个整体的对比分析，以期让读者对当前该领域的研究进展有一个整体认识。</p></li><li><p>对ICDAR2019表格检测与识别比赛的情况概述。参赛队伍提交方法的讨论与比赛结果的分析。参赛队伍中既有来自于工业界的同行，也有来自于学术界的学者，我们可以从中看到整个领域的方法趋势和性能进展状况。</p></li><li><p>对于表格识别领域目前仍然存在的问题和未来研究方向的探讨。表格本身的特性导致某些识别问题和挑战是非常难以解决的，我们对这些问题加以分析，并尝试针对这些问题给出一些未来可能的研究方向。</p></li></ol> 
 <p><strong>二、论文方法</strong></p> 
 <p>在今年的ICDAR2019会议中，共有16篇与表格识别相关的论文，其中有5篇针对表格检测任务，有8篇针对表格结构识别任务，有1篇在他们的方法里同时进行了表格检测与结构识别的任务，还有2篇则是发布了新的表格识别相关的数据集。在接下来3个子章节中，我们分别介绍表格检测、表格结构识别和表格数据集三部分的论文情况，以探讨目前该领域的最新研究进展。在此之前，这些论文的整体情况概述展示在了表1中。</p> 
 <p style="text-align: center">表1 ICDAR2019表格识别领域论文整体情况</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/ea/09/OB8kTJWw_o.png"></p> 
 <p style="text-align: center">注：“论文名称”一列中，论文名称前标有*号的论文表示其未使用基于深度学习的方法。每一种方法的“效果”一列中展示的均为论文中的最好效果。</p> 
 <p> </p> 
 <p><strong>a)     表格检测任务</strong></p> 
 <p>随着深度学习方法的迅速发展，越来越多研究者倾向于使用深度学习的方法来进行表格检测任务。对于文档页面图像中的表格检测任务来说，研究者通常将其视为目标检测（Object Detection）或者图像语义分割（Semantic Segmentation）任务，而近些年来，随着图神经网络（Graph Neural Network）的兴起，也有一些研究者尝试将图神经网络应用到文档识别或表格识别领域中来。本次ICDAR2019会议中，共有5篇论文尝试解决表格检测中的一些已知问题，除了其中1篇使用了基于规则的传统方法，其余4篇均使用了深度学习方法，并且其中一篇创新性地将图神经网络应用到发票中的表格检测中来。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/fe/0e/FvYx3YvK_o.png"></p> 
 <p style="text-align: center">图3 论文[1]中提出的open table和closed table的检测框架</p> 
 <p>《Parameter-free Table Detection Method》[1]一文使用了传统的规则方法，将多高斯分析应用到了表格检测任务中来，使得无参数无训练集的表格检测成为可能，从而规避了该领域缺少用于训练的大量数据的问题。作者将表格检测任务分为了Open Table和Closed Table两个类别来处理。整体框架如图3所示。对于Closed Table，作者先删除表格中的所有行和列的线，之后使用距离方法确定每个单元格的边界，然后对单元格的边界进行水平和竖直方向的投影来获得行和列的分割间隙，最后使用HPP（Horizontal Projection Profile）来确认表格检测结果。对于Open Table，由于缺少表框和水平与竖直线，文本单元格的内容可能会被误识别为常规的段落文本，因此作者通过将多高斯分布模型扩展到文本块的高度和宽度直方图来解决该问题。宽度直方图显示了两种类型文本块对应的两种不同的区域。高低直方图原点附近的峰表明，有一些高度较小的块可能对应于表格的单元格文本。最终作者使用GMM-EM方法来拟合平滑的数据，用加权高斯方法将数据分为三组，分别是小、中和大尺寸的表格。经过处理，对文档中的表格和段落分类为表格块和文本块。将相邻的表格块进行合并，将没有左邻居或右邻居的块，作为表格结束单元格，以得到表格区域。本方法在ICDAR2013数据集上进行了测试，取得了Open Table F1-score=91.3%，Closed Table F1-score=92.3%的效果。</p> 
 <p>在《A GAN-based Feature Generator forTable Detection》[7]一文中，作者将在图像生成领域实现很好效果的生成对抗网络加入到了表格检测任务中来。作者认为，常见的表格检测网络模型过于关注表格的表格线特征，而对表格的文本布局特征关注不够，这导致了常规模型对无线表和少线表的检测结果较差。文中提出了一个基于生成对抗网络的表格特征生成网络，网络的输入是擦除表格线的图片和原始图片，生成器的目标是从文档图片中抽取特征，判别器的目标是判断生成器抽取到的特征是来自真实图片还是擦除了表格线的文档图片。经过训练后，生成器可以从两种图片中抽取到相似的特征，即文本布局特征。该特征提取器以一个函数的形式，添加到表格检测网络的Upsampling阶段，以改进最终的效果。整体的结构如图4所示。通过在ICDAR2017训练集上进行训练后，分别在ICDAR2017测试集和手动标注的少线表数据集上进行了测试。实验结果证明：使用生成对抗模型训练出的文本布局特征生成器可以提高常见的语义分割网络和目标检测网络的表格定位效果。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/da/c2/jvgyXSzx_o.png"></p> 
 <p style="text-align: center">图4 论文[7]中表格检测网络结构</p> 
 <p>在《A YOLO-based Table Detection Method》[25]这篇文章中，作者使用著名的目标检测模型YOLOv3来进行表格检测的任务，并对模型中的Anchor进行了针对表格的适应性优化，从而让该模型在从自然目标迁移过来后更适应于表格目标的检测任务。YOLOv3模型预先设置一些符合目标几何特征的Anchor，最后预测的是目标与某个Anchor的位置与大小差异，在COCO数据集上取得了非常好的效果。作者在将该模型应用到表格检测领域时，注意到了自然场景的目标和文档页面中的表格目标之间的差异，于是对Anchor做了针对表格几何特征的适应性优化，然后再辅以若干简单有效的后处理措施，作者也通过实验证明了这两个主要改进的有效性。作者将他们的最终模型在ICDAR2017页面对象识别竞赛的数据集的表格类别上进行测试，得到了State-of-the-art的效果，并在ICDAR2013表格竞赛表格检测子任务的数据集上也达到了非常好的效果。值得注意的是，该方法在两个数据集上不同IoU阈值下测试得到的F1值均超过97%。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/ec/83/DbXL8DAh_o.png"></p> 
 <p style="text-align: center">图5 论文[3]中角点的定义</p> 
 <p style="text-align: left">《Faster R-CNN BasedTable Detection Combining Corner Locating》[3]一文使用了“角点”来提升表格检测的精准度，以此对基本模型得到的检测结果进行进一步的校准与提升。文章首先引入了角点的概念：角点是表格四个顶点周围的一部分区域，这些区域大小相同，同时属于同一个表格的角点构成一个角点组，如图5所示。角点的检测和表格检测一样，可以用目标检测模型来解决。作者使用了单个Faster R-CNN模型，同时进行角点和表格的检测。在定位出各个角点后，针对角点的横纵坐标，分析出属于同一个表格的角点，并过滤未能成功成组的角点。根据大多数的少线表都存在水平线，而没有竖直表格线的这一特点，作者使用角点组对对应的表格检测的横坐标进行了校准，得到最终的表格区域。该方法的整体框架如图6。该文章在ICDAR2017 POD数据集上进行了实验，F1-score达到了94.9%.同时该方法和未加入角点的Faster R-CNN模型相比，在像素级的结果上有了较大的提升，平均IoU可以达到0.832，有超过2个百分点的提升。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/54/12/yHBmOMvr_o.png"></p> 
 <p style="text-align: center">图6 论文[3]提出的加入角点的表格检测网络结构</p> 
 <p>《Table Detection in Invoice Documents by Graph Neural Networks》[12]则是使用了图神经网络（GNN）来进行发票中的表格检测，尝试将表格中的结构特征应用进来。作者先检测出文档中的图像和文本区域，然后根据它们的视线可见性建立可见性图。在作者提出的模型中，之前建立的图被用作输入，最终完成顶点分类和边预测的任务，预测顶点表示的区域是否属于表格，以及预测两个区域是否应该合并。整个模型架构如图7所示，首先对顶点的特征做了一个Embedding得到高层特征，然后传入多层带有残差连接的GNN中，最终在最后一层使用Softmax对每个顶点进行最后的分类。此外，作者还将图邻接关系的调整也添加到了GNN层中，每一次根据隐藏层每个节点的特征更新邻接关系中边的权重，以解决原GNN中邻接关系和层数无关的局限。模型在CON-ANONYM数据集和从已有数据集上新标注的RVL-CDIP上进行测试，均取得了还不错的效果。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/8a/a0/rKCpX7vn_o.png"></p> 
 <p style="text-align: center">图7 论文[12]中提出的基于GNN的表格检测模型结构</p> 
 <p><strong>b)    表格结构识别任务</strong><br></p> 
 <p>表格结构识别任务与其他的计算机视觉任务都不大一样，所以深度学习方法在这个任务上的发展与进步要比表格检测任务慢一些。但随着各种方法的创新，近些年来，深度学习方法也越来越多地被应用到表格结构识别任务上来，并取得了很不错的效果。除了常规的视觉方法，如语义分割等，今年的ICDAR2019会议上还诞生了诸如使用图神经网络或者循环神经网络技术进行表格结构识别的方法，都算是为未来的工作指了一些可能的发展道路。在8篇论文中，既有针对手写文档的表格结构识别方法，也有针对电子文档或者电子表格的结构识别方法，其中有2篇基于一些规则和机器学习方法，其余均使用了各种类型的深度学习方法。此外，有一篇论文则是使用语义分割技术同时进行了表格检测与结构识别任务。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/79/2a/v89YGfvV_o.png"></p> 
 <p style="text-align: center">图8 论文[8]一文中进行电子表格中的表格识别的流程</p> 
 <p>在《A Genetic-based Search for Adaptive Table Recognition in Spreadsheets》[8]一文中，作者对电子表格进行结构识别。首先，作者将电子表格中的单元格分类为不同标签，包括Header、Data和Metadata（忽略），然后相邻单元格根据标签异同组成不同的区域，这些区域根据相邻关系则构成了一个标签区域图。作者正是在这个图的基础上进行表格结构识别任务的，这时，表格结构识别任务仅剩下将图划分为不同的表格区域这一个部分了，变成了子图分割任务，如图8所示。作者定义了10个衡量方法来衡量某一种分割的好坏，并将它们进行加权求和。之后对于每一种分割，使用序列二次规划的方法来自动调节权重，以达到最优。在这里，作者使用了遗传算法来查找边数较多的图的最优分割方案，作者将每一条边视为一个布尔值，真表示这条边存在，假表示不存在，从而得到遗传算法中的个体向量。作者还通过一些启发式方法预先找出一些种子个体向量添加到遗传算法的输入中，将它视为一个比较好的候选解，并参与到迭代过程中，从而减少了迭代代数。对于边数较少的图，作者直接使用穷举搜索来查找。作者最终在从ENRON语料中生成的数据集上进行测试，训练数据集中包含一部分随意选择性标错的噪声数据项，以此保证算法的鲁棒性。作者将预测结果与Ground Truth的IoU超过0.9的表格视为成功识别出的表格，并计算最终的准确率，达到了89.6%的准确率，并验证了遗传算法、预设种子以及故意制造噪声训练数据对性能提升的有效性。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/d5/ba/DizR3NFB_o.png"></p> 
 <p style="text-align: center">图9 论文[2]中用文本与分隔符构造的示意图</p> 
 <p style="text-align: center">黑细线是表格真实的单元格边界。蓝色框为文本区域，被B、I、O标注。绿实线为候选行分隔符，被圆圈中的S、I、O标注。虚线为它们之间的边。</p> 
 <p>《Table Row Segmentation》[2]一文针对手写历史文档中的表格进行了行分割，以此对表格结构进行识别。作者首先找出可能的行分隔符候选项，然后再使用机器学习方法从中找出正确的那些候选项，以此对表格进行行分割。作者尝试了三种行分隔符，包括任意生成的水平线组成的“栅格分隔符”，可以将文本区域分为上下两个部分的“二分分隔符”和有一定倾斜角并且不会穿过文本基线的“倾斜分隔符”。作者首先使用基于规则的方法得到上述行分隔符的候选，然后将它们和文本区域构成一个图，文本区域和分隔符作为顶点，文本区域之间如果没有其他文本阻挡，则他们之间存在一条边，而文本区域和分隔符、分隔符和分隔符之间如果距离不超过一个预设的视觉范围，则也存在一条边，顶点和边的特征主要是一些几何特征。最终，作者使用条件随机场来对顶点进行分类，文本区域包括三个标签：B-某个单元格的开始，I-某个单元格的内部内容，O-表格区域外的其他文本；分隔符也包括三个标签：S-真正的行分隔符，I-在表格内部但不是一个合理的分隔符候选，O-表格区域外的分隔符候选。一个构建并标注完成的图如图9所示。在最终实验里，作者在ABP和NAF两个数据集上进行了测试，最终发现倾斜分隔符效果最好，对这些标签分类的F1值大部分均超过90%，甚至接近100%，一个原因是倾斜的文本行在手写文档中出现频率较高。此外，在两个数据集上进行了IoU阈值为0.8/1的行分割性能评测实验，F1值分别达到86%/78%和79%/72%。但作者也阐述了方法的局限性，就是无法处理跨行合并的单元格的情况，以及可以对方法中的超参数进行优化以达到更好的效果。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/32/ce/upXDTse0_o.png"></p> 
 <p style="text-align: center">图10 SPLERGE方法[10]整体架构</p> 
 <p>在《Deep Splitting and Merging for Table Structure Decomposition》[10]一文中，作者提出了一种先自顶向下、再自底向上的两阶段表格结构识别方法SPLERGE，分为Split和Merge两个部分，整体架构如图10所示。Split部分先把整个表格区域分割成表格所具有的网格状结构，该部分由图11所示的深度学习模块组成两个独立的模型，分别预测表格区域的行分割和列分割情况。</p> 
 <p><img src="https://images2.imgbox.com/a0/ab/4NAcCGig_o.png"></p> 
 <p style="text-align: center">图 11 Split模型中每个模块的架构</p> 
 <p style="text-align: center">*号部分的结构仅出现在模型的部分模块中。</p> 
 <p>每个模块中，除了常规的多尺度特征提取部分，作者还提出了投影池化（Projection Pooling）操作，它的输出实际上就是求取每一行或列的平均特征值，用于将每一行或列的整体特征整合到原先的局部特征上。最终，模型预测每一行或列像素是否属于单元格间的分隔符区域。而Merge部分则是对Split的结果中的每对邻接网格对进行预测，判断它们是否应该合并。这里作者尝试了深度学习方法和启发式的方法，发现两者在不同的数据集上各有千秋。该模型最终在ICDAR2013表格竞赛表格结构识别子任务的数据集上取得了State-of-the-art的效果，预测的单元格对与Ground truth匹配的F1值达到95.26%，并在作者准备的非公开数据集上也达到95.92%的效果，远远超过复现的已有方法和商业软件的性能。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/ed/4c/X5LXkhZf_o.png"></p> 
 <p style="text-align: center">图12 DeepTabstr[15]中引入可变形卷积后的网络结构</p> 
 <p>在《DeepTabStr:Deep Learning based Table Structure Recognition》[15]一文中，作者首先提出，文档图片中表格的位置和大小不同，导致表格特征可能在特征的任意区域以任意大小出现，传统的卷积网络在处理时，会遇到问题。因此，作者将变形卷积的概念引入，用来解决表格的检测问题。由于基于FCN的语义分割类方法，最终结果严重依赖于后处理的方案，因此作者舍弃此类方案，将表格结构检测视为一个对象检测问题，将表格的行和列当做是要检测的对象。变形卷积网络加入了各个像素的偏移向量Offset来训练卷积窗口的形状。传统的ROI-pooling层将ROI转换为k*k的固定大小，可变形的ROI-pooling层也引入了额外的偏移量，使得ROI-pooling层也具有了变形的属性，以适应不同区域的对象检测。本文表格结构识别方法的整体结构如图12所示。此外，为了弥补表格结构识别数据的不足，本文提出了一个基于ICDAR2017的表格行列结构数据集TabStructDB。作者分别用Faster R-CNN、FPN、RFCN进行了实验，并在ICDAR2013和TabStructDB上进行了训练和测试，在ICDAR2013数据集上可以达到F1-Score为93%的效果。</p> 
 <p>《ReS2TIM: Reconstruct SyntacticStructures from Table Images》[4]一文则是重点关注了单元格检测定位后的表格重建工作。作者先将各个单元格之间定义为上下、左右相邻的关系，使用一个单元格关系判别网络来判断任意两个单元格的相邻关系。给定带有单元格边界框的表格图像，关系判别网络将单元格深度数据特征和空间特征进行了连接，作为关系对的联合特征，再判断单元格之间的关系。网络整体结构如图13。而对于一个表格来说，具有相邻关系的单元格占比极少，这会极大地影响网络的效果，因此作者又提出了基于距离的损失权重。在设计损失函数时，按照单元格的距离来设定对应损失权重，距离越远，损失权重越小。在判断完单元格之间的关系之后，根据相邻关系构建出对应的图模型。再根据图模型，按照单元格的上下相邻关系，使用Dijkstra算法确定原表格的行和列的最大数量。之后确定表格内每个单元格的起始的行和列以及跨行跨列的数量。作者在CMDD数据集和ICDAR2017数据集上进行了实验，在CMDD数据集单元格关系的判定任务上，F1-score达到了99.8%的效果。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/16/c4/mMj6bZnf_o.png"></p> 
 <p style="text-align: center">图13 Res2Tim[4]中的单元格关系判别网络结构</p> 
 <p>《Rethinking Semantic Segmentationfor Table Structure Recognition in Documents》[14]一文将表格结构的识别定义为语义分割问题，使用FCN网络框架，对表格的行和列分别进行预测。同时基于表格的一致性假设，介绍了一种对预测结果进行切片的方法，降低了表格识别的复杂度。作者使用了FCN的Encoder和Decoder的结构模型，并加载了在ImageNet预训练好的模型。图片经过模型生成了与原图大小相同的特征，切片过程将特征按照行和列进行平均，将H*W*C（高*宽*Channel）的特征合并成了H*C和W*C大小特征，对这些特征进行卷积后，再进行复制，扩展为H*W*C的大小，再通过卷积层得到每个像素点的标签。最后进行后处理得到最终的结果。整体的框架如图14所示。文章在ICDAR2013数据集上进行了实验，在IoU为0.5的情况下，取得了F1-score为93.42%的效果。然而本文假设表格中所有的单元格不存在跨行跨列，每行每列都从表格的最左侧和最上端开始，到最右侧和最下端结束，因此本方法还存在局限。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/bd/99/nW0YNyzZ_o.png"></p> 
 <p style="text-align: center">图14 论文[14]判别表格行和列的网络结构</p> 
 <p>而《Rethinking Table Recognitionusing Graph Neural Networks》[5]一文则是将GNN应用到了表格结构识别任务中，把视觉特征、位置特征和图中的结构特征进行有效融合。作者使用基于表格区域的建图方法，以OCR识别出的单词区域作为顶点。之后，作者先根据建好的图，从表格图像中提取各个顶点的图像位置特征和CNN提取的视觉特征等特征，然后使用GNN进行特征的交互融合，得到每个顶点的表征特征。作者尝试了两种GNN模型，分别是动态图卷积神经网络DGCNN（Dynamic Graph Convolutional Neural Networks）和GravNet，并且把应用了常规CNN的DenseNet作为基线模型以进行性能对比。训练时随机对顶点对采样，使用DenseNet分别进行是否同行、同列、同单元格的结构关系分类，而在测试时，则对图中每一个顶点对都进行三种分类，得到测试结果。模型整体架构如图15所示。作者将模型在他们自己生成的约有50万个表格的数据集上进行测试，表格被分为4种类别：全线表、无线表、少线表和错切表，其中最后一种是为了模拟由相机等设备获取到的有形变的表格数据而准备的。作者使用完美匹配率来衡量识别效果，模型预测的三种结构关系分类全部正确的表格视为完美匹配表格。最终在4种表格数据上测试最好结果依次为96.9%、94.7%、52.9%和68.5%，均为使用DGCNN得到的结果，可见GNN的确可以产生更好的效果，然而少线表和错切表仍然是难点。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/eb/c3/GgNn7808_o.png"></p> 
 <p style="text-align: center">图15 论文[5]模型的整体架构</p> 
 <p style="text-align: center">由特征提取，特征交互整合，结构关系分类三个部分组成。</p> 
 <p>与大部分的基于CNN的方法不一样，《TableStructure Extraction with Bi-directional Gated Recurrent Unit Networks》[9]中则是针对单元格在行列上具有重复性的序列特征这个特点，提出使用循环神经网络来进行表格结构识别任务。该文作者同样是使用两个独立的模型来进行行列分割，整体架构如图16所示。针对不同的分割任务，首先使用类似的预处理操作使得表格区域变为一种对于深度学习网络来说更容易处理的形式，包括去除非文本前景对象、二值化和水平或竖直的膨胀操作，膨胀操作是为了使得图像中的行或列特征更明显。然后，将预处理结果按像素行或列放入独立的两个两层双向循环神经网络，以同时将某个像素行或列的相邻两个邻居考虑进去。接着将循环神经网络的输出行列特征分类为是否属于行列分隔符区域，最终把预测分隔区域的中点作为最终的行列分割结果。作者尝试了LSTM和GRU这两个经典循环神经网络模型，发现GRU在实验效果上更有优势。最后，作者在UNLV和ICDAR2013表格竞赛表格结构识别子任务的数据集上进行测试，都超过了之前方法中的最好结果，其中在ICDAR2013数据集上单元格关系匹配F1值达到93.39%。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/61/f5/N8tllXkT_o.png"></p> 
 <p style="text-align: center">图 16 论文[9]使用方法的整体架构</p> 
 <p style="text-align: center">分为行分割（左）和列分割（右）两个独立的部分。</p> 
 <p>最后，在《TableNet: Deep Learning Model for End-to-end Table Detection and Tabular Data Extraction from Scanned Document Images》[13]一文中，作者则使用深度学习模型同时解决表格检测和表格结构识别两个任务。作者提出了一种端到端的、多任务的、基于编解码器的图像语义分割模型TableNet，整体架构类似于U-Net，如图17所示。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/c9/86/QkwgQveI_o.png"></p> 
 <p style="text-align: center">图17 (a) 一个简单的根据语义信息高亮过的文档示意；(b) TableNet[13]模型整体架构。</p> 
 <p>编码器阶段使用了ImageNet上预训练的VGG-19模型来提取特征，而解码器阶段则分成两个分支，分别上采样恢复到原图大小并最终得到表格和表格区域中列分割的mask图。下采样中对应大小的特征图被添加到上采样对应层的特征图中，以恢复最终图像中的位置信息。最终，再使用基于规则的方法将列分割结果处理为最终的表格单元格邻接关系结果，得到表格真正的逻辑结构。多任务模型有利于融合表格分割和表格列分割任务中涉及到的信息或特征，以产生相互促进性能提升的效果。此外，作者还尝试将OCR识别出的文本区域的数据类型这种语义特征添加到输入中，采用的做法是使用正则表达式简单对文本区域进行数据类型匹配分类，然后对于不同数据类型的文本区域添加上不同的、独有的颜色高亮背景。作者在Marmot和ICDAR2013表格竞赛数据集上训练，然后在ICDAR2013表格竞赛数据集上进行测试，实验结果也达到了非常好的水平，表格检测和结构识别任务上最好的F1值分别达到96.62%和91.51%，证明了模型的有效性，同时也通过对比实验证明了语义信息和在目标数据集上进一步微调对性能有提升作用。作者最后提出之后可以将行分割任务也结合进来，或者使用更多的其他语义信息。</p> 
 <p>（未完待续...)</p> 
 <p><strong>参考文献</strong></p> 
 <p>[1] L.Melinda and C. Bhagvati, "Parameter-free table detection method," in the 15th IAPR International Conference onDocument Analysis and Recognition, 2019.</p> 
 <p>[2] J.-L. Meunier and H. Déjean,"Table Rows Segmentation," in the15th IAPR International Conference on Document Analysis and Recognition,2019.</p> 
 <p>[3] N. Sun, Y. Zhu, and X. Hu,"Faster R-CNN Based Table Detection Combining Corner Locating," in the 15th IAPR International Conference on Document Analysis and Recognition, 2019.</p> 
 <p>[4] W. Xue, Q. Li, and D. Tao,"ReS2TIM: Reconstruct Syntactic Structures from Table Images," in the 15th IAPR International Conference onDocument Analysis and Recognition, 2019.</p> 
 <p>[5] S. R. Qasim, H. Mahmood, and F.Shafait, "Rethinking Table Recognition using Graph Neural Networks,"in the 15th IAPR International Conferenceon Document Analysis and Recognition, 2019.</p> 
 <p>[6] Y. Deng, D. Rosenberg, and G. Mann,"Challenges in end-to-end neural scientific table recognition," in the 15th IAPR International Conference on Document Analysis and Recognition, 2019.</p> 
 <p>[7] Y. Li, Q. Yan, Y. Huang, L. Gao, andZ. Tang, "A GAN-based Feature Generator for Table Detection," in the 15th IAPR International Conference onDocument Analysis and Recognition, 2019.</p> 
 <p>[8] E. Koci, M. Thiele, O. Romero, and W.Lehner, "A Genetic-based Search for Adaptive Table Recognition in Spreadsheets," in the 15th IAPR International Conference on Document Analysis and Recognition, 2019.</p> 
 <p>[9]  S. A. Khan, S. M. D. Khalid, M. A.Shahzad, and F. Shafait, "Table Structure Extraction with Bi-directional Gated Recurrent Unit Networks," in the15th IAPR International Conference on Document Analysis and Recognition,2019.</p> 
 <p>[10] C. Tensmeyer, V. I. Morariu, B. Price,S. Cohen, and T. Martinez, "Deep Splitting and Merging for Table Structure Decomposition," in the 15th IAPR International Conference on Document Analysis and Recognition, 2019.</p> 
 <p>[11] E. Koci, M. Thiele, J. Rehak, O.Romero, and W. Lehner, "DECO: A Dataset of Annotated Spreadsheets forLayout and Table Recognition," in the15th IAPR International Conference on Document Analysis and Recognition,2019.</p> 
 <p>[12] P. Riba, A. Dutta, L. Goldmann, A.Forn´es, O. Ramos, and J. Llad´os, "Table Detection in Invoice Documents by Graph Neural Networks," in the15th IAPR International Conference on Document Analysis and Recognition,2019.</p> 
 <p>[13] S. Paliwal, V. D, R. Rahul, M. Sharma,and L. Vig, "TableNet: Deep Learning model for end-to-end Table detectionand Tabular data extraction from Scanned Document Images," in the 15th IAPR International Conference on Document Analysis and Recognition, 2019.</p> 
 <p>[14] S. A. Siddiqui, P. I. Khan, A. Dengel,and S. Ahmed, "Rethinking Semantic Segmentation for Table Structure Recognition in Documents," in the15th IAPR International Conference on Document Analysis and Recognition,2019.</p> 
 <p>[15] S. A. Siddiqui, I. A. Fateh, S. T. R.Rizvi, A. Dengel, and S. Ahmed, "DeepTabStR: Deep Learning based TableStructure Recognition," in the 15th IAPRInternational Conference on Document Analysis and Recognition, 2019.</p> 
 <p>[16] T. Kieninger and A. Dengel, "Apaper-to-HTML table converting system," in Proceedings of document analysis systems (DAS), 1998, vol. 98.</p> 
 <p>[17] T. Kieninger and A. Dengel,"Applying the T-RECS table recognition system to the business letter domain," in Proceedings of SixthInternational Conference on Document Analysis and Recognition, 2001, pp.518-522: IEEE.</p> 
 <p>[18] B. Yildiz, K. Kaiser, and S. Miksch,"pdf2table: A method to extract table information from pdf files," in IICAI, 2005, pp. 1773-1785.</p> 
 <p>[19] T. Hassan and R. Baumgartner,"Table recognition and understanding from pdf files," in Ninth International Conference on Document Analysis and Recognition (ICDAR 2007), 2007, vol. 2, pp. 1143-1147: IEEE.</p> 
 <p>[20] J. Fang, L. Gao, K. Bai, R. Qiu, X.Tao, and Z. Tang, "A table detection method for multipage pdf documents via visual seperators and tabular structures," in 2011 International Conference on Document Analysis and Recognition,2011, pp. 779-783: IEEE.</p> 
 <p>[21] X. Chen, L. Chiticariu, M. Danilevsky,A. Evfimievski, and P. Sen, "A Rectangle Mining Method for Understandingthe Semantics of Financial Tables," in 201714th IAPR International Conference on Document Analysis and Recognition (ICDAR),2017, vol. 1, pp. 268-273: IEEE.</p> 
 <p>[22] E. Koci, M. Thiele, W. Lehner, and O.Romero, "Table recognition in spreadsheets via a graph representation," in 2018 13th IAPR International Workshop on Document Analysis Systems (DAS), 2018, pp.139-144: IEEE.</p> 
 <p>[23] M. Göbel, T. Hassan, E. Oro, and G.Orsi, "ICDAR 2013 table competition," in 2013 12th International Conference on Document Analysis and Recognition,2013, pp. 1449-1453: IEEE.</p> 
 <p>[24] L. Gao, X. Yi, Z. Jiang, L. Hao, and Z.Tang, "Icdar2017 competition on page object detection," in 2017 14th IAPR International Conference onDocument Analysis and Recognition (ICDAR), 2017, vol. 1, pp. 1417-1422:IEEE.</p> 
 <p>[25] Y. Huang et al., "A YOLO-based Table Detection Method," in the 15th IAPR International Conference on Document Analysis and Recognition, 2019.</p> 
 <p>[26] L. Gao et al., "ICDAR 2019 Competition on Table Detection and Recognition (cTDaR)," in the 15thIAPR International Conference on Document Analysis and Recognition, 2019.</p> 
 <hr> 
 <hr> 
 <p style="text-align: center">撰稿：黄一伦、李一博</p> 
 <p style="text-align: center">推荐委员：高良才</p> 
 <p style="text-align: center">编排：高 学</p> 
 <p style="text-align: center">审校：殷 飞</p> 
 <p style="text-align: center"> 发布：金连文 </p> 
 <p style="text-align: left"><strong>免责声明：</strong>（1）本文仅代表撰稿者观点，个人理解及总结不一定准确及全面，论文完整思想及论点应以原论文为准。（2）本文观点不代表本公众号立场。 </p> 
 <hr> 
 <p style="text-align: center"><strong>OCR交流群</strong><br></p> 
 <p style="text-align: left">关注最新最前沿的文本检测、文本识别、文字编辑技术，扫码添加CV君拉你入群，（如已为CV君其他账号好友请直接私信）</p> 
 <p style="text-align: center"><strong>（请务必注明：OCR）</strong></p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/ed/ec/HMGiIq4F_o.png"></p> 
 <p style="text-align: center">喜欢在QQ交流的童鞋，可以加52CV官方<strong>QQ群</strong>：805388940。<br></p> 
 <p style="text-align: center">（不会时时在线，如果没能及时通过验证还请见谅）<br></p> 
 <hr> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/4a/9d/X8SjmyvM_o.png"></p> 
 <p style="text-align: center">长按关注我爱计算机视觉</p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/210f84dc50e7d02a914d81ec7bfaae26/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">个人NAS家庭服务器解决方案概况</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/41e5554008b585f72f2c2748c6f644e7/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Esxi虚拟机无法上网的问题解决记录</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>