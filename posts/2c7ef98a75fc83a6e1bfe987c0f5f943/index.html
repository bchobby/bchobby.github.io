<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Python 爬虫入门 - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Python 爬虫入门" />
<meta property="og:description" content="文章目录 Python 爬虫入门`requests` 库`beautifulsoup4`库函数`findall()`，`find()`函数`get()` 爬虫实例 1：抓小说爬虫实例 2：抓豆瓣 top 250 的电影信息后记 Python 爬虫入门 Python 的爬虫功能使得程序员可以快速抓取并分析网页中的信息，它实质上是模拟浏览器访问网页。本章主要常用的两个爬虫相关的库requests，beautifulsoup4。若要模拟鼠标点击等，要用到selenium 库，限于篇幅限制，本章不再介绍，具体可以查阅相关资料。
requests 库 每个网页都有源代码，可以通过鼠标单击右键查看网页源代码。网页中的很多信息都在源代码里面，requests 是一个访问网页源代码的库。一般通过 get 函数访问网页，另外一个常用来访问网页的函数是 post，与 get 函数的区别在于 post 能够传递表格或文件到网页所在服务器上。
get(url, [timeout], [headers], [proxies], **kwargs)url网页链接timeout可选参数，请求网页的最大时长，单位为秒headers可选参数，模拟浏览器设置proxies可选参数，代理服务器设置**kwargs其他参数 get 或 post 函数返回一个 Resoponse 对象，该对象包括以下常用的属性或函数。
属性或函数描述status_code网页请求的返回状态，200 表示连接成功，404 表示连接失败text响应网页的字符串内容encoding响应网页的编码方式，可以更改content相应网页的字节形式内容，例如图片或 pdf 文件等raise_for_status()如果网页访问不成功，抛出异常，一般结合 try-except 语句使用json()该函数可以解析网页内容中 JSON 格式的数据 import requests r = requests.get(&#39;http://www.baidu.com&#39;) r.status_code 200 r.text # 结果省略 r.encoding # 若为 ISO-8859-1 则中文为乱码 &#39;ISO-8859-1&#39; r.encoding = &#39;utf-8&#39; # 无论网页原内容是什么编码，都改成 utf-8 编码 beautifulsoup4库 使用requests获取的网页源代码一般非常复杂，不仅包括常规内容，还包括很多定义页面格式的代码。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/2c7ef98a75fc83a6e1bfe987c0f5f943/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-18T12:10:35+08:00" />
<meta property="article:modified_time" content="2023-11-18T12:10:35+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Python 爬虫入门</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#Python__2" rel="nofollow">Python 爬虫入门</a></li><li><ul><li><a href="#requests__6" rel="nofollow">`requests` 库</a></li><li><a href="#beautifulsoup4_84" rel="nofollow">`beautifulsoup4`库</a></li><li><ul><li><a href="#findallfind_281" rel="nofollow">函数`findall()`，`find()`</a></li><li><a href="#get_363" rel="nofollow">函数`get()`</a></li></ul> 
   </li><li><a href="#_1_490" rel="nofollow">爬虫实例 1：抓小说</a></li><li><a href="#_2_top_250__593" rel="nofollow">爬虫实例 2：抓豆瓣 top 250 的电影信息</a></li><li><a href="#_865" rel="nofollow">后记</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="Python__2"></a>Python 爬虫入门</h2> 
<p>Python 的爬虫功能使得程序员可以快速抓取并分析网页中的信息，它实质上是模拟浏览器访问网页。本章主要常用的两个爬虫相关的库<code>requests</code>，<code>beautifulsoup4</code>。若要模拟鼠标点击等，要用到<code>selenium</code> 库，限于篇幅限制，本章不再介绍，具体可以查阅相关资料。</p> 
<h3><a id="requests__6"></a><code>requests</code> 库</h3> 
<p>每个网页都有源代码，可以通过鼠标单击右键查看网页源代码。网页中的很多信息都在源代码里面，<code>requests</code> 是一个访问网页源代码的库。一般通过 get 函数访问网页，另外一个常用来访问网页的函数是 post，与 get 函数的区别在于 post 能够传递表格或文件到网页所在服务器上。</p> 
<table><tbody><tr><th colspan="2">get(url, [timeout], [headers], [proxies], **kwargs)</th></tr><tr><td>url</td><td>网页链接</td></tr><tr><td>timeout</td><td>可选参数，请求网页的最大时长，单位为秒</td></tr><tr><td>headers</td><td>可选参数，模拟浏览器设置</td></tr><tr><td>proxies</td><td>可选参数，代理服务器设置</td></tr><tr><td>**kwargs</td><td>其他参数</td></tr></tbody></table> 
<br> 
<p>get 或 post 函数返回一个 Resoponse 对象，该对象包括以下常用的属性或函数。</p> 
<table><thead><tr><th align="left">属性或函数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">status_code</td><td align="left">网页请求的返回状态，200 表示连接成功，404 表示连接失败</td></tr><tr><td align="left">text</td><td align="left">响应网页的字符串内容</td></tr><tr><td align="left">encoding</td><td align="left">响应网页的编码方式，可以更改</td></tr><tr><td align="left">content</td><td align="left">相应网页的字节形式内容，例如图片或 pdf 文件等</td></tr><tr><td align="left">raise_for_status()</td><td align="left">如果网页访问不成功，抛出异常，一般结合 try-except 语句使用</td></tr><tr><td align="left">json()</td><td align="left">该函数可以解析网页内容中 JSON 格式的数据</td></tr></tbody></table> 
<pre><code class="prism language-python"><span class="token keyword">import</span> requests

r <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'http://www.baidu.com'</span><span class="token punctuation">)</span>
r<span class="token punctuation">.</span>status_code
</code></pre> 
<pre><code>200
</code></pre> 
<pre><code class="prism language-{code-block}python">r.text # 结果省略
</code></pre> 
<pre><code class="prism language-python">r<span class="token punctuation">.</span>encoding  <span class="token comment"># 若为 ISO-8859-1 则中文为乱码</span>
</code></pre> 
<pre><code>'ISO-8859-1'
</code></pre> 
<pre><code class="prism language-python">r<span class="token punctuation">.</span>encoding <span class="token operator">=</span> <span class="token string">'utf-8'</span> <span class="token comment"># 无论网页原内容是什么编码，都改成 utf-8 编码</span>
</code></pre> 
<h3><a id="beautifulsoup4_84"></a><code>beautifulsoup4</code>库</h3> 
<p>使用<code>requests</code>获取的网页源代码一般非常复杂，不仅包括常规内容，还包括很多定义页面格式的代码。</p> 
<pre><code class="prism language-{admonition}">:class: tip
- 网页中的内容一般在网页源代码的各个标签里
</code></pre> 
<p>例如有下面的 html 代码：</p> 
<pre><code class="prism language-python">html_doc <span class="token operator">=</span> <span class="token triple-quoted-string string">"""
&lt;html&gt;
    &lt;body&gt;
        &lt;h1&gt;Hello, BeautifulSoup!&lt;/h1&gt;
        &lt;ul&gt;
            &lt;li&gt;&lt;a class = "c1" href="http://example1.com"&gt;Link 1&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a class = "c2" href="http://example2.org"&gt;Link 2&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
        &lt;div&gt;
        &lt;p&gt;text1&lt;/p&gt; &lt;p&gt;text2&lt;/p&gt;
        &lt;/div&gt;
    &lt;/body&gt;
&lt;/html&gt;
"""</span>
</code></pre> 
<p><code>beautifulsoup4</code>库提供了大量的属性或函数，能够方便地将网页（html）不同标签（tag）中的内容提取出来。常用的属性有下面几个：</p> 
<table><thead><tr><th align="left">属性</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">head</td><td align="left">网页源代码中的 &lt;head&gt; 标签内容</td></tr><tr><td align="left">title</td><td align="left">网页源代码中的 &lt;title&gt; 标签内容</td></tr><tr><td align="left">body</td><td align="left">网页源代码中的 &lt;body&gt; 标签内容</td></tr><tr><td align="left">p</td><td align="left">网页源代码中的第一个 &lt;p&gt; 标签内容</td></tr><tr><td align="left">a</td><td align="left">网页源代码中的第一个 &lt;a&gt; 标签内容</td></tr><tr><td align="left">div</td><td align="left">网页源代码中的第一个 &lt;div&gt; 标签内容</td></tr><tr><td align="left">script</td><td align="left">网页源代码中的第一个 &lt;script&gt; 标签内容</td></tr></tbody></table> 
<pre><code class="prism language-python"><span class="token keyword">from</span> bs4 <span class="token keyword">import</span> BeautifulSoup

r <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'http://www.baidu.com'</span><span class="token punctuation">)</span>
r<span class="token punctuation">.</span>encoding <span class="token operator">=</span> <span class="token string">'utf-8'</span>
soup <span class="token operator">=</span> BeautifulSoup<span class="token punctuation">(</span>r<span class="token punctuation">.</span>text<span class="token punctuation">)</span> <span class="token comment"># 将网页内容传递给 BeautifulSoup 提取</span>
</code></pre> 
<pre><code class="prism language-python">soup<span class="token punctuation">.</span>title
</code></pre> 
<pre><code>&lt;title&gt;百度一下，你就知道&lt;/title&gt;
</code></pre> 
<pre><code class="prism language-python">soup<span class="token punctuation">.</span>p <span class="token comment"># html 语言中 p 标签表示一个文本段落</span>
</code></pre> 
<pre><code>&lt;p id="lh"&gt; &lt;a href="http://home.baidu.com"&gt;关于百度&lt;/a&gt; &lt;a href="http://ir.baidu.com"&gt;About Baidu&lt;/a&gt; &lt;/p&gt;
</code></pre> 
<p>上面的这些标签对象还有自己的属性可以访问更具体的内容。</p> 
<table><thead><tr><th align="left">标签属性</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">name</td><td align="left">标签名字，例如 a，p，div等</td></tr><tr><td align="left">attrs</td><td align="left">标签的具体属性</td></tr><tr><td align="left">contents</td><td align="left">第一个该标签下的所有内容，为列表形式</td></tr><tr><td align="left">string</td><td align="left">第一个该标签或子标签下的文本内容，若标签中没有内容或者超过一层的子标签，则返回 None</td></tr><tr><td align="left">text</td><td align="left">第一个该标签下（包括子标签）的所有文本内容，若没有内容则返回空文本</td></tr></tbody></table> 
<pre><code class="prism language-python">soup<span class="token punctuation">.</span>p<span class="token punctuation">.</span>name
</code></pre> 
<pre><code>'p'
</code></pre> 
<pre><code class="prism language-python">soup<span class="token punctuation">.</span>p<span class="token punctuation">.</span>attrs
</code></pre> 
<pre><code>{'id': 'lh'}
</code></pre> 
<pre><code class="prism language-python">soup<span class="token punctuation">.</span>p<span class="token punctuation">.</span>contents
</code></pre> 
<pre><code>[' ',
 &lt;a href="http://home.baidu.com"&gt;关于百度&lt;/a&gt;,
 ' ',
 &lt;a href="http://ir.baidu.com"&gt;About Baidu&lt;/a&gt;,
 ' ']
</code></pre> 
<pre><code class="prism language-python">soup<span class="token punctuation">.</span>p<span class="token punctuation">.</span>string
</code></pre> 
<pre><code class="prism language-python">soup<span class="token punctuation">.</span>p<span class="token punctuation">.</span>text
</code></pre> 
<pre><code>' 关于百度 About Baidu '
</code></pre> 
<p>将挖网页内容传递给 BeautifulSoup 提取时，还可以指定解析器，例如 ‘html.parser’, ‘lxml’, ‘xml’, ‘html5lib’。这些解析器各有利弊，其中 ‘lxml’ 解析速度最快。将上面的 html_doc 的网页文件传递给 BeautifulSoup：</p> 
<pre><code class="prism language-python">soup2 <span class="token operator">=</span> BeautifulSoup<span class="token punctuation">(</span>html_doc<span class="token punctuation">,</span> <span class="token string">'lxml'</span><span class="token punctuation">)</span> <span class="token comment"># 或者 soup2 = BeautifulSoup(html_doc, features = 'lxml')</span>
</code></pre> 
<pre><code class="prism language-python">soup2<span class="token punctuation">.</span>ul<span class="token punctuation">.</span>text
</code></pre> 
<pre><code>'\nLink 1\nLink 2\n'
</code></pre> 
<pre><code class="prism language-python"><span class="token builtin">type</span><span class="token punctuation">(</span>soup2<span class="token punctuation">.</span>ul<span class="token punctuation">.</span>string<span class="token punctuation">)</span> <span class="token comment"># 文本内容嵌套超过一层，返回 None</span>
</code></pre> 
<pre><code>NoneType
</code></pre> 
<pre><code class="prism language-python">soup2<span class="token punctuation">.</span>li<span class="token punctuation">.</span>contents <span class="token comment"># html 语言中 a 标签是超链接，ul 与 li 标签表示一个无序列表</span>
</code></pre> 
<pre><code>[&lt;a class="c1" href="http://example1.com"&gt;Link 1&lt;/a&gt;]
</code></pre> 
<pre><code class="prism language-python">soup2<span class="token punctuation">.</span>ul<span class="token punctuation">.</span>li<span class="token punctuation">.</span>text <span class="token comment"># 标签通过逗点套子标签</span>
</code></pre> 
<pre><code>'Link 1'
</code></pre> 
<h4><a id="findallfind_281"></a>函数<code>findall()</code>，<code>find()</code></h4> 
<p>当需要列出同一类标签对应的所有内容时，需要用到 BeautifulSoup 中的 findall() 函数。</p> 
<table><tbody><tr><th colspan="2">find_all([name], [attrs], [string], [limit], **kwargs)</th></tr><tr><td>name</td><td>标签名字</td></tr><tr><td>attrs</td><td>按照标签的具体属性检索, 采用 class_= 形式或 JSON 格式等</td></tr><tr><td>string</td><td>按照关键字检索，采用 string= 形式，返回与关键字完全匹配的字符串</td></tr><tr><td>limit</td><td>返回结果的个数，默认返回所有结果</td></tr><tr><td>**kwargs</td><td>其他参数</td></tr></tbody></table> 
<br> 
<p>还有一个函数<code>find()</code>，与<code>findall()</code>的区别在于<code>find()</code>只寻找第一个对应标签的内容。还有一个函数<code>find_next()</code>，可以查找标签的下一个标签。类似的检索函数还有<code>select</code>，本书不再赘述。</p> 
<pre><code class="prism language-python">soup2<span class="token punctuation">.</span>find_all<span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">)</span> <span class="token comment"># 检索出网页中所有的 a 标签</span>
</code></pre> 
<pre><code>[&lt;a class="c1" href="http://example1.com"&gt;Link 1&lt;/a&gt;,
 &lt;a class="c2" href="http://example2.org"&gt;Link 2&lt;/a&gt;]
</code></pre> 
<pre><code class="prism language-python">soup2<span class="token punctuation">.</span>find_all<span class="token punctuation">(</span><span class="token string">'a'</span><span class="token punctuation">,</span> class_ <span class="token operator">=</span> <span class="token string">'c1'</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code>[&lt;a class="c1" href="http://example1.com"&gt;Link 1&lt;/a&gt;]
</code></pre> 
<pre><code class="prism language-python">soup2<span class="token punctuation">.</span>find_all<span class="token punctuation">(</span>class_ <span class="token operator">=</span> <span class="token string">'c1'</span><span class="token punctuation">)</span> <span class="token comment"># 参数中可以没有标签名字</span>
</code></pre> 
<pre><code>[&lt;a class="c1" href="http://example1.com"&gt;Link 1&lt;/a&gt;]
</code></pre> 
<pre><code class="prism language-python">soup2<span class="token punctuation">.</span>find_all<span class="token punctuation">(</span>string <span class="token operator">=</span> <span class="token string">'Link'</span><span class="token punctuation">)</span> <span class="token comment"># 关键词检索</span>
</code></pre> 
<pre><code>[]
</code></pre> 
<h4><a id="get_363"></a>函数<code>get()</code></h4> 
<p>通过函数<code>get</code>可以访问标签的具体属性，例如 class 类型，具体超链接等。</p> 
<pre><code class="prism language-python">soup2<span class="token punctuation">.</span>a<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'class'</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code>['c1']
</code></pre> 
<pre><code class="prism language-python">soup2<span class="token punctuation">.</span>a<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'href'</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code>'http://example1.com'
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">import</span> re <span class="token comment"># 导入正则表达式库</span>

soup2<span class="token punctuation">.</span>find_all<span class="token punctuation">(</span>string <span class="token operator">=</span> re<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span><span class="token string">'Link'</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 通过正则表达式将所有包含 Link 的字符串都检索出来</span>
</code></pre> 
<pre><code>['Link 1', 'Link 2']
</code></pre> 
<pre><code class="prism language-{note}">正则表达式是计算机科学中用来描述字符串的一种表达式。正则表达式定义一个字符串表达规则，只要字符串满足这个规则，就算则匹配。可以通过字符串结合符号 .*?&lt;=()[]{}dw 等定义多种表达规则，具体可以进一步查阅相关资料。
</code></pre> 
<p>BeautifulSoup 另外一个常用的获取文本内容的函数为<code>get_text</code>。与直接使用标签的属性 text 相比，<code>get_text</code> 更加灵活，能够方便地实现文本换行。它的一般用法为：</p> 
<table><tbody><tr><th colspan="2">get_text([separator=''], [strip=False], **kwargs)</th></tr><tr><td>separator</td><td>各标签下文本的分隔符</td></tr><tr><td>strip</td><td>是否去掉各标签文本的前后空格</td></tr><tr><td>**kwargs</td><td>其他参数</td></tr></tbody></table> 
<br> 
<pre><code class="prism language-python">soup2<span class="token punctuation">.</span>div<span class="token punctuation">.</span>text <span class="token comment"># html 语言中 div 标签表示网页中的一个区域</span>
</code></pre> 
<pre><code>'\ntext1 text2\n'
</code></pre> 
<pre><code class="prism language-python">soup2<span class="token punctuation">.</span>div<span class="token punctuation">.</span>get_text<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code>'\ntext1 text2\n'
</code></pre> 
<pre><code class="prism language-python">soup2<span class="token punctuation">.</span>div<span class="token punctuation">.</span>get_text<span class="token punctuation">(</span>separator <span class="token operator">=</span> <span class="token string">'\n'</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code>'\n\ntext1\n \ntext2\n\n'
</code></pre> 
<pre><code class="prism language-python">soup2<span class="token punctuation">.</span>div<span class="token punctuation">.</span>get_text<span class="token punctuation">(</span>separator <span class="token operator">=</span> <span class="token string">'**'</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code>'\n**text1** **text2**\n'
</code></pre> 
<pre><code class="prism language-python">soup2<span class="token punctuation">.</span>div<span class="token punctuation">.</span>get_text<span class="token punctuation">(</span>strip <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code>'text1text2'
</code></pre> 
<h3><a id="_1_490"></a>爬虫实例 1：抓小说</h3> 
<p>下面，我们举例如何用爬虫抓取网上的《三国演义》小说，并将每回的内容下载到电脑里。</p> 
<p>首先，找到一个包含《三国演义》小说的网站，本例中，使用了古诗文网：https://so.gushiwen.cn/guwen/book_46653FD803893E4F7F702BCF1F7CCE17.aspx</p> 
<p>然后，在网页的空白处，单击鼠标右键，查看网页源代码：</p> 
<p><img src="https://images2.imgbox.com/e5/f7/dnWjUXnY_o.png" alt="在这里插入图片描述"></p> 
<p>网页的源代码大致如下：</p> 
<p><img src="https://images2.imgbox.com/b8/ad/aOEmE1na_o.png" alt="在这里插入图片描述"></p> 
<ul><li>我们发现每一回的链接在网页中的标签 ul 里面，具体在其每个子标签 span 中的子标签 a 里面。</li></ul> 
<p>打开其中任一回的链接，在网页空白处单击右键，查看网页源代码：</p> 
<p><img src="https://images2.imgbox.com/d2/0f/nbQEQ6pw_o.png" alt="在这里插入图片描述"></p> 
<ul><li>我们发现每一回的内容在具体属性为 class=‘contson’ 的 div 标签里，其中第一句为每一回的标题，而之后的内容为正文。</li></ul> 
<p>根据网页的以上两个特点，设计 Python 的爬虫代码如下。</p> 
<pre><code class="prism language-{code-block}python">import requests  # 联系网络的包，a package for requesting from websites
from bs4 import BeautifulSoup  # 分析网页数据的包，a package for webstie data analysis
import time
import random
import os


# 获取单个网页信息
def get_url_content(url):
    # headers = {
    #     'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.108 Safari/537.36'
    # }  # 主要是模拟浏览器登录，防止反爬，一般的网站没有这个也行，谷歌浏览器 按 F12 能够找到网站支持的 headers
    r = requests.get(url, timeout=30)  # 获取网页的内容，并返回给r变量，timeout 为超时时间
    # r.raise_for_status()  # 检查返回网页的内容的状态码，200表示成功
    # r.encoding = r.apparent_encoding  # 统一编码方式
    return r.text  # 返回网页中的文本内容，数据内容为 r.content


# 解析出网页中想要的信息，爬虫的关键步骤
def filter_info(chapter_num, url_text):
    soup = BeautifulSoup(url_text, "lxml")  # 解析网页返回内容，lxml 是一个解码方式，效率比较快，被推荐使用
    contents = soup.find('div', class_='contson')
    # 使用 get_text 可以实现网页里面 p 标签中文本内容的换行，而 text 不能
    chapter_title = contents.find('p').get_text()  # 第一句话是本回的标题
    chapter_title = '第' + str(chapter_num) + '回 ' + chapter_title.lstrip()
    # chapter_content = contents.get_text(separator="\n")  # find 返回的不是列表，不用跟 [0]
    chapter_content = contents.text
    chapter_content = chapter_content.lstrip() # 去掉字符串左边的空字符
    this_chapter = [chapter_title, chapter_content]
    return this_chapter

# 从网页中找到每一回的链接
def get_url_links(url_head_content):
    soup = BeautifulSoup(url_head_content, "lxml")  # 解析网页返回内容，lxml 是一个解码方式，效率比较快，被推荐使用
    # links = soup.find('div', class_='bookcont')  # 每一回的链接都在类 span 里面
    links = soup.find('ul')  # 每一回的链接都在类 ul 里面
    links = links.findAll('span')
    link_list = []
    for each in links:
        link_list.append(each.a.get('href'))  # 获取每一回的链接，存储到列表里
    return link_list

# 将每章内容输出到 txt 文档里
def write_txt(string_array):
    file_address = 'E:/三国演义/'  # txt 存放地址
    if not os.path.exists(file_address):       #用os库判断对应文件夹是否存在
        os.makedirs(file_address)              #如果没有对应文件夹则自动生成 
    file_name = string_array[0]
    with open(file_address + file_name + '.txt', 'w', encoding='utf-8') as f: # 必须跟解码形式，不然有的网页中文内容写不到txt里
        f.write(string_array[1])

# 主函数
def main():
    url = 'https://so.gushiwen.cn/gushi/tangshi.aspx'  # 古诗文网三国演义网址
    url_head_content = get_url_content(url)  # 获取网页
    links = get_url_links(url_head_content)  # 获取每一回的链接地址
    # enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列
    # 同时列出数据下标和数据
    for index, each in enumerate(links):
        url_link_content = get_url_content(each)  # 获取每一回的网页内容
        chapter_content = filter_info(index + 1, url_link_content)  # 解析每一回的网页内容，获取小说文本
        write_txt(chapter_content)  # 输出小说内容到 txt
        time.sleep(random.random() * 1)  # 每抓一个网页休息0~1秒，防止被反爬措施封锁 IP

# 运行函数
main()
</code></pre> 
<p>其中，使用 requests 库时，有的网站需要设置 headers 参数才能正确模拟浏览器登录。headers 参数可以在打开网页时，按键盘<code>F12</code> 打开， 然后刷新一下网页，单击<code>F12</code>弹出窗口的任何一个文件，就能查看网站的所有 headers 信息，有 cookie，user-agent 等信息。如下图所示：</p> 
<p><img src="https://images2.imgbox.com/31/e4/KLDkaN45_o.png" alt="在这里插入图片描述"></p> 
<p>我们可以复制一部分信息或全部信息，放到 request 函数的 headers 参数里面。</p> 
<h3><a id="_2_top_250__593"></a>爬虫实例 2：抓豆瓣 top 250 的电影信息</h3> 
<p>我们想获取豆瓣高分评价的 250 部电影的信息：电影名字，导演，上映年代，评分，评分人数等。打开豆瓣 top 250 的网页链接 https://movie.douban.com/top250?start= ，发现每个网页显示 25 部电影，一共有 10 个挖网页，网页地址与链接中的 start = 后面的数字有关。例如第 3 个子网页的链接地址是 'https://movie.douban.com/top250?start=50 ，根据这个规律，我们可以用一个 for 循环，将 10 个网页分别打开抓取信息。</p> 
<p>打开一个网页，单击右键查看网页源代码，如下图所示：</p> 
<p><img src="https://images2.imgbox.com/cb/60/kRD07Sqf_o.png" alt="在这里插入图片描述"></p> 
<p>通过源代码发现以下几个特点：</p> 
<ul><li> <p>每部电影的信息都在标签 &lt;div class=‘item’&gt; 里面。据此，我们可以用一个 for 循环，将每个网页的 &lt;div class=‘item’&gt; 标签都检索出来</p> </li><li> <p>电影的中英文名字都在标签 &lt;div class=‘title’&gt; 里，而其他名字在标签 &lt;div class=‘other’&gt; 里。我们可以使用函数<code>find()</code>以及<code>find_next()</code> 检索出来。</p> </li><li> <p>导演，主演，上映年代，电影产地，电影类型信息在标签 &lt;p class=‘’&gt; 里。我们可以使用函数<code>find()</code>检索，然后利用正则表达式将里面对应文本信息分别提取出来。</p> </li><li> <p>评分信息在标签 &lt;div class=‘rating_num’&gt; 里。我们可以使用函数<code>find()</code>检索出来。</p> </li><li> <p>评分人数信息在标签 &lt;span&gt; 里。我们可以通过函数<code>find()</code>结合关键字’评价’检索出来。（网页中的 &lt; span &gt; 提供了一种将文本的一部分或者文档的一部分独立出来的方式）</p> </li><li> <p>代表性评价信息在标签 &lt;div class=‘quote’&gt; 里。我们可以使用函数<code>find()</code>检索出来。</p> </li></ul> 
<p>根据以上的特点，设计出以下的 python 代码。其中中文分词库<code>jieba</code>，词云库<code>WordCloud</code>库等，若没有安装，需要提前安装（使用 pip 或 conda 安装）到电脑上。</p> 
<p>在画图时，使用 matplotlib 画出饼图，使用 WordCloud 生成词云图，因为 matplotlib 不能自动将不同柱子分配不同的颜色，使用 seaborn 画出柱状图。</p> 
<pre><code class="prism language-{code-block}python">import requests  # 联系网络的包，a package for requesting from websites
import pandas as pd
from bs4 import BeautifulSoup # 分析网页数据的包，a package for webstie data analysis
import matplotlib.pyplot as plt # 画图的包
import time 
import random
import jieba # 中文分词包
from wordcloud import WordCloud # 词云包
import re  # 正则表达式包
import seaborn as sns # 用 seaborn 画柱状图

plt.rcParams['font.sans-serif'] = ['SimHei'] 

# 主要是模拟浏览器登录，防止反爬
headers = {
    'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.108 Safari/537.36'
    }

## store the information
rank_list = []
name_list = [] # 用来存储电影名字的一个二维列表
director_list = [] # 用来存储导演名字的一个列表，下面几个变量类似
actor_list = []
year_list = []
rating_list = []
ratingNum_list = []
quote_list = []
place_list = []
category_list = []

num = 0
for i in range(0, 10):
    link = 'https://movie.douban.com/top250?start=' + str(i*25) # 250部电影一共 10个网页， 10 pages for total 250 movies    
    res = requests.get(link, headers = headers, timeout = 10)
    time.sleep(random.random()*3) # 每抓一个网页休息2~3秒，防止被反爬措施封锁 IP，avoid being blocked of the IP address
    
    # res.text is the content of the crawler
    soup = BeautifulSoup(res.text, "lxml")  # lxml 是一个解码方式，lxml is one decoding model for Beautifulsoup
    movie_list = soup.find_all('div', class_ = 'item') # 检索所有类型为 item 的 div 标签
    
    for each in movie_list:
        # 排名在 em 标签里
        rank = each.em.text
        rank_list.append(rank)
        
        # 电影名字在标签 &lt;span class="title"&gt; 与 &lt;span class="other"&gt; 里
        other_name = each.find('span', class_ = 'other').get_text()
        other_name = other_name.replace(' ', '') # 去掉空格
        other_name = other_name.replace('\xa0/\xa0', '') # 去掉多余字符 \xa0/\xa0
        cn_name = each.find('span', class_ = 'title').get_text()
        eg_name = each.find('span', class_ = 'title').find_next().get_text() # find_next() 查找满足条件的下一个标签
        eg_name = eg_name.replace('\xa0/\xa0', '') # 去掉多余字符 \xa0/\xa0
        name_list.append([cn_name, eg_name, other_name])   
              
        # 评分信息在标签  &lt;span class="rating_num"&gt; 里
        rating =  each.find('span', class_ = 'rating_num').get_text()
        rating_list.append(float(rating))
        
        # 评价人数通过关键词'评价'检索
        rating_num = each.find(string = re.compile('评价')).get_text()
        rating_num = rating_num.replace('人评价', '')
        ratingNum_list.append(int(rating_num))
        
        # 代表性评价在  &lt;p class="quote"&gt; 里  
        try:
            quote =  each.find('p', class_ = 'quote').get_text()
        except Exception: # 有的电影代表性评价没有显示
            quote =  ''
        quote_list.append(quote)
        
        info = each.p.get_text(strip = True)
        # 定义正则表达式提取出导演，主演，上映时间，地点，电影类型信息        
        try: 
            # (?&lt;=导演: ) match any character begins after character '导演: '
            # .*? match any character (.), zero or more times (*) but as less as possible (?)
            # (?=主) match any character before character '主'
            director = re.compile('(?&lt;=导演: ).*?(?=主)').findall(info)[0]
            actor = re.compile('(?&lt;=主演: ).*?(?=/)').findall(info)[0]
        except Exception: # 有的电影导演名字太长，主演没有显示出来
            director = re.compile('(?&lt;=导演: ).*?(?=\xa0)').findall(info)[0]
            actor = ''  
        director_list.append(director)
        actor_list.append(actor)
        
        # \d{4} is a four digit number
        year = re.compile('(?&lt;=...)\d{4}').findall(info)[0]
        year_list.append(year)
        place_category = re.compile('(?&lt;=\xa0/\xa0).*').findall(info)[0]
        place_category = place_category.replace('\xa0', '')
        place_category = place_category.split('/')
        place = place_category[0]
        category = place_category[1]
        place_list.append(place)
        category_list.append(category)
          
# 将数据存到 pandas 里
df = pd.DataFrame(rank_list, columns = ['排名'])    
df['电影名字'] = [i[0] for i in name_list]
df['外文名字'] = [i[1] for i in name_list]
df['其他名字'] = [i[2] for i in name_list]
df['评分'] = rating_list
df['评价人数'] = ratingNum_list
df['导演'] = director_list
df['主演'] = actor_list
df['上映日期'] = year_list
df['地区'] = place_list
df['类型'] = category_list
df['代表性评论'] = quote_list

# 导出到 xls 文件里，save to xls file    
df.to_csv('豆瓣 top 250 电影爬虫抓取.csv')

# 分析电影来源地并画饼图
locations = []
for i in range(len(place_list)):
    nations = place_list[i].split(' ') 
    for j in range(len(nations)):
        if nations[j] == '西德':
            nations[j] = '德国'
        locations.append(nations[j])

df_location = pd.DataFrame(locations, columns = ['地区'])
# 按照出现次数排序, size() 可以查数，生成一个 Series 类型
# 然后用 reset_index 重新命名，参数为 name
# DataFrame 类型的 reset_index 参数为 names
df2 = df_location.groupby('地区').size().reset_index(name='counts') # df2 = df2['地区'].value_counts(ascending = False).reset_index()
df2.sort_values(by = 'counts', ascending = False, inplace = True, ignore_index = True)
# 画饼状图
values = []
labels = []
other_count = 0
for i in range(9):
    values.append(df2['counts'][i])
    labels.append(df2['地区'][i])
for i in range(9, df2.shape[0]):
    other_count += int(df2['counts'][i])
values.append(other_count)
labels.append('其他地区')
plt.figure(1)
plt.pie(values, labels=labels, autopct='%.2f%%')
plt.legend()  # 显示标签
plt.show()


# 分析电影类型并画饼图
categories = []
for i in range(len(category_list)):
    category = category_list[i].split(' ') 
    for j in range(len(category)):
        categories.append(category[j])
        
df_category = pd.DataFrame(categories, columns = ['类型'])    
df3 = df_category.groupby('类型').size().reset_index(name='counts') 
df3.sort_values(by = 'counts', ascending = False, inplace = True, ignore_index = True)

values = []
labels = []
other_count = 0
for i in range(15):
    values.append(df3['counts'][i])
    labels.append(df3['类型'][i])
for i in range(15, df3.shape[0]):
    other_count += int(df3['counts'][i])
values.append(other_count)
labels.append('其他类型')
plt.figure(2)
plt.pie(values, labels=labels, autopct='%.2f%%')
plt.legend()
plt.show()

# 画词云图
jieba.add_word('久石让')
jieba.add_word('谢耳朵')
# 一些语气词和没有意义的词
del_words = [ '就是', '一个', '被', '电影', '我们',
              '不是', '每个',  '不会',  '没有', 
              '这样', '那么', '不要', '如果',
              '不能',  '一种', '不过', '只有', '不得不', 
              '不得不', '一部']
all_quotes = ''.join(quote_list) # 将所有代表性评论拼接为一个文本
# 去掉标点符号
all_quotes = re.sub(r"[0-9\s+\.\!\/_,$%^*()?;；:-【】+\"\']+|[+——！，;:。？、~@#￥%……&amp;*（）]+", " ", all_quotes)
words = jieba.lcut(all_quotes)
words_final = []
for i in range(len(words)): 
    if len(words[i]) &gt; 1 and  words[i] not in del_words: # 去掉一些语气词，单字词 
        words_final.append(words[i])
text_result = Counter(words_final)
cloud = WordCloud(
    font_path = 'C:\Windows\Fonts\FZSTK.TTF', # 中文字体地址 C:\Windows\Fonts\FZSTK.TTF，提前下载字体或指定，否则中文无法显示
    # mask = '' 通过 mask 参数指定一个图片地址作为词云的背景图像 
    background_color = 'white',
    width = 1000,
    height = 860,
    max_words = 25   
  )
#wc = cloud.generate(words) # 这种方法对中文支持不太好，this mehtod is better for only english string
wc = cloud.generate_from_frequencies(text_result)
wc.to_file("豆瓣 TOP 250 词云.jpg") 
plt.figure(3)
plt.imshow(wc)
plt.axis('off')
plt.title('豆瓣 TOP 250 电影代表性评论的词云分析')
plt.show()

# 评分最高的 15 部电影
# 用 seaborn 画柱状图，因为自动将不同柱子分配不同的颜色
df_star = df.sort_values(by = '评分', ascending = False, ignore_index = True)
number = 15
df_star_top = df_star.head(number)
plt.figure(4)
sns.barplot(data = df_star_top, y = '电影名字', x = '评分', orient = 'h')
plt.title('评分最高的 ' + str(number) + ' 部电影')
plt.show()

# 评分人数最多的 15 部电影
df_num = df.sort_values(by = '评价人数', ascending = False, ignore_index = True)
df_num_top = df_num.head(number)
plt.figure(5)
sns.barplot(data = df_num_top, y = '电影名字', x = '评价人数', orient = 'h')
plt.title('评分人数最多的 ' + str(number) + ' 部电影')
plt.show()

# 电影年代画图
df_year = df.groupby('上映日期').size().reset_index(name = '部数')
df_year.sort_values('部数', ascending = False, inplace = True)
df_year_top = df_year.head(20)
df_year_top.sort_values('上映日期', inplace = True)
plt.figure(6)
sns.barplot(data = df_year_top, x = '上映日期', y = '部数')
plt.title('电影上映年代')
plt.show()
</code></pre> 
<p>生成的图形如下：<br> <img src="https://images2.imgbox.com/57/74/vGOV3FXz_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/84/25/tMKZm13W_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/49/2f/wR4XNqam_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/d9/f8/m1QgriE6_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/07/7f/hVpXLUNT_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/c1/84/RT56uI0Z_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_865"></a>后记</h3> 
<ul><li>不同的网站的爬虫技巧经常不一样，要具体网站具体分析。</li><li>由于一些网站经常更新数据的存储地址或源代码，某段时间的爬虫代码可能在下一个时间段就不能用了，爬虫代码也要更新。</li><li>有的网页信息并不在网站直接打开的网页源代码里（例如弹幕、电商评论等），还要结合按键<code>F12</code>找到网页信息的真实地址。</li><li>提取网页信息时，经常要用到正则表达式处理字符串。</li></ul> 
<p>近年来，许多网站都提高了反爬手段防止网站的数据被无端获取。因此，提醒读者避免过度使用爬虫，给一些网站造成不必要的访问负担。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/54fe7528a1e035608d73958112732083/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Java 类之 java.lang.reflect.Method</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/82a5d64119417b25043064d04101b978/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">简朴博客系统测试报告</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>