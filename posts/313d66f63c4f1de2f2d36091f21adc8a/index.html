<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>进化算法——昂贵、有噪声与动态适应度函数 - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="进化算法——昂贵、有噪声与动态适应度函数" />
<meta property="og:description" content="1.昂贵适应度函数 在很多实际问题中，对适应度做一次评价会需要几分钟、几小时、几天甚至更长时间的计算或实验。我们在这里讨论如何减少适应度评价所需的时间以便降低进化算法对计算量的要求。
实际问题涉及的适应度函数常常包含下列的一种或多种特征：
做一次适应度函数评价需要数分钟、数小时、数天的时间；不能以并行的方式评价适应度函数；适应度函数评价的次数受到时间或其他资源的限制 下面几种方式有助于减少适应度函数评价的计算量。
对以评价过得个体不再计算其费用。如果个体看起来很好或太差，可以缩短适应度函数评价。如果需要在大量案例测试中评价费用函数，可以用测试案例的一个子集求近似费用。如果在计算机上评价适应度函数，可以采用让软件运行加速的标准方法。 （1）适应度函数的近似 我们可以建立适应度函数模型以减少评价适应度函数的工作。即使计算量不是瓶颈，我们也可以使用适应度函数模型来改善进化算法的性能。我们称这样的模型为代理、响应曲面或元模型。用代理这个词是因为可以用适应度模型临时替换精确的适应度评价。用元模型这个词是因为适应度评价本身也是一个近似值。因此，适应度函数模型是对高阶模型的降阶。
假设有一个适应度函数f(x)，我们已经在M个个体上对它做了评价。可以利用这M个函数值估计搜索空间中任一点的适应度。下图说明适应度估计的基本思路，基于已知的函数值生成一个估计。
几乎所有的近似算法都能用于适应度近似。最简单的一个估计算法是将个体的适应度近似为已经评价过得最近邻居的适应度。这个方法被称为适应度模仿，它退化为适应度景观的分段常值近似。
在上图中，当得到新的数据时应该如何更新适应度估计。我们希望适应度估计算法是迭代的，当有新的适应度信息时就更新。然而，如果适应度景观是动态的，在生成的时候估计算法不能完全相信旧的适应度函数。用新的数据更新适应度近似被称为在线代理更新。
适应度近似的另个一方法基于父代的适应度值为子代分配适应度，它被称为适应度继承。我们可以将子代的适应度近似为父代适应度值的平均，或者它与每一个父代的相似程度取加权平均。这个想法可以推广到子代有任意多父代的算法，也可以将这个思路扩展，让子代的适应度近似为整个群众适应度的加权平均，由子代与种群中已被评价过得每一个个体相似的程度来决定权重。还可以采用更高级的适应度继承，比如考虑独立变量和适应度值之间的相关性。特别地，对于多目标问题，适应度继承仅在帕累托前沿连续且为凸的情况下才有效。
a.多项式模型 适应度模仿的分段常值近似是一个好的起点，它告诉我们如何将适应度近似扩展为高阶多项式。例如，可以将适应度近似为线性函数
其中，n是问题的维数，x(k)是个体x的第k个元素。这是多项式模型的一个简单例子，它也被称为响应曲面，通过解下面的问题可以算出a(k)的值：
其中，M是我们已经确切知道适应度值的个体数，是其中第i个个体，是的第k个元素。欲求出让（2）式最小的（n&#43;1）个参数a(k)，k属于[0,n]，可以用迭代最小二乘法，当得到更多适应度值（M=1,2，以此类推）时更新（2）式的解所需的计算量很少。
我们可以写出一个比（1）式线性模型更精确的模型：
这是一个二次模型，有（n^2&#43;n&#43;1）个参数。此模型关于参数a(k)和a(j，k)仍然是线性的，可以用迭代最小二乘法来求解，只要领会了多项式建模的思想，就可以尝试不同形式的模型，比如：
其中，g(·)和·h(·)可以是任意的线性或非线性函数。
我们也许想用非最小二乘的方法来近似适应度模型，比如，不是通过求解（2）式而是求解下面的问题找出模型：
这里还是（n&#43;1）个参数a(k)上最小化。下图显示了最小化估计误差的平方和与最小最大化估计误差之间的差别。
（2）式的最小二乘准则较好，因为容易用解析方法求解，但最小最大化准则可能更稳健，因为能找出在最坏情况下误差最小的近似。为了在搜索空间的较困难的区域中减小近似误差，最小最大化近似会牺牲掉搜索空间中易于拟合区域的近似误差。
b.计算机实验的设计与分析 计算机实验设计与分析DACE是一个随机的近似方法，它用诊断测试度量近似的好坏。已知n维向量x的M个适应度函数评价，我们假定适应度函数可以近似为
其中，是一个常数（不一定是适应度函数评价的均值），是一个修正项。DACE假定,对所有x修正项是均值为0方差为的高斯分布；也就是说，f(x)的概率密度函数为
DACE的另一个重要假设是，对于x的不同值， 不独立，即若x的值相似，修正项也应该相似。DACE假定和之间的关系数可以表示如下：
其中，是第i个候选解的第k个元素，∈[1,2]和是参数模型，＞0是一个距离度量。若小，和的相关性接近1；若大，和的相关性接近0。已知M个适应度函数评价，我们把它们集中写成向量的形式：
其中，是一个列向量，它的每一个元素都是1.注意，我们用符号f(x)表示单个候选解x的适应度，也表示M个元素的向量，它包含的M个适应度；（9）中M个适应度函数的高斯概率密度哈数为
其中，C是协方差矩阵，方差都为的两个随机变量和之间的协方差可以表示为如下
因此，（10）式可以写成
其中，R是相关矩阵，R的第i行第j列的元素等于。
已知一组候选解和适应度评价f(x)的向量，我们可以找出测量得到的f(x)的值与假定的f(x)的参数形式匹配得最好的和的值。已知f(x)的随机性质，（12）式给出f(x)的概率密度函数与得到具体的f(x)的可能性成正比。因此，为找到f(x)的参数形式与f(x)的测量值之间的最好的拟合，我们要找出让（12）式的PDF(f(x))最大的和。首先考虑关于的偏导数并令它等于0，得到
这里忽略坟墓中的2，因为他独立于，解（13）式，有
取（12）式关于的偏导数，得到
令上面的式子等于0，有
（14）和（16）式给出了用DACE近似适应度函数的和的最优值。
现考虑候选解的M个适应度函数评价f(x)。假设适应度函数相关，如（8）式所示，假设我们得到另一个候选解的适应度函数评价。将适应度函数向量扩大为（M&#43;1）个元素。
新的相关矩阵为
其中，r是M个适应度函数评价f(x)与新增的适应度函数评价的相关性向量。 我们想要关于 最大化（12）式的概率密度函数，由此得到最适合新数据 的形如（16）式的估计。它被称为极大似然估计。通过最大化
就能最大化（12）式的概率密度函数。用矩阵求逆引理可以证明
将它带入（19）式，有
想要最大化这个关于的表达式，就对求导并置为零，即
求，得到
这个式子告诉我们如何利用已有的模型来近似新的点的适应度值。推导出近似的均方误差为
我们利用均方误差可以为更多的适应度评价确定合适的采样点。在搜索空间中有两个区域会特别需要更多的适应度评价。首先，在适应度近似的最小值附近采样以期找到优化问题的更好的解。其次，在均方误差大的区域采样，因为这些区域中有很大的不确定性。
选择采样点以提高建模的准确度这种方式被称为主动学习。主动学习通常意味着学习算法中通过采样点来优化某个费用函数。
只要得到新的候选解，就可以用（23）式计算其适应度近似。
例1，我们用DACE估计二维Branin基准函数
其中，x(1)和x(2)是候选解的两个分量（n=2）。函数的域是x(1)∈[-5,10]，x(2)∈[0.15]。首先要决定用哪些采样点。我们在这里任意选定25个均匀分布于二维搜索域的采样点（M=25）。下面用MATLAB中的fmincon函数关于和最大化（12）式得到
接下来利用（23）式在细网格上近似f(x)。结果如下图所示，这个近似很好的捕捉了基准函数的基本形状，尤其是多峰的特征。
上例用的是均匀采样，用其他的采样方法得到的结果可能效果会更好。常用的采样方法有Latin超立方采样。此方法在每一维上将域分割为多个区间，然后在每一维的每一个区间中只设置一个采样点。与均匀采样相比，有时候它能捕捉函数无法预知的未知特征。下图说明均匀采样和Latin超立方采样的区别。
（a）显示在搜索域中均匀采样的4个点，（b）显示Latin超立方采样。注意，每一行每一列都只有一个点。Latin超立方采样不唯一。 例2，用Latin超立方采样与DACE估计例1的二维函数。我们任意决定用21个采样点（M=21）。用 MATLAB中的fmincon函数关于和最大化（12）式得到
接下来利用（23）式在细网格上近似f(x)。结果如下图所示，看起来用Latin超立方采样的结果比均匀采样的好。事实上，例1中用均匀采样的DACE近似的均方误差是24.9，而用Latin超立方采样的均方误差是14.3。即使采样点少，由Latin超立方采样得到的近似误差比均匀采样几乎好50%。可见采样方法会明显影响DACE的近似结果。
（2）近似变换函数 适应度近似方法有时候表现得不好，（23）式的DACE方法需要对矩阵求逆，但矩阵的逆可能不存在。如果逆不存在，可以用伪逆。不过，这里的要点是用来近似适应度函数的基函数可能与适应度函数的形状不太匹配。例如，用傅里叶函数近似一个不规则并带有尖锐边缘的函数，就不能指望在所有的函数域中的所有点上都有好的近似。在这种情况下，可以先对原始的适应度函数做变换，然后找出变换后的函数的近似。例如，我们对M个采样点评价了它的适应度函数。如果近似效果不好，在变换时可以取适应度函数样本的自然对数：
然后用采样点的找出L(x)的近似，记为。然后通过反变换得到原函数的近似：
例3，我们在Goldstein-Price函数上用DACE方法。
其中，x(1)和x(2)的域为[-2,2]。这个函数在它的最小值附近很平坦，在(1)=0和(2)=1处函数达到最小值，=3.DACE的近似方法对这个平坦的区域无能为力。
因为在平坦的区域中采样点高度相关，这意味着相关矩阵R中的一些列几乎全部都由1组成，R几乎是奇异的。 在本例中我们选用采样点的自然对数，如（29）式所示。函数的形状因此发生了巨大改变；它将靠在一起的较小的f(x)值摊开，将较大的f(x)值聚拢，这样会让相似的函数值分开同时压缩函数的总范围。然后我们用DACE近似L(x)，再计算原始函数的近似，如（30）式所示，凭借对近似过程的简单修改，我们可以得到一个不错的近似，如下图所示。
（3）在进化算法中如何使用适应度近似 这里有几个方案，首先，可以让适应度近似只替换固定比例r的适应度评价。假设适应度函数评价在进化算法的计算量中占据支配地位，这会让计算量从E减少到（1-r）E。但我们不能滥用这个思路，如果用近似替换的适应度评价过多，进化算法的收敛需要更多的时间，这样反而会浪费计算资源。在极端情况下，如果用近似替换所有的适应度评价r=1.此时计算量的确可能近似为0，但进化算法不会收敛到一个有用的结果。
另一个方案是在每一代多生成一些子代并用它们的近似适应度值决定哪些留作下一代。我们称这个想法为进化控制，或模型管理。如果用准确适应度评价某些个体并用近似适应度函数评价其他个体，我们称之为个体进化控制。可以用不同的方法决定哪些个体用准确评价哪些个体用近似评价。比如，对每一个个体随机的决定它的评价方式。也可以在每一代只对近似适应度好的个体用准确适应度评价。用准确适应度评价的个体被称为受控个体。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/313d66f63c4f1de2f2d36091f21adc8a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-14T16:09:49+08:00" />
<meta property="article:modified_time" content="2022-04-14T16:09:49+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">进化算法——昂贵、有噪声与动态适应度函数</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>1.昂贵适应度函数</h2> 
<p>在很多实际问题中，对适应度做一次评价会需要几分钟、几小时、几天甚至更长时间的计算或实验。我们在这里讨论如何减少适应度评价所需的时间以便降低进化算法对计算量的要求。</p> 
<p>实际问题涉及的适应度函数常常包含下列的一种或多种特征：</p> 
<ul><li>做一次适应度函数评价需要数分钟、数小时、数天的时间；</li><li>不能以并行的方式评价适应度函数；</li><li>适应度函数评价的次数受到时间或其他资源的限制</li></ul> 
<p>下面几种方式有助于减少适应度函数评价的计算量。</p> 
<ul><li>对以评价过得个体不再计算其费用。</li><li>如果个体看起来很好或太差，可以缩短适应度函数评价。</li><li>如果需要在大量案例测试中评价费用函数，可以用测试案例的一个子集求近似费用。</li><li>如果在计算机上评价适应度函数，可以采用让软件运行加速的标准方法。</li></ul> 
<h3>（1）适应度函数的近似</h3> 
<p>我们可以建立<strong>适应度函数模型</strong>以减少评价适应度函数的工作。即使计算量不是瓶颈，我们也可以使用适应度函数模型来改善进化算法的性能。我们称这样的模型为<strong>代理</strong>、<strong>响应曲面</strong>或<strong>元模型</strong>。用代理这个词是因为可以用适应度模型临时替换精确的适应度评价。用元模型这个词是因为适应度评价本身也是一个近似值。因此，适应度函数模型是对高阶模型的降阶。</p> 
<p>假设有一个适应度函数f(x)，我们已经在M个个体<img alt="\left \{ x_{i} \right \}" class="mathcode" src="https://images2.imgbox.com/44/42/GLSKpjir_o.png">上对它做了评价。可以利用这M个函数值估计搜索空间中任一点的适应度。下图说明适应度估计的基本思路，基于已知的函数值<img alt="f(x_{i})" class="mathcode" src="https://images2.imgbox.com/d7/55/bRCcwxys_o.png">生成一个估计<img alt="\hat{f}(x)" class="mathcode" src="https://images2.imgbox.com/57/ed/TdK8Clsb_o.png">。</p> 
<p class="img-center"><img alt="" height="86" src="https://images2.imgbox.com/84/ac/uq4Ixsox_o.png" width="425"></p> 
<p>几乎所有的近似算法都能用于适应度近似。最简单的一个估计算法是将个体的适应度近似为已经评价过得最近邻居的适应度。这个方法被称为<strong>适应度模仿</strong>，它退化为适应度景观的分段常值近似。</p> 
<p>在上图中，当得到新的数据时应该如何更新适应度估计<img alt="\hat{f}(\cdot )" class="mathcode" src="https://images2.imgbox.com/9d/c4/mXM1lt17_o.png">。我们希望适应度估计算法是迭代的，当有新的适应度信息时就更新<img alt="\hat{f}(\cdot )" class="mathcode" src="https://images2.imgbox.com/50/67/5rn4yjdN_o.png">。然而，如果适应度景观是动态的，在生成<img alt="\hat{f}(\cdot )" class="mathcode" src="https://images2.imgbox.com/7a/2d/bIRgDdln_o.png">的时候估计算法不能完全相信旧的适应度函数。用新的数据更新适应度近似被称为<strong>在线代理更新</strong>。</p> 
<p> 适应度近似的另个一方法<strong>基于父代的适应度值为子代分配适应度</strong>，它被称为<strong>适应度继承</strong>。我们可以将子代的适应度近似为父代适应度值的平均，或者它与每一个父代的相似程度取加权平均。这个想法可以推广到子代有任意多父代的算法，也可以将这个思路扩展，让子代的适应度近似为整个群众适应度的加权平均，由子代与种群中已被评价过得每一个个体相似的程度来决定权重。还可以采用更高级的适应度继承，比如考虑独立变量和适应度值之间的相关性。特别地，对于多目标问题，适应度继承仅在帕累托前沿连续且为凸的情况下才有效。</p> 
<h4>a.多项式模型</h4> 
<p> 适应度模仿的分段常值近似是一个好的起点，它告诉我们如何将适应度近似扩展为高阶多项式。例如，可以将适应度近似为线性函数</p> 
<p class="img-center"><img alt="" height="62" src="https://images2.imgbox.com/61/00/pruEWAUj_o.png" width="348"></p> 
<p>其中，n是问题的维数，x(k)是个体x的第k个元素。这是多项式模型的一个简单例子，它也被称为响应曲面，通过解下面的问题可以算出a(k)的值：</p> 
<p class="img-center"><img alt="" height="72" src="https://images2.imgbox.com/9a/79/9gcmJ4ah_o.png" width="372"></p> 
<p>其中，M是我们已经确切知道适应度值的个体数，<img alt="x_{i}" class="mathcode" src="https://images2.imgbox.com/5c/2c/BFuAYN5z_o.png">是其中第i个个体，<img alt="x_{i}(k)" class="mathcode" src="https://images2.imgbox.com/4d/7f/RN9gSJyt_o.png">是<img alt="x_{i}" class="mathcode" src="https://images2.imgbox.com/de/85/manT8WSC_o.png">的第k个元素。欲求出让（2）式最小的（n+1）个参数a(k)，k属于[0,n]，可以用迭代最小二乘法，当得到更多适应度值（M=1,2，以此类推）时更新（2）式的解所需的计算量很少。</p> 
<p>我们可以写出一个比（1）式线性模型更精确的模型：</p> 
<p class="img-center"><img alt="" height="57" src="https://images2.imgbox.com/53/75/SwPvK6Y9_o.png" width="417"></p> 
<p>这是一个二次模型，有（n^2+n+1）个参数。此模型关于参数a(k)和a(j，k)仍然是线性的，可以用迭代最小二乘法来求解，只要领会了多项式建模的思想，就可以尝试不同形式的模型，比如：</p> 
<p class="img-center"><img alt="" height="61" src="https://images2.imgbox.com/a2/06/P1OESUrI_o.png" width="446"></p> 
<p> 其中，g(·)和·h(·)可以是任意的线性或非线性函数。</p> 
<p>我们也许想用非最小二乘的方法来近似适应度模型，比如，不是通过求解（2）式而是求解下面的问题找出模型：</p> 
<p class="img-center"><img alt="" height="61" src="https://images2.imgbox.com/cd/d9/NBAjA8Wd_o.png" width="379"></p> 
<p> 这里还是（n+1）个参数a(k)上最小化。下图显示了最小化估计误差的平方和与最小最大化估计误差之间的差别。</p> 
<p class="img-center"><img alt="" height="162" src="https://images2.imgbox.com/35/18/Y6YQMoUY_o.png" width="298"></p> 
<p>（2）式的最小二乘准则较好，因为容易用解析方法求解，但最小最大化准则可能更稳健，因为能找出在最坏情况下误差最小的近似。为了在搜索空间的较困难的区域中减小近似误差，最小最大化近似会牺牲掉搜索空间中易于拟合区域的近似误差。</p> 
<h4> b.计算机实验的设计与分析</h4> 
<p><strong>计算机实验设计与分析DACE</strong>是一个随机的近似方法，它用<strong>诊断测试</strong>度量近似的好坏。已知n维向量x的M个适应度函数评价<img alt="f(x_{i})" class="mathcode" src="https://images2.imgbox.com/3c/b2/1FTkuq23_o.png">，我们假定适应度函数可以近似为</p> 
<p class="img-center"><img alt="" height="44" src="https://images2.imgbox.com/e6/24/CDjopGh5_o.png" width="270"></p> 
<p> 其中，<img alt="\mu" class="mathcode" src="https://images2.imgbox.com/c5/5e/dnSJrSGf_o.png">是一个常数（不一定是适应度函数<img alt="f(x_{i})" class="mathcode" src="https://images2.imgbox.com/32/e0/Td75KBpD_o.png">评价的均值），<img alt="\epsilon (x)" class="mathcode" src="https://images2.imgbox.com/c1/9f/UEpdNZ7f_o.png">是一个<strong>修正项</strong>。<strong>DACE假定,</strong>对所有x修正项是<img alt="\epsilon (x)" class="mathcode" src="https://images2.imgbox.com/fb/78/vUjTJBfb_o.png">均值为0方差为<img alt="\sigma ^{2}" class="mathcode" src="https://images2.imgbox.com/c3/87/6pPtenJt_o.png">的<strong>高斯分布</strong>；也就是说，f(x)的概率密度函数为</p> 
<p class="img-center"><img alt="" height="56" src="https://images2.imgbox.com/0b/63/NIM4dEW9_o.png" width="383"></p> 
<p>DACE的另一个重要假设是，对于x的不同值， <img alt="\epsilon (x)" class="mathcode" src="https://images2.imgbox.com/dd/f4/6CDxCjnv_o.png">不独立，即若x的值相似，修正项也应该相似。DACE假定<img alt="f(x_{i})" class="mathcode" src="https://images2.imgbox.com/c1/ad/wUFcQvGd_o.png">和<img alt="f(x_{j})" class="mathcode" src="https://images2.imgbox.com/9c/6f/w95YTyZM_o.png">之间的关系数<img alt="\rho _{ij}" class="mathcode" src="https://images2.imgbox.com/2f/52/qfJegu19_o.png">可以表示如下：</p> 
<p class="img-center"><img alt="" height="87" src="https://images2.imgbox.com/8f/fd/OE72JycQ_o.png" width="383"></p> 
<p> 其中，<img alt="x_{i}(k)" class="mathcode" src="https://images2.imgbox.com/2d/eb/ZE4AKOvW_o.png">是第i个候选解的第k个元素，<img alt="p_{k}" class="mathcode" src="https://images2.imgbox.com/1c/5f/CapyxPMx_o.png">∈[1,2]和<img alt="\theta _{k}\geq 0" class="mathcode" src="https://images2.imgbox.com/1a/f3/6rP1HL4q_o.png">是参数模型，<img alt="d_{ij}" class="mathcode" src="https://images2.imgbox.com/18/5f/aFrnTj5X_o.png">＞0是一个<strong>距离度量</strong>。若<img alt="d_{ij}" class="mathcode" src="https://images2.imgbox.com/eb/dc/KyRoK0zM_o.png">小，<img alt="x_{i}" class="mathcode" src="https://images2.imgbox.com/4d/ad/fbumf3uK_o.png">和<img alt="x_{j}" class="mathcode" src="https://images2.imgbox.com/e7/d0/QHqkUZDE_o.png">的相关性接近1；若<img alt="d_{ij}" class="mathcode" src="https://images2.imgbox.com/10/f6/diNPNw7v_o.png">大，<img alt="x_{i}" class="mathcode" src="https://images2.imgbox.com/65/e3/Hy4Kinc5_o.png">和<img alt="x_{j}" class="mathcode" src="https://images2.imgbox.com/4e/94/bYJI2Eq6_o.png">的相关性接近0。已知M个适应度函数评价，我们把它们集中写成向量的形式：</p> 
<p class="img-center"><img alt="" height="72" src="https://images2.imgbox.com/74/6e/lcifr8SJ_o.png" width="397"></p> 
<p> 其中，<img alt="1_{M}" class="mathcode" src="https://images2.imgbox.com/0c/0d/8GhjwBLl_o.png">是一个列向量，它的每一个元素都是1.注意，我们用符号f(x)表示单个候选解x的适应度，也表示M个元素的向量，它包含<img alt="\left \{ x_{i} \right \}" class="mathcode" src="https://images2.imgbox.com/65/d6/Vky92Jd2_o.png">的M个适应度；（9）中M个适应度函数的高斯概率密度哈数为</p> 
<p class="img-center"><img alt="" height="56" src="https://images2.imgbox.com/ee/8d/M15lJdNe_o.png" width="570"></p> 
<p> 其中，C是<strong>协方差矩阵</strong>，方差都为<img alt="\sigma ^{2}" class="mathcode" src="https://images2.imgbox.com/30/be/bGfPpuWT_o.png">的两个随机变量<img alt="f(x_{i})" class="mathcode" src="https://images2.imgbox.com/3e/fd/vtIEVwCh_o.png">和<img alt="f(x_{j})" class="mathcode" src="https://images2.imgbox.com/55/35/oN2QfPoB_o.png">之间的协方差<img alt="C_{ij}" class="mathcode" src="https://images2.imgbox.com/b9/37/0YcrBSGy_o.png">可以表示为如下</p> 
<p class="img-center"><img alt="" height="42" src="https://images2.imgbox.com/40/09/AwZHdn8e_o.png" width="269"></p> 
<p> 因此，（10）式可以写成</p> 
<p class="img-center"><img alt="" height="60" src="https://images2.imgbox.com/0e/fe/RjrIDqvQ_o.png" width="599"></p> 
<p> 其中，R是<strong>相关矩阵</strong>，R的第i行第j列的元素等于<img alt="\rho _{ij}" class="mathcode" src="https://images2.imgbox.com/78/40/TkMNV0Ai_o.png">。</p> 
<p><strong>已知一组候选解<img alt="\left \{ x_{i} \right \}" class="mathcode" src="https://images2.imgbox.com/02/72/GpQuv1Xw_o.png">和适应度评价f(x)的向量</strong>，我们可以找出测量得到的f(x)的值与假定的f(x)的参数形式匹配得最好的<img alt="\mu" class="mathcode" src="https://images2.imgbox.com/cc/c8/FC3bxXC5_o.png">和<img alt="\sigma" class="mathcode" src="https://images2.imgbox.com/f7/06/Mn3sxizP_o.png">的值。已知f(x)的随机性质，（12）式给出f(x)的概率密度函数与得到具体的f(x)的可能性成正比。因此，为找到f(x)的参数形式与f(x)的测量值之间的最好的拟合，我们要找出让（12）式的PDF(f(x))最大的<img alt="\mu" class="mathcode" src="https://images2.imgbox.com/a1/ea/oGACxjts_o.png">和<img alt="\sigma" class="mathcode" src="https://images2.imgbox.com/2f/98/MQb2TYbK_o.png">。首先考虑关于<img alt="\mu" class="mathcode" src="https://images2.imgbox.com/d2/11/yNScRvfn_o.png">的偏导数并令它等于0，得到</p> 
<p class="img-center"><img alt="" height="63" src="https://images2.imgbox.com/74/4f/z8Rtq0ov_o.png" width="371"></p> 
<p>这里忽略坟墓中的2<img alt="\sigma ^{2}" class="mathcode" src="https://images2.imgbox.com/d4/88/4ytDuvDC_o.png">，因为他独立于<img alt="\mu" class="mathcode" src="https://images2.imgbox.com/18/ac/HAEUse6N_o.png">，解（13）式，有</p> 
<p class="img-center"><img alt="" height="75" src="https://images2.imgbox.com/54/79/88saGgKL_o.png" width="408"></p> 
<p> 取（12）式关于<img alt="\sigma ^{2}" class="mathcode" src="https://images2.imgbox.com/64/78/RgKQnOUg_o.png">的偏导数，得到</p> 
<p class="img-center"><img alt="" height="73" src="https://images2.imgbox.com/e9/ec/9EJbNult_o.png" width="600"></p> 
<p>令上面的式子等于0，有</p> 
<p class="img-center"><img alt="" height="46" src="https://images2.imgbox.com/92/31/6dXoEXze_o.png" width="376"></p> 
<p> （14）和（16）式给出了用DACE近似适应度函数的<img alt="\mu" class="mathcode" src="https://images2.imgbox.com/58/08/ZkLBHnPa_o.png">和<img alt="\sigma" class="mathcode" src="https://images2.imgbox.com/5a/63/vHhDApUc_o.png">的最优值。</p> 
<p>现考虑候选解<img alt="\left \{ x_{i} \right \}" class="mathcode" src="https://images2.imgbox.com/e1/f6/uviJXfHb_o.png">的M个适应度函数评价f(x)。假设适应度函数相关，如（8）式所示，假设我们得到另一个候选解<img alt="x^{*}" class="mathcode" src="https://images2.imgbox.com/82/d6/TgtIISPg_o.png">的适应度函数评价<img alt="f(x^{*})" class="mathcode" src="https://images2.imgbox.com/40/65/vdW5cVSV_o.png">。将适应度函数向量扩大为（M+1）个元素。</p> 
<p class="img-center"><img alt="" height="43" src="https://images2.imgbox.com/9c/79/cmtGUtSt_o.png" width="288"></p> 
<p>新的相关矩阵为</p> 
<p class="img-center"><img alt="" height="54" src="https://images2.imgbox.com/ca/f5/NcUwoWJL_o.png" width="208"></p> 
<p>其中，r是M个适应度函数评价f(x)与新增的适应度函数评价<img alt="f(x^{*})" class="mathcode" src="https://images2.imgbox.com/1a/51/CEkQqAjP_o.png">的相关性向量。 我们想要关于<img alt="f(x^{*})" class="mathcode" src="https://images2.imgbox.com/45/9e/9tIx9GuH_o.png"> 最大化（12）式的概率密度函数，由此得到最适合新数据<img alt="f(x^{*})" class="mathcode" src="https://images2.imgbox.com/1a/44/FJt9GJKk_o.png"> 的形如（16）式的估计。它被称为极大似然估计。通过最大化</p> 
<p class="img-center"><img alt="" height="77" src="https://images2.imgbox.com/37/12/upJFYvY0_o.png" width="581"></p> 
<p> 就能最大化（12）式的概率密度函数。用矩阵求逆引理可以证明</p> 
<p class="img-center"><img alt="" height="61" src="https://images2.imgbox.com/e9/44/CWw0jgt2_o.png" width="587"></p> 
<p> 将它带入（19）式，有</p> 
<p class="img-center"><img alt="" height="52" src="https://images2.imgbox.com/4c/25/GKThCfYt_o.png" width="567"></p> 
<p>想要最大化这个关于<img alt="f(x^{*})" class="mathcode" src="https://images2.imgbox.com/29/0b/dDTS7KcQ_o.png">的表达式，就对<img alt="f(x^{*})" class="mathcode" src="https://images2.imgbox.com/89/f1/eP2UBC3M_o.png">求导并置为零，即</p> 
<p class="img-center"><img alt="" height="59" src="https://images2.imgbox.com/10/09/vENmZv9E_o.png" width="388"></p> 
<p> 求<img alt="f(x^{*})" class="mathcode" src="https://images2.imgbox.com/1c/57/a4RKvGRq_o.png">，得到</p> 
<p class="img-center"><img alt="" height="50" src="https://images2.imgbox.com/93/cf/ntA2wWFC_o.png" width="333"></p> 
<p> 这个式子告诉我们如何利用已有的模型来近似新的点<img alt="x^{*}" class="mathcode" src="https://images2.imgbox.com/c0/8a/1jipyimG_o.png">的适应度值。推导出近似的均方误差为</p> 
<p class="img-center"><img alt="" height="49" src="https://images2.imgbox.com/95/da/obxKdVDQ_o.png" width="384"></p> 
<p>我们利用均方误差可以为更多的适应度评价确定合适的采样点。在搜索空间中有两个区域会特别需要更多的适应度评价。首先，在适应度近似的最小值附近采样以期找到优化问题的更好的解。其次，在均方误差大的区域采样，因为这些区域中有很大的不确定性。</p> 
<p>选择采样点以提高建模的准确度这种方式被称为<strong>主动学习</strong>。主动学习通常意味着学习算法中通过采样点来优化某个费用函数。</p> 
<p>只要得到新的候选解<img alt="x^{*}" class="mathcode" src="https://images2.imgbox.com/ed/80/P01JzKCf_o.png">，就可以用（23）式计算其适应度近似。</p> 
<p><strong>例1</strong>，我们用DACE估计二维Branin基准函数</p> 
<p class="img-center"><img alt="" height="39" src="https://images2.imgbox.com/3b/ab/MtbDikgx_o.png" width="586"></p> 
<p>其中，x(1)和x(2)是候选解的两个分量（n=2）。函数的域是x(1)∈[-5,10]，x(2)∈[0.15]。首先要决定用哪些采样点。我们在这里任意选定25个均匀分布于二维搜索域的采样点（M=25）。下面用MATLAB中的fmincon函数关于<img alt="p_{k}" class="mathcode" src="https://images2.imgbox.com/5b/41/o2DQXMV1_o.png">和<img alt="\theta _{k}" class="mathcode" src="https://images2.imgbox.com/8d/19/XRKAOdY2_o.png">最大化（12）式得到</p> 
<p class="img-center"><img alt="" height="51" src="https://images2.imgbox.com/f5/08/t3fhJRqF_o.png" width="330"></p> 
<p>接下来利用（23）式在细网格上近似f(x)。结果如下图所示，这个近似很好的捕捉了基准函数的基本形状，尤其是多峰的特征。</p> 
<p class="img-center"><img alt="" height="308" src="https://images2.imgbox.com/e3/58/FarML7PE_o.png" width="465"></p> 
<p> 上例用的是<strong>均匀采样</strong>，用其他的采样方法得到的结果可能效果会更好。常用的采样方法有<strong>Latin超立方采样</strong>。此方法在每一维上将域分割为多个区间，然后在每一维的每一个区间中只设置一个采样点。与均匀采样相比，有时候它能捕捉函数无法预知的未知特征。下图说明均匀采样和Latin超立方采样的区别。</p> 
<p class="img-center"><img alt="" height="127" src="https://images2.imgbox.com/83/cc/KtBdz7N0_o.png" width="287"></p> 
<p>（a）显示在搜索域中均匀采样的4个点，（b）显示Latin超立方采样。注意，每一行每一列都只有一个点。Latin超立方采样不唯一。 </p> 
<p><strong>例2</strong>，用Latin超立方采样与DACE估计例1的二维函数。我们任意决定用21个采样点（M=21）。用 MATLAB中的fmincon函数关于<img alt="p_{k}" class="mathcode" src="https://images2.imgbox.com/6f/76/WYa6oI98_o.png">和<img alt="\theta _{k}" class="mathcode" src="https://images2.imgbox.com/77/65/u7mV4TrZ_o.png">最大化（12）式得到</p> 
<p class="img-center"><img alt="" height="53" src="https://images2.imgbox.com/b8/26/qmCwKFMh_o.png" width="349"></p> 
<p>接下来利用（23）式在细网格上近似f(x)。结果如下图所示，看起来用Latin超立方采样的结果比均匀采样的好。事实上，例1中用均匀采样的DACE近似的均方误差是24.9，而用Latin超立方采样的均方误差是14.3。即使采样点少，由Latin超立方采样得到的近似误差比均匀采样几乎好50%。可见采样方法会明显影响DACE的近似结果。</p> 
<p class="img-center"><img alt="" height="290" src="https://images2.imgbox.com/f9/21/xhuzmwq7_o.png" width="388"></p> 
<h3> （2）近似变换函数</h3> 
<p>适应度近似方法有时候表现得不好，（23）式的DACE方法需要对矩阵求逆，但矩阵的逆可能不存在。如果逆不存在，可以用伪逆。不过，这里的要点是用来近似适应度函数的基函数可能与适应度函数的形状不太匹配。例如，用傅里叶函数近似一个不规则并带有尖锐边缘的函数，就不能指望在所有的函数域中的所有点上都有好的近似。在这种情况下，可以先对原始的适应度函数做变换，然后找出变换后的函数的近似。例如，我们对M个采样点<img alt="\left \{ x_{i} \right \}" class="mathcode" src="https://images2.imgbox.com/ed/74/d2SRFgAb_o.png">评价了它的适应度函数。如果近似效果不好，在变换时可以取适应度函数样本<img alt="f(x_{i})" class="mathcode" src="https://images2.imgbox.com/78/19/Y7oOl17z_o.png">的自然对数：</p> 
<p class="img-center"><img alt="" height="37" src="https://images2.imgbox.com/23/3c/QQati742_o.png" width="254"></p> 
<p>然后用采样点的<img alt="L(x_{i})" class="mathcode" src="https://images2.imgbox.com/64/f3/qtx0N3Uj_o.png">找出L(x)的近似，记为<img alt="\hat{L}(x)" class="mathcode" src="https://images2.imgbox.com/06/94/sfiug3MB_o.png">。然后通过反变换得到原函数的近似：</p> 
<p class="img-center"><img alt="" height="40" src="https://images2.imgbox.com/a0/e4/ZKL22RCC_o.png" width="232"></p> 
<p> 例3，我们在Goldstein-Price函数上用DACE方法。</p> 
<p class="img-center"><img alt="" height="133" src="https://images2.imgbox.com/1b/74/aKi2gcYn_o.png" width="538"></p> 
<p> 其中，x(1)和x(2)的域为[-2,2]。这个函数在它的最小值附近很平坦，在<img alt="x^{*}" class="mathcode" src="https://images2.imgbox.com/46/75/XSSHOQAi_o.png">(1)=0和<img alt="x^{*}" class="mathcode" src="https://images2.imgbox.com/48/24/yjidvIla_o.png">(2)=1处函数达到最小值，<img alt="f^{*}" class="mathcode" src="https://images2.imgbox.com/33/81/19wPAk86_o.png">=3.DACE的近似方法对这个平坦的区域无能为力。</p> 
<p>因为在平坦的区域中采样点<strong>高度相关</strong>，这意味着相关矩阵R中的一些列几乎全部都由1组成，R几乎是<strong>奇异的</strong>。 在本例中我们选用采样点的自然对数，如（29）式所示。函数的形状因此发生了巨大改变；它将靠在一起的较小的f(x)值摊开，将较大的f(x)值聚拢，这样会让相似的函数值分开同时压缩函数的总范围。然后我们用DACE近似L(x)，再计算原始函数的近似，如（30）式所示，凭借对近似过程的简单修改，我们可以得到一个不错的近似，如下图所示。</p> 
<p class="img-center"><img alt="" height="293" src="https://images2.imgbox.com/c6/ed/Rsz5vY5y_o.png" width="414"></p> 
<h3> （3）在进化算法中如何使用适应度近似</h3> 
<p>这里有几个方案，<strong>首先，可以让适应度近似只替换固定比例r的适应度评价。假设适应度函数评价在进化算法的计算量中占据支配地位</strong>，这会让计算量从E减少到（1-r）E。但我们不能滥用这个思路，如果用近似替换的适应度评价过多，进化算法的收敛需要更多的时间，这样反而会浪费计算资源。在极端情况下，如果用近似替换所有的适应度评价r=1.此时计算量的确可能近似为0，但进化算法不会收敛到一个有用的结果。</p> 
<p>另一个方案是<strong>在每一代多生成一些子代并用它们的近似适应度值决定哪些留作下一代</strong>。我们称这个想法为<strong>进化控制</strong>，或<strong>模型管理</strong>。如果用准确适应度评价某些个体并用近似适应度函数评价其他个体，我们称之为<strong>个体进化控制</strong>。可以用不同的方法决定哪些个体用准确评价哪些个体用近似评价。比如，对每一个个体随机的决定它的评价方式。也可以在每一代只对近似适应度好的个体用准确适应度评价。用准确适应度评价的个体被称为<strong>受控个体</strong>。</p> 
<p>如果在某些代用准确适应度函数评价所有个体，在另一些代用近似适应度函数评价所有个体，就得到<strong>基于代的进化控制</strong>。可以用不同的方法决定哪些代用准确评价哪些代用近似函数评价。比如，确定每k代中仅有一代用准确适应度函数评价，这里k是用户自定义的参数。或者，随机的决定在每一代用准确评价或近似评价。也可以一直用近似适应度评价直到检测到收敛（例如，最好的个体在某几代中不再有改善，或者种群的标准差降到某个阈值内），然后用准确适应度函数评价下一代，之后再回到近似适应度评价。用准确适应度评价所有个体的代被称为<strong>受控代</strong>。</p> 
<p>算法1概述了<strong>基于动态近似适应度的混合算法</strong>（DAFFHEA）。</p> 
<p class="img-center"><img alt="" height="304" src="https://images2.imgbox.com/c4/a1/9Urkoodx_o.png" width="606"></p> 
<p> 我们可以尝试算法1的变种，比如<img alt="N_{c}" class="mathcode" src="https://images2.imgbox.com/e5/29/eut5dZ1B_o.png">可以不用5N这个值；还可以尝试适应度近似的各种算法；可以尝试不同的方法决定何时生成新的近似。我们可以使用置信域来决定何时生成新的近似。<strong>置信域</strong>方法以近似适应度值和实际值的对比为基础。如果近似值和实际值接近，就是一个好的近似，因此可以延长生成新的近似的时间间隔。反之，就要缩短生成新的近似的时间间隔。</p> 
<p>可对算法1做修改以便只用适应度近似就能决定为下一代保留哪些子代，它被称为<strong>知情算子方法</strong>。</p> 
<p class="img-center"><img alt="" height="275" src="https://images2.imgbox.com/57/46/YvovRYbL_o.png" width="582"></p> 
<h3>（4）多重模型 </h3> 
<p>在进化算法的早期的代中可以用近似费用函数评价，在后期的代中做更准确的评价。假设费用函数评价涉及求Riccati方程：</p> 
<p class="img-center"><img alt="" height="46" src="https://images2.imgbox.com/84/e7/UMk6e00S_o.png" width="422"></p> 
<p>这种类型的方程经常出现在求解控制和估计问题中。已知方阵F，Q和R，以及不一定是方阵的矩阵H。控制或估计算法的性能常常与P的迹成正比。解上述方程很费事，但我们可以用近似方法得到解的估计。在进化算法的早期，用（32）式解做粗略的近似，在后期的代中用更准确的近似。</p> 
<p> 下图说明这个过程。采用低准确度的适应度模型的进化算法运行<img alt="T_{1}" class="mathcode" src="https://images2.imgbox.com/58/0e/bWkeQ0oE_o.png">代。在<img alt="T_{1}" class="mathcode" src="https://images2.imgbox.com/c9/f8/odfrmIch_o.png">代之后用所得的种群初始化接下来的进化算法，这个算法用中准确度的适应度模型的进化算法运行<img alt="T_{2}" class="mathcode" src="https://images2.imgbox.com/f4/16/dXxCdKeo_o.png">代。在进化算法结束时，用最后的种群初始化最后的进化算法，这个进化算法采用高准确度的适应度模型的进化算法运行<img alt="T_{3}" class="mathcode" src="https://images2.imgbox.com/9c/ee/NvmTWGWP_o.png">代。</p> 
<p><img alt="" height="114" src="https://images2.imgbox.com/c7/29/v4aEiVdg_o.png" width="467"></p> 
<p>若需要很多准确度水平的模型时，可以将这个放法扩展。用这个方法是要注意，每一个进化算法都需要不同的种群初始化。我们可以采取一些措施，保证每一个种群在它迁移到下一个高水平书适应度函数近似之前能充分地多样化。</p> 
<p><strong>多模型优化更紧密的集成方法是用不同水平的适应度函数近似并行的运行进化算法</strong>。这个方法中的个体按指定的频率在并行的进化算法之间迁移。这个方法被称为<strong>分层进化计算</strong>，它包含几种不同的方案。<strong>首先</strong>，可以让个体从准确度较高的进化算法迁移到准确度较低的进化算法，如下图所示。<strong>其次</strong>，可以让个体在适应度近似水平差不多的进化算法发之间来回迁移，如下图所示。无所所需的模型准确度水平有多少层都可以用分层进化算法。</p> 
<p class="img-center"><img alt="" height="163" src="https://images2.imgbox.com/c6/be/l5wfpE36_o.png" width="333"></p> 
<p class="img-center"><img alt="" height="95" src="https://images2.imgbox.com/de/db/rZM6Ruio_o.png" width="480"></p> 
<p><strong>使用多重模型的另一个方法是对每一个个体x生成在各种组合中使用的多重模型</strong>。 例如，假设我们已经评价了M个个体<img alt="\left \{ x_{i} \right \}" class="mathcode" src="https://images2.imgbox.com/4e/5e/cwqR6nPC_o.png">的适应度。我们可以使用<strong>聚类算法</strong>将<img alt="\left \{ x_{i} \right \}" class="mathcode" src="https://images2.imgbox.com/b2/4b/6pa25xtL_o.png">分为C个簇。然后对C个簇中每一个生成适应度近似模型。由此得到<img alt="\hat{f_{k}}(x)" class="mathcode" src="https://images2.imgbox.com/ad/48/fB97K67c_o.png">，k∈[1,C]。当我们想近似个体x的适应度时，可以用这几种方法中的一种。比如将f(x)近似为<img alt="\hat{f_{k}}(x)" class="mathcode" src="https://images2.imgbox.com/a4/56/iKYjvLLV_o.png">，这里指标k是距x最近的簇。或者，将f(x)近似为<img alt="\hat{f}(x)" class="mathcode" src="https://images2.imgbox.com/ce/ef/qfQwvld2_o.png">的加权组合，这里的权重和为1且权重会随着x与每一簇的距离变化。</p> 
<h3>（5）过拟合</h3> 
<p> 某些适应度近似的方法会遇到过拟合问题。在使用适应度近似时，进化算法的设计者总是小心过拟合的情况。下图为针对一组数据点拟合曲线这一简单问题的过拟合的例子。如图所示，与低阶多项式相比，高阶多项式能更好的与数据匹配，但高阶多项式的推广并不好，它记住了数据点但在数据点之间的性能不好。为了得到更好的推广性能，我们经常允许在数据点处存在较大的拟合误差。</p> 
<p class="img-center"><img alt="" height="302" src="https://images2.imgbox.com/84/15/13wOxxdj_o.png" width="377"></p> 
<p>利用<strong>集成</strong>的技巧可以减轻过拟合的程度。<strong>集成</strong>是一组单独训练的适应度近似，对于以前在搜索域中从未遇到的点，将集成的适应度近似组合起来估计它们的适应度值。 </p> 
<h3>（6）近似方法的评价</h3> 
<p>在得到函数近似之后，用它之前需要验证近似能够得到较好的结果。我们往往会从检查采样点（即，用来生成近似的点）的近似值开始。</p> 
<p><strong>评估近似方法准确度的一种方法是选出几个采样点，它们没有被用来构造近似</strong>。比方说，我们选择另外Q个采样点<img alt="\left \{ x_{i} \right \}" class="mathcode" src="https://images2.imgbox.com/21/a2/OsNmPJpH_o.png">，称之为测试点。这里i∈[M+1,M+Q]，然后评价在测试点出的函数和近似，度量<strong>均方根RMS近似误差</strong></p> 
<p class="img-center"><img alt="" height="59" src="https://images2.imgbox.com/01/be/bFdxAagI_o.png" width="346"></p> 
<p>还可以度量在最坏情况下的近似误差</p> 
<p class="img-center"><img alt="" height="46" src="https://images2.imgbox.com/41/d0/Ye9kLuKD_o.png" width="350"></p> 
<p> 可以根据<strong>优先级</strong>和<strong>具体问题</strong>，选择其中一个估计指标。</p> 
<p><strong>另一种方法名为交叉验证或旋转估计，它在评估近似的质量时不需要追加采样点</strong>。 在交叉验证中，我们使用除第k个点之外的所有采样点计算近似，记为<img alt="\hat{f_{k}}(x)" class="mathcode" src="https://images2.imgbox.com/de/aa/Km7zGO1V_o.png">。按这个方式每次略掉一个点，总共会算出M个近似。这M个近似<img alt="\hat{f_{k}}(x)" class="mathcode" src="https://images2.imgbox.com/03/b4/SPHas2h8_o.png">，k∈[1,M]的每一个都用了不同的（M-1）个采样点。与上面一样，我们才用均方误差RMS或最坏情况下的近似误差评价<img alt="\hat{f_{k}}(x)" class="mathcode" src="https://images2.imgbox.com/01/ce/0Yz1gbt5_o.png"></p> 
<p class="img-center"><img alt="" height="91" src="https://images2.imgbox.com/17/32/5pSeM0Qd_o.png" width="364"></p> 
<p>由交叉验证让我们确信近似方法正确之后，就使用全部的M个采样点找出将在进化算法中使用的函数近似<img alt="\hat{f}(x)" class="mathcode" src="https://images2.imgbox.com/59/fb/Krj28G7G_o.png">。</p> 
<p>如果有足够的资源对近似适应度值和准确适应度值作比较，可以用其他指标来评价适应度近似的质量。这些指标包括：比较用近似适应度值和真实适应度值选出的重组个体的个数；用近似适应度值选错了的个体的排名；真实适应度值和近似适应度值之间的相关性；真实适应度值和近似适应度值之间排名的相关性。</p> 
<p>要定义“最好的” 适应度函数近似方法并不容易，除（35）式的均方根和最大误差指标之外，我们还需考虑其他几个准则。</p> 
<ul><li>对已知的问题近似方法有多准确？</li><li>不管近似方法的准确度如何，在使用近似方法时进化算法的表现如何？</li><li>近似方法能减少多少计算量？</li><li>近似方法有多复杂？（代码的可维护性、扩展性及移植性）</li></ul> 
<h4>精英</h4> 
<p> 对于进化算法总是采用精英这一规则来说，昂贵的适应度函数可能会是一个例外。所需要函数评价次数较少的进化算法在无精英时的表现会比有精英时的表现好。这是因为当种群规模较小或者评价次数较少时，探索会比开发更重要。非精英的进化算法允许加大探索力度。</p> 
<h2>2.动态适应度函数</h2> 
<p> 适应度函数常常随时间变化，也就是说，它们是非稳态的。</p> 
<p>动态优化的首要挑战在于检测适应度函数景观的变化。我们可以使用一些标记个体，评价它们在每一代的适应度值来检测景观的变化。如果它们的适应度值从一代到另一代明显改变（超出因噪声而改变的预期），就可以判断适应度景观已经发生两个变化。检测适应度景观不是一个非此即彼的命题，景观可能会在标记位置发生变化而在最优点处未发生变化，也有可能在最优点处发生变化而在标记点处未发生变化。如下图所示。</p> 
<p class="img-center"><img alt="" height="163" src="https://images2.imgbox.com/d3/e2/UlTYhWVa_o.png" width="410"></p> 
<p>检测到适应度景观的改变之后，可以用几种方法来跟踪变化后的最优值。<strong>一种是用新的随机种群替换全部旧种群并重新开始进化优化过程</strong>。如果适应度景观变化很大，大到令先前的种群不再包含新景观的有用信息，这样做可能是适当的。</p> 
<p>然而对于大多数问题，旧适应度景观和新景观之间会有某种相似性。也就是说，景观是渐变的不是巨变的。<strong>在这种情况下，我们要探索新景观同时也要利用进化算法在旧景观上取得的进展</strong>。比如，可以留下大部分旧种群，但在尝试探索新景观时用一些新的个体作为种子。为了增加探索也可以临时提高变异水平。这个方法被称为<strong>突变</strong>。</p> 
<p>下面讨论几种动态进化算法。</p> 
<h3>（1）预测进化算法</h3> 
<p>将预测的技巧与进化算法相结合，这就是所谓的进化预测算法。如果正在运行的进化算法可以预测其最优值随时间变化的方式，也许我们就能建立它随时间变化的模型。当检测到景观发生变化时，可以利用模型培育新的成员。这里的关键是最优值必须“以一种可预测的方式”变化。如果不满足这个条件，也可以用随机初始化的种群重新培育整个种群并重新开始进化算法。算法3说明预测进化算法的基本思想，下图是进化算法的最优值动态进化的例子。</p> 
<p class="img-center"><img alt="" height="221" src="https://images2.imgbox.com/82/0e/p7D4hlz4_o.png" width="578"></p> 
<p class="img-center"><img alt="" height="197" src="https://images2.imgbox.com/5b/83/VgtVMcPb_o.png" width="348"></p> 
<p> 算法3在实施上还有几个细节需要设计者来确定。例如，如果在外推之间运行恒定的T代，应该如何确定T的值?如何才能检测到适应度景观的变化？如果使用标记个体，应该用多少个？标记个体太少，检测不到适应度景观的变化，标记个体太多，又可能在适应度函数评价上浪费时间。</p> 
<p>应如何用算法3中的集合<img alt="X^{*}" class="mathcode" src="https://images2.imgbox.com/54/4b/GfiohT10_o.png">来估计新的最优值？也就是说，应该用那种外推算法？如何生成<img alt="\hat{x}^{*}" class="mathcode" src="https://images2.imgbox.com/68/10/GjVGqYL7_o.png">附近的子种群？我们可以简单的将S设置为只有一个元素的集合{<!-- --><img alt="\hat{x}^{*}" class="mathcode" src="https://images2.imgbox.com/19/5a/QTs0ux2V_o.png">}。另一种可能是将S设置为<img alt="\hat{x}^{*}" class="mathcode" src="https://images2.imgbox.com/9c/60/A80ZPuDW_o.png">的M个变异版本的集合，这里M是由用户定义的参数。还有一个方案是将S设置为围绕<img alt="\hat{x}^{*}" class="mathcode" src="https://images2.imgbox.com/0d/d1/NHBZAt8a_o.png">的超立方或超球面中的M个个体的确定性的集合。最后我们还得决定在算法3中用S替换旧种群的哪些个体，通常是替换旧种群中最差的个体或替换其中随机选出的个体。</p> 
<h3>（2）迁入方案 </h3> 
<p>在某些情况下，我们检测不到适应度函数的变化，或者适应度函数几乎在不停地变化。这时可以在种群中不断地加入新个体，从而让进化算法能够稳健应对适应度景观的变化。我们称这种算法为<strong>迁入方案</strong>。有两个基本的方式。<strong>直接迁入方案</strong>采用种群中的个体生成新个体。<strong>间接迁入方案</strong>基于种群的模型生成新个体。</p> 
<p>在我们决定使用直接或者间接迁入方案之后，还需要确定下列几项：</p> 
<ol><li>如何生成新个体？可以基于精英生成一组个体<img alt="X_{e}" class="mathcode" src="https://images2.imgbox.com/9e/ad/fO7WyPNA_o.png">，也可以随机生成一组个体<img alt="X_{r}" class="mathcode" src="https://images2.imgbox.com/5c/c0/kFn3qtV7_o.png">。如果我们认为适应度景观可能大幅变化，可以生成一组对偶个体<img alt="X_{d}" class="mathcode" src="https://images2.imgbox.com/53/b1/uIVWnNh9_o.png">。也可以将三种方案组合。</li><li>应该将多少新个体引入种群？大多是研究人员会引进0.2N-0.3N个新个体，N是种群规模。可以根据适应度景观的变化来调整替换频率。比如，适应度景观变化很大，进引入更多的新个体。</li><li>应该用新个体替换种群中的哪些个体？常见的是替换随机选出来的个体或者替换最差的个体。新个体在种群中并不是非常合适，因为景观的变化它们的适应度可能随时得到改进。因此，我们要记录<strong>年龄因子</strong>以防止个体还不到某个年龄就被替换掉。</li></ol> 
<p>算法4概述动态优化基于迁入的进化算法。</p> 
<p class="img-center"><img alt="" height="307" src="https://images2.imgbox.com/dc/80/8IVSXkky_o.png" width="646"></p> 
<p>它为算法决策者留有很大的决策空间。</p> 
<p><strong>1.</strong><img alt="r_{r}" class="mathcode" src="https://images2.imgbox.com/1a/f9/UA4uLTuv_o.png">，<img alt="r_{d}" class="mathcode" src="https://images2.imgbox.com/c2/89/rwcUgZge_o.png">和<img alt="r_{e}" class="mathcode" src="https://images2.imgbox.com/b8/6a/Rq5b9GYu_o.png">应该取什么值？总替换比<img alt="r_{T}" class="mathcode" src="https://images2.imgbox.com/bd/e1/Lc6ERMoq_o.png">的值是在0.2或03左右，我们在开始时通常让随机、对偶和精英代替个体的个数相同，因此，<img alt="r_{r}" class="mathcode" src="https://images2.imgbox.com/a3/3a/1PN8ZV5R_o.png"><img alt="\approx" class="mathcode" src="https://images2.imgbox.com/c6/b6/Tt4BWbbJ_o.png"><img alt="r_{d}" class="mathcode" src="https://images2.imgbox.com/87/a1/cbfs3jgs_o.png"><img alt="\approx" class="mathcode" src="https://images2.imgbox.com/8e/b4/KJREjp10_o.png"><img alt="r_{e}" class="mathcode" src="https://images2.imgbox.com/14/49/0zGGipVi_o.png"><img alt="\approx" class="mathcode" src="https://images2.imgbox.com/a5/27/7igSuPBG_o.png"><img alt="r_{T}/3" class="mathcode" src="https://images2.imgbox.com/b6/37/SnBUxW2r_o.png">。</p> 
<p><strong>2.</strong>是否应该调整<img alt="r_{r}" class="mathcode" src="https://images2.imgbox.com/35/ec/FKXLjxeJ_o.png">，<img alt="r_{d}" class="mathcode" src="https://images2.imgbox.com/96/d3/kPr6zgnb_o.png">和<img alt="r_{e}" class="mathcode" src="https://images2.imgbox.com/fe/a0/l7B1byM2_o.png">？如算法4所示，可以对它们进行调整以改善算法的性能，但这样会让算法更复杂。下面的调整方案：在每一代评价新个体的适应度，如果随机个体的那一组表现得比对偶和精英个体好，就做入下调整：</p> 
<p class="img-center"><img alt="" height="80" src="https://images2.imgbox.com/30/03/723e2Wns_o.png" width="310"></p> 
<p>其中，α控制调整的速度，<img alt="r_{min}" class="mathcode" src="https://images2.imgbox.com/0c/ac/ny5sDOSo_o.png">定义每类新个体在每一代中所占的最小比例，常数<img alt="r_{T}" class="mathcode" src="https://images2.imgbox.com/f4/c3/tEl474fO_o.png">定义新个体在每一代中的总比例。这个方法调整存在一个问题，如何决定哪一类新个体表现“最好”？可以用最好的随机个体、最好的对偶个体，以及最好的基于精英个体为基础来决定；或者根据整组随机个体、对偶个体，以及基于精英个体的平均性能来决定。</p> 
<p><strong>3.</strong>应该用新个体替换种群<img alt="\left \{ x_{i} \right \}" class="mathcode" src="https://images2.imgbox.com/37/5a/Gg95fs8J_o.png">中的哪些个体？ 替换最差的个体，或替换随机选出的个体，或者替换最差个体和随机个体的一个组合。此外，可以将最初的种群<img alt="\left \{ x_{i} \right \}" class="mathcode" src="https://images2.imgbox.com/75/1b/M6Y9STLf_o.png">与替换种群X合在一起，再用随机选择的机制选出最好的N个个体。</p> 
<p><strong>4.</strong>算法4中需要选择重组和变异方法来进化<img alt="\left \{ x_{i} \right \}" class="mathcode" src="https://images2.imgbox.com/92/90/0IkLBsLR_o.png">。</p> 
<h3>（3）基于记忆的方法</h3> 
<p> 费用函数有时候会在函数的一个有限集中变化，这种变化是确定的而不是随机的。显式的基于记忆的方法是将好的个体存入档案。每次检测到费用函数发生变化时，就从档案中取回个体并把它们加入种群。如果费用函数变成之前曾遇到过的函数，档案中的个体就是好的解进而进化算法非常迅速地收敛到新的最优值。算法5说明这个方法，但只是一个基本的概述。</p> 
<p class="img-center"><img alt="" height="257" src="https://images2.imgbox.com/c4/75/X6wvTRj0_o.png" width="624"></p> 
<p>对于适应度函数为时间的周期函数或适应度函数等于适应度函数的一个有限集的情况，本算法特别适合。下面这些问题值得进一步研究：</p> 
<ol><li>当检测到变化时我们应该在档案中保存多少个个体？</li><li>我们应该允许档案增加到多大？算法5并没有给它设限。实际上，我们可能想要检测问题的“工况点” 。如果工况点与前面遇到的工况点相同，则对此工况点的精英已经存入了档案，我们不会把当前的精英存入档案除非它们比档案中的还好。</li><li>如何才能检测到f(·)的变化？</li><li>当检测到f(·)的变化时，应该用归档的个体替换种群中的哪些个体？</li></ol> 
<h3>（4）动态优化性能的评价</h3> 
<p>在评价静态优化性能时，我们通常由上一代的种群了解进化算法的表现。在评价动态优化性能时，我们反倒要着眼于在所有代的性能。动态优化问题的两个常见指标是最好性能的平均<img alt="\bar{f}_{b}" class="mathcode" src="https://images2.imgbox.com/ce/b4/EGRWvwgx_o.png">，以及平均性能的平均<img alt="\bar{f}_{a}" class="mathcode" src="https://images2.imgbox.com/16/38/h0Feuikt_o.png">：</p> 
<p class="img-center"><img alt="" height="114" src="https://images2.imgbox.com/35/33/rv7kHnyq_o.png" width="297"></p> 
<h2>3.有噪声适应度函数 </h2> 
<p>进化算法的适应度评价经常会伴有噪声。有噪声的适应度函数评价可能会错误地将高适应度分配给低适应度的个体。下图为两个有噪声但无偏的适应度函数<img alt="f(x_{1})" class="mathcode" src="https://images2.imgbox.com/3d/7a/n4Q6gert_o.png">和<img alt="f(x_{2})" class="mathcode" src="https://images2.imgbox.com/47/dd/w5O2Hqf1_o.png">的概率密度函数。我们看到，<img alt="f(x_{1})" class="mathcode" src="https://images2.imgbox.com/d7/e5/wEAlfLOC_o.png">的真实值为0，<img alt="f(x_{2})" class="mathcode" src="https://images2.imgbox.com/60/aa/vvlSeASP_o.png">的真实值为4，但这些评价都是有噪声的。因此，对<img alt="x_{1}" class="mathcode" src="https://images2.imgbox.com/37/30/qK82YQIN_o.png">的评价所得的适应度实际上可能会大于对<img alt="x_{2}" class="mathcode" src="https://images2.imgbox.com/cf/60/FoT5W5rc_o.png">的评价。在这种情况下，对<img alt="x_{1}" class="mathcode" src="https://images2.imgbox.com/43/ae/2VYgVKhx_o.png">和<img alt="x_{2}" class="mathcode" src="https://images2.imgbox.com/e9/39/hxUtCHfh_o.png">的相对适应度的评价不再准确，会让进化算法在重组时选错个体。噪声会欺骗进化算法。</p> 
<p class="img-center"><img alt="" height="260" src="https://images2.imgbox.com/68/93/vcbS01zH_o.png" width="345"></p> 
<p></p> 
<p><strong>当适应度函数评价带有噪声时，就无法确定哪一个个最好</strong>。在进化算法的执行过程中因为不知道真实的适应度函数（假设它是有噪声适应度函数的均值），所以也不知道有噪声适应度函数的概率密度函数。不过我们可能直到真实的适应度函数的概率密度函数。这类似于上图，但是要反过来，<strong>将真实的适应度函数看成是均值等于有噪声评价的适应度函数的随机变量，而不是将有噪声适应度函数看成均值等于真实的适应度函数的随机变量</strong>。 </p> 
<p>下面介绍处理有噪声适应度函数的三种方法。</p> 
<h3>（1）再采样</h3> 
<p>   降低噪声的一种简单的方法是对适应度函数再采样。如果对给定个体的适应度函数做N次评价，并且这N个样本的噪声值是独立的，则平均适应度函数的方差会减少为原来的1/N。假设候选解x的被评估的适应度g(x)为：</p> 
<p class="img-center"><img alt="" height="40" src="https://images2.imgbox.com/53/38/TNvxLTRW_o.png" width="267"></p> 
<p>其中，f(x)是真实的适应度，w是方差为<img alt="\sigma ^{2}" class="mathcode" src="https://images2.imgbox.com/87/80/ZE4hD01d_o.png">的零均值噪声。这意味着测量到的适应度值g(x)的均值为f(x)，方差为<img alt="\sigma ^{2}" class="mathcode" src="https://images2.imgbox.com/5a/57/HqmV78Ji_o.png">。如果我们做N次独立的测量<img alt="\left \{ g_{i}(x) \right \}" class="mathcode" src="https://images2.imgbox.com/03/c4/hu0Yt6wq_o.png">，则每一个测量值<img alt="g_{i}(x)" class="mathcode" src="https://images2.imgbox.com/35/1a/TOjJTogu_o.png"><img alt="g_{i}(x)" class="mathcode" src="https://images2.imgbox.com/4e/17/H8GWPFWp_o.png">的方差为<img alt="\sigma ^{2}" class="mathcode" src="https://images2.imgbox.com/7d/71/yp3EHrWi_o.png">，<img alt="g_{i}(x)" class="mathcode" src="https://images2.imgbox.com/d4/df/zACsDLdc_o.png">真实适应度最好的估计为</p> 
<p class="img-center"><img alt="" height="73" src="https://images2.imgbox.com/6d/4e/tr9oNcop_o.png" width="289"></p> 
<p><img alt="\hat{f}(x)" class="mathcode" src="https://images2.imgbox.com/34/d6/NSz8gRtr_o.png">的方差为<img alt="\sigma ^{2}" class="mathcode" src="https://images2.imgbox.com/15/a2/nkXqc3nK_o.png">/N。如下图所示，N个有噪声适应度函数评价的平均比单个评价准确N倍。</p> 
<p class="img-center"><img alt="" height="244" src="https://images2.imgbox.com/32/5a/NqjlkiLR_o.png" width="343"></p> 
<p>实线为有噪声适应度函数的概率密度函数。虚线为4个适应度函数评价的概率密度函数。它们都是零均值，但平均评价的方差是单个评价方差的1/4.平均评价有可能比单个评价更接近其均值。</p> 
<p> <strong>只有当适应度函数评价的噪声对不同样本独立的时候，再采样策略才完全有效</strong>。比如，假设我们用有噪声的仪表测量候选解的适应度，如果仪表的噪声和它本身的采样时间相关，则N个样本的平均不会让方差减少到1/N。在这种情况下，方差减少的量取决于样本之间噪声的相关性。</p> 
<p>假设我们有候选解x的N个适应度评价<img alt="\left \{ g_{i}(x) \right \}" class="mathcode" src="https://images2.imgbox.com/69/18/zzkfEUDF_o.png">，可以找出适应度估计方差<img alt="\sigma ^{2}" class="mathcode" src="https://images2.imgbox.com/63/79/Pd76b5DV_o.png">个估计<img alt="\hat{\sigma }^{2}" class="mathcode" src="https://images2.imgbox.com/f8/90/lye9KW0H_o.png">，即</p> 
<p class="img-center"><img alt="" height="102" src="https://images2.imgbox.com/73/a5/m35bWErH_o.png" width="343"></p> 
<p> <img alt="\hat{\sigma }^{2}" class="mathcode" src="https://images2.imgbox.com/31/ac/ajmbW5eX_o.png">为<img alt="\sigma ^{2}" class="mathcode" src="https://images2.imgbox.com/da/6d/uPrnjWHa_o.png">的无偏估计，这点概率上已经做出了证明。为了适应度估计的方差达到想要的值可以用（41）式确定采样的次数。想要的方差由用户定义，它也依赖于具体的问题。当N→无穷，方差趋于0适应度估计也不再有误差。</p> 
<p><strong>有些研究人员建议对每一个候选解按固定次数采样</strong>。这样做忽略了不同候选解适应度评价的噪声可能不同的情形。如果用噪声与测量到的信号成比例的传感器进行适应度评价，可能就会如此。在这种情况下，可以将（39）式替换为</p> 
<p class="img-center"><img alt="" height="34" src="https://images2.imgbox.com/5a/90/aPMr6pgH_o.png" width="222"></p> 
<p>即适应度的噪声依赖于被评价的具体的候选解。这时，对每一个x按固定次数采样效率会很低，因为不同候选解的准确度不同。但我们还是可以用（41）式来决定每一个候选解的采样次数。</p> 
<p>为了达到想要的方差，再采样策略需要很多样本。对于昂贵适应度函数来说不太可行。因此，我们需要<strong>将再采样与前面提到的适应度近似的方法结合</strong>。只对种群中最好的个体采样，也可以减少再采样的次数。只凭单个适应度评价，可能不知道哪些个体最好，但是至少会有一些思路，并能为种群中最好的个体预留再采样所需的计算量。</p> 
<p>决定再采样次数的另一种方法是基于前面提到的适应度继承的想法。假设由几个父代<img alt="\left \{ p_{i} \right \}" class="mathcode" src="https://images2.imgbox.com/96/85/AGqFlFmr_o.png">生成一个子代x。进一步假设第i个父代适应度评价估计值为<img alt="g(p_{i})" class="mathcode" src="https://images2.imgbox.com/2a/11/Jwd1coMC_o.png">，标准差为<img alt="\sigma (p_{i})" class="mathcode" src="https://images2.imgbox.com/c5/ce/OWHgBtau_o.png">。在我们评价子代的适应度之前，可以用适应度继承得到适应度的估计<img alt="\hat{f}(x)" class="mathcode" src="https://images2.imgbox.com/52/0a/OpfYt7K1_o.png">及其标准差<img alt="\hat{\sigma }(x)" class="mathcode" src="https://images2.imgbox.com/8b/9e/3udYRZAP_o.png">，然后评价子代的适应度。如果适应度评价g(x)落在<img alt="\hat{f}(x)" class="mathcode" src="https://images2.imgbox.com/5c/b1/uvhcIrXO_o.png">的±3<img alt="\hat{\sigma }(x)" class="mathcode" src="https://images2.imgbox.com/18/47/tbjFkZEM_o.png">之间，则认为g(x)有效。否则可以推断g(x)的噪声很大，需要继续利用再采样降低噪声。这个方法在某些方面与我们的直觉相反。它表明父代的噪声越大，我们越有可能接受对子代的评价。但我们期望的是，父代的噪声越大，就更应该对子代的适应度再采样。</p> 
<p>较少适应度评价次数的另一个方法是在进化算法开始时只做几次采样，在进化算法运行过程中经常的在采样。这样做的道理是，在优化过程初期，进化算法的搜索通常比较粗糙。在早期的代中进化算法试图找到最优解的一个大致的邻域；在后期的代中则试图收敛到更准确的解。当进化算法开始收敛，优化过程快结束，我们才需要更精确的适应度函数值。也就是说，只有在准确度较好的情况下准确度才有意义。</p> 
<h3>（2）适应度估计</h3> 
<p>（41）式是基于有噪声的样本估计适应度的简单平均方法。我们还可以用其他有关概率的方法，比如，假设在搜索空间中<img alt="x_{1}" class="mathcode" src="https://images2.imgbox.com/ba/a7/zTBKcbGS_o.png">和<img alt="x_{2}" class="mathcode" src="https://images2.imgbox.com/80/b8/p0UgRP3m_o.png">的适应度值相似：</p> 
<p class="img-center"><img alt="" height="43" src="https://images2.imgbox.com/82/f1/Elpu8NOu_o.png" width="295"></p> 
<p>即<img alt="x_{1}" class="mathcode" src="https://images2.imgbox.com/04/f3/gXqrTphL_o.png">的适应度是均值为<img alt="f(x_{2})" class="mathcode" src="https://images2.imgbox.com/c0/d4/3XJEOSoS_o.png">方差为kd的高斯随机变量，这里k是未知参数，d是<img alt="x_{1}" class="mathcode" src="https://images2.imgbox.com/39/a5/A90ytq1K_o.png">和<img alt="x_{2}" class="mathcode" src="https://images2.imgbox.com/60/8f/3JTXoyby_o.png">之间的距离。如果适应度评价<img alt="g(x_{1})" class="mathcode" src="https://images2.imgbox.com/5d/f1/yTmEMpdi_o.png">的方差为<img alt="\sigma ^{2}" class="mathcode" src="https://images2.imgbox.com/f9/e6/yWqhkYcW_o.png">，则</p> 
<p class="img-center"><img alt="" height="46" src="https://images2.imgbox.com/9e/23/VETXjGoO_o.png" width="299"></p> 
<p>利用这些假设，已知<img alt="x_{1}" class="mathcode" src="https://images2.imgbox.com/87/a5/PAMBCLsq_o.png">和<img alt="x_{2}" class="mathcode" src="https://images2.imgbox.com/02/d1/dxPkC0Tl_o.png">的噪声评价，可以用极大似然估计<img alt="x_{1}" class="mathcode" src="https://images2.imgbox.com/33/92/nkmJlX5e_o.png">和<img alt="x_{2}" class="mathcode" src="https://images2.imgbox.com/8a/a6/TzbK825D_o.png">的适应度。也就是说，我们不仅可以用个体的有噪声评价，还可以用它的邻居的有噪声评价来估计它的适应度。</p> 
<h3>（3）卡尔曼进化算法</h3> 
<p>卡尔曼进化算法是为适应度函数评价有噪声且昂贵的问题设计的。最初在遗传算法背景下提出的卡尔曼进化算法会记录适应度的不确定性并相应地分配适应度评价。</p> 
<p>卡尔曼滤波是线性动态系统状态的一个最优估计器。卡尔曼进化算法假设已知个体x的适应度是一个常数，再进一步假设适应度评价噪声不是x的函数。利用这些假设，可以用简化的卡尔曼滤波的标量形式记录每一个适应度估计的不确定性。单个适应度函数评价的方差记为R。<strong>在k次适应度函数评价后个体x的适应度估计的方差记为</strong><img alt="P_{k}(x)" class="mathcode" src="https://images2.imgbox.com/24/a0/pGwRzxaX_o.png">。<strong>x的第k次适应度函数评价的值记为</strong><img alt="g_{k}(x)" class="mathcode" src="https://images2.imgbox.com/f9/4b/atxG111s_o.png">。最后<strong>在k次适应度函数评价之后对x的适应度的估计记为</strong><img alt="\hat{f}_{k}(x)" class="mathcode" src="https://images2.imgbox.com/b1/f6/vFDONK9L_o.png">。用这些记号，由卡尔曼滤波理论可以得到</p> 
<p class="img-center"><img alt="" height="94" src="https://images2.imgbox.com/26/fb/nNMyBCbe_o.png" width="405"></p> 
<p>这里k=0,1,2...，对所有x，初始化 <img alt="P_{0}(x)" class="mathcode" src="https://images2.imgbox.com/04/f5/I7GD6AOY_o.png">=无穷，有</p> 
<p class="img-center"><img alt="" height="67" src="https://images2.imgbox.com/4a/09/MGLJRtXQ_o.png" width="241"></p> 
<p> 也就是说，在第一次适应度函数评价<img alt="g_{1}(x)" class="mathcode" src="https://images2.imgbox.com/c9/f3/nqGYQ8WJ_o.png">之后我们的估计<img alt="\hat{f}_{1}(x)" class="mathcode" src="https://images2.imgbox.com/32/dc/GIhm3IXd_o.png">就等于第一次评价。在第一次评价之后适应度估计中的不确定性<img alt="P_{1}(x)" class="mathcode" src="https://images2.imgbox.com/80/40/b5XedDM8_o.png">等于评价中的不确定性。</p> 
<p>（45）式显示，每当评价x的适应度时，我们会基于前面的估计，不确定性以及最近的适应度函数评价的结果g(x)来修正我们的估计<img alt="\hat{f}(x)" class="mathcode" src="https://images2.imgbox.com/2c/da/TdEgvs5n_o.png">，由（45）式可知</p> 
<p class="img-center"><img alt="" height="78" src="https://images2.imgbox.com/48/20/9eu9O6xO_o.png" width="325"></p> 
<p> 换言之，如果我们能完全确定x的适应度（<img alt="P_{k}(x)" class="mathcode" src="https://images2.imgbox.com/43/cc/Yr5qRiOI_o.png">=0） ，再评价x的适应度也不会改变我们对它适应度的估计。另一方面，如果我们完全不能确定x的适应度（<img alt="P_{k}(x)" class="mathcode" src="https://images2.imgbox.com/61/48/eVtGEH7d_o.png">趋近于无穷），就会将适应度的估计设置为下一次适应度函数评价的结果。（45）式还说明，每次评价x的适应度时，它的不确定性P(x)都在减小（也就是说我们对估计值的信心在不断地增加）。由（45）式可得</p> 
<p class="img-center"><img alt="" height="130" src="https://images2.imgbox.com/74/18/YdPzwLLt_o.png" width="330"></p> 
<p>这些结果与我们的直觉一致。如果适应度函数噪声的方差R为0，则适应度函数很完美，所以我们的估计就是适应度函数评价的结果，并且在估计中的不确定性为0.另一方面，如果适应度函数噪声的方差R为无穷，噪声过大会令适应度函数无法提供任何有用的信息。在这种情况下，再多的评价也不能改变我们对适应度函数的估计，也不能降低其中的不确定性。</p> 
<p> 对于每一个个体x的每次适应度函数评价，卡尔曼进化算法会记下适应度函数评估<img alt="\hat{f}_{k}(x)" class="mathcode" src="https://images2.imgbox.com/e3/f8/3zg5ia0i_o.png">和方差<img alt="P_{k}(x)" class="mathcode" src="https://images2.imgbox.com/6b/4b/rL74vs8c_o.png">（k=1，2，...）。我们按用户定义的比例F，取出现有评价的F部分用与生成和评价新个体。按（46）式初始化新个体的适应度估计和方差。用现有评价的（1-F）部分用于重新评价已有的个体。在这种情况下，按照（45）式更新适应度估计和方差。每当有足够的资源评价适应度函数，就生成在[0,1]上均匀分布的一个随机数r，如果r＜F，用进化算法重组和变异生成新个体，然后评价它的适应度；否则，重新评价一个已有的个体。</p> 
<p>当重新评级一个已有的个体时，要遵循两个指导原则：首先，对适应度估计的方差较大的个体可以重新评价以生成更多的信息。其次，对于适应度估计值较大的个体可以重新评估以生成更多有用的信息。也就是说，对适应度估计值低的个体，我们不用介意能否得到高精度，因为不会用它们重组生成下一代。下面是一种个体<img alt="x_{s}" class="mathcode" src="https://images2.imgbox.com/9a/56/J1VbXdUY_o.png">重新评价的方法：</p> 
<p class="img-center"><img alt="" height="83" src="https://images2.imgbox.com/8a/68/4BlKzDpK_o.png" width="510"></p> 
<p>其中 <img alt="\hat{f}_{k}(x)" class="mathcode" src="https://images2.imgbox.com/b1/00/PD6Njd4T_o.png">和<img alt="P_{k}(x)" class="mathcode" src="https://images2.imgbox.com/e1/97/bQwv7CBm_o.png">省去了下标k；对（49）式中的每一个个体x，使用最近更新的 <img alt="\hat{f}_{k}(x)" class="mathcode" src="https://images2.imgbox.com/64/a6/qMJc0rvt_o.png">和<img alt="P_{k}(x)" class="mathcode" src="https://images2.imgbox.com/f1/ef/mVQbITug_o.png">的值。这个式子说明，适应度估计值大于均值一下一个标准差的所有个体中，选择不确定性最大的个体重新评价。这个策略假定f(x)为适应度，所以f(x)越大越好。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/eb59867aac33d5f2c597bc38f1f7254b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">FPGA&amp;MSP430实现等精度频率计</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/18f216076ea78216dd50cd95e32e7af2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">node配置淘宝镜像源</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>