<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Flink部署——命令行界面 - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Flink部署——命令行界面" />
<meta property="og:description" content="文章目录 作业生命周期管理（Job Lifecycle Management）提交作业作业监控创建一个保存点设置保存点终止作业正常停止作业以创建最终保存点非优雅的停止作业 从保存点启动作业 CLI Actions高级命令行界面REST API选择部署目标提交 PyFlink 作业 Flink 提供了一个命令行界面 （CLI） bin/flink 来运行打包为 JAR 文件的程序并控制其执行。CLI 是任何 Flink 设置的一部分，可在本地单节点设置和分布式设置中使用。它连接到在conf/flink-conf.yaml中指定的正在运行的JobManager。
作业生命周期管理（Job Lifecycle Management） 本节中列出的命令工作的先决条件是运行 Flink 部署，如 Kubernetes、YARN 或任何其他可用的选项。您可以随意在本地启动 Flink 集群，在您自己的机器上尝试这些命令。
提交作业 提交作业意味着将作业的 JAR 和相关依赖项上传到 Flink 集群并启动作业执行。对于此示例，我们选择一个长时间运行的作业，如 examples/streaming/StateMachineExample.jar。您可以随意从 examples/文件夹中选择任何其他JAR存档或部署自己的作业。
$ ./bin/flink run \ --detached \ ./examples/streaming/StateMachineExample.jar 使用 --detached 提交作业将使该命令在提交完成后返回。输出包含（除其他内容外）新提交的作业的 ID。
Usage with built-in data generator: StateMachineExample [--error-rate &lt;probability-of-invalid-transition&gt;] [--sleep &lt;sleep-per-record-in-ms&gt;] Usage with Kafka: StateMachineExample --kafka-topic &lt;topic&gt; [--brokers &lt;brokers&gt;] Options for both the above setups: [--backend &lt;file|rocks&gt;] [--checkpoint-dir &lt;filepath&gt;] [--async-checkpoints &lt;true|false&gt;] [--incremental-checkpoints &lt;true|false&gt;] [--output &lt;filepath&gt; OR null for stdout] Using standalone source with error rate 0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/364bf12d5257cd9ed2e44e6ec12e0b9a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-06-26T18:21:00+08:00" />
<meta property="article:modified_time" content="2022-06-26T18:21:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Flink部署——命令行界面</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#Job_Lifecycle_Management_4" rel="nofollow">作业生命周期管理（Job Lifecycle Management）</a></li><li><ul><li><a href="#_7" rel="nofollow">提交作业</a></li><li><a href="#_43" rel="nofollow">作业监控</a></li><li><a href="#_60" rel="nofollow">创建一个保存点</a></li><li><a href="#_79" rel="nofollow">设置保存点</a></li><li><a href="#_103" rel="nofollow">终止作业</a></li><li><ul><li><a href="#_104" rel="nofollow">正常停止作业以创建最终保存点</a></li><li><a href="#_126" rel="nofollow">非优雅的停止作业</a></li></ul> 
   </li><li><a href="#_143" rel="nofollow">从保存点启动作业</a></li></ul> 
  </li><li><a href="#CLI_Actions_181" rel="nofollow">CLI Actions</a></li><li><a href="#_196" rel="nofollow">高级命令行界面</a></li><li><ul><li><a href="#REST_API_197" rel="nofollow">REST API</a></li><li><a href="#_200" rel="nofollow">选择部署目标</a></li><li><a href="#_PyFlink__219" rel="nofollow">提交 PyFlink 作业</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<p>Flink 提供了一个命令行界面 （CLI） bin/flink 来运行打包为 JAR 文件的程序并控制其执行。CLI 是任何 Flink 设置的一部分，可在本地单节点设置和分布式设置中使用。它连接到在conf/flink-conf.yaml中指定的正在运行的JobManager。</p> 
<h2><a id="Job_Lifecycle_Management_4"></a>作业生命周期管理（Job Lifecycle Management）</h2> 
<p>本节中列出的命令工作的先决条件是运行 Flink 部署，如 Kubernetes、YARN 或任何其他可用的选项。您可以随意在本地启动 Flink 集群，在您自己的机器上尝试这些命令。</p> 
<h3><a id="_7"></a>提交作业</h3> 
<p>提交作业意味着将作业的 JAR 和相关依赖项上传到 Flink 集群并启动作业执行。对于此示例，我们选择一个长时间运行的作业，如 examples/streaming/StateMachineExample.jar。您可以随意从 examples/文件夹中选择任何其他JAR存档或部署自己的作业。</p> 
<pre><code class="prism language-shell">$ ./bin/flink run <span class="token punctuation">\</span>
      --detached <span class="token punctuation">\</span>
      ./examples/streaming/StateMachineExample.jar
</code></pre> 
<p>使用 --detached 提交作业将使该命令在提交完成后返回。输出包含（除其他内容外）新提交的作业的 ID。</p> 
<pre><code class="prism language-shell">Usage with built-in data generator: StateMachineExample <span class="token punctuation">[</span>--error-rate <span class="token operator">&lt;</span>probability-of-invalid-transition<span class="token operator">&gt;</span><span class="token punctuation">]</span> <span class="token punctuation">[</span>--sleep <span class="token operator">&lt;</span>sleep-per-record-in-ms<span class="token operator">&gt;</span><span class="token punctuation">]</span>
Usage with Kafka: StateMachineExample --kafka-topic <span class="token operator">&lt;</span>topic<span class="token operator">&gt;</span> <span class="token punctuation">[</span>--brokers <span class="token operator">&lt;</span>brokers<span class="token operator">&gt;</span><span class="token punctuation">]</span>
Options <span class="token keyword">for</span> both the above setups:
        <span class="token punctuation">[</span>--backend <span class="token operator">&lt;</span>file<span class="token operator">|</span>rocks<span class="token operator">&gt;</span><span class="token punctuation">]</span>
        <span class="token punctuation">[</span>--checkpoint-dir <span class="token operator">&lt;</span>filepath<span class="token operator">&gt;</span><span class="token punctuation">]</span>
        <span class="token punctuation">[</span>--async-checkpoints <span class="token operator">&lt;</span>true<span class="token operator">|</span>false<span class="token operator">&gt;</span><span class="token punctuation">]</span>
        <span class="token punctuation">[</span>--incremental-checkpoints <span class="token operator">&lt;</span>true<span class="token operator">|</span>false<span class="token operator">&gt;</span><span class="token punctuation">]</span>
        <span class="token punctuation">[</span>--output <span class="token operator">&lt;</span>filepath<span class="token operator">&gt;</span> OR null <span class="token keyword">for</span> stdout<span class="token punctuation">]</span>

Using standalone <span class="token builtin class-name">source</span> with error rate <span class="token number">0.000000</span> and <span class="token function">sleep</span> delay <span class="token number">1</span> millis

Job has been submitted with JobID cca7bc1061d61cf15238e92312c2fc20
</code></pre> 
<p>打印的使用情况信息列出了与作业相关的参数，如有必要，可以将其添加到作业提交命令的末尾。为了便于阅读，我们假设返回的 JobID 存储在以下命令的变量JOB_ID中：</p> 
<pre><code class="prism language-bash">$ <span class="token builtin class-name">export</span> <span class="token assign-left variable">JOB_ID</span><span class="token operator">=</span><span class="token string">"cca7bc1061d61cf15238e92312c2fc20"</span>
</code></pre> 
<p>还有另一个名为 run-application 的操作可用于在 Application Mode下运行作业。本文档不单独处理此操作，因为它在 CLI 前端方面的工作方式类似于运行操作。</p> 
<p>run 和run-application程序命令支持通过 -D 参数传递其他配置参数。例如，可以通过设置 -Dpipeline.max-parallelism=120 来完成设置作业的最大并行度。此参数对于配置每个作业或应用程序模式群集非常有用，因为您可以将任何配置参数传递给群集，而无需更改配置文件。</p> 
<p>将作业提交到现有会话集群时，仅支持执行配置参数。</p> 
<h3><a id="_43"></a>作业监控</h3> 
<p>您可以使用列表操作监视任何正在运行的作业：</p> 
<pre><code class="prism language-bash">$ ./bin/flink list
</code></pre> 
<pre><code class="prism language-bash">Waiting <span class="token keyword">for</span> response<span class="token punctuation">..</span>.
------------------ Running/Restarting Jobs -------------------
<span class="token number">30.11</span>.2020 <span class="token number">16</span>:02:29 <span class="token builtin class-name">:</span> cca7bc1061d61cf15238e92312c2fc20 <span class="token builtin class-name">:</span> State machine job <span class="token punctuation">(</span>RUNNING<span class="token punctuation">)</span>
--------------------------------------------------------------
No scheduled jobs.
</code></pre> 
<p>已提交但尚未启动的作业将列在“Scheduled Jobs”下。</p> 
<h3><a id="_60"></a>创建一个保存点</h3> 
<p>可以创建保存点以保存作业所处的当前状态。所需要的只是工作 ID：</p> 
<pre><code class="prism language-bash">$ ./bin/flink savepoint <span class="token variable">$JOB_ID</span>  /tmp/flink-savepoints
</code></pre> 
<pre><code class="prism language-bash">Triggering savepoint <span class="token keyword">for</span> job cca7bc1061d61cf15238e92312c2fc20.
Waiting <span class="token keyword">for</span> response<span class="token punctuation">..</span>.
Savepoint completed. Path: file:/tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab
You can resume your program from this savepoint with the run command.
</code></pre> 
<p>savepoint 文件夹是可选的，如果未设置 state.savepoints.dir，则需要指定该文件夹。</p> 
<p>最后，您可以选择提供保存点的二进制格式。</p> 
<p>保存点的路径可以在以后用于重新启动 Flink 作业。</p> 
<h3><a id="_79"></a>设置保存点</h3> 
<p>保存点操作还可用于删除保存点。–dispose 需要添加相应的保存点路径：</p> 
<pre><code class="prism language-bash">$ ./bin/flink savepoint <span class="token punctuation">\</span> 
      --dispose <span class="token punctuation">\</span>
      /tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab <span class="token punctuation">\</span> 
      <span class="token variable">$JOB_ID</span>
</code></pre> 
<pre><code class="prism language-bash">Disposing savepoint <span class="token string">'/tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab'</span><span class="token builtin class-name">.</span>
Waiting <span class="token keyword">for</span> response<span class="token punctuation">..</span>.
Savepoint <span class="token string">'/tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab'</span> disposed.
</code></pre> 
<p>如果使用自定义状态实例（例如自定义还原状态或 RocksDB 状态），则必须指定用于触发保存点的程序 JAR 的路径。否则，您将遇到一个 ClassNotFoundException：</p> 
<pre><code class="prism language-bash">$ ./bin/flink savepoint <span class="token punctuation">\</span>
      --dispose <span class="token operator">&lt;</span>savepointPath<span class="token operator">&gt;</span> <span class="token punctuation">\</span> 
      --jarfile <span class="token operator">&lt;</span>jarFile<span class="token operator">&gt;</span>
</code></pre> 
<p>通过保存点操作触发保存点处置不仅会从存储中删除数据，还会使 Flink 清理与保存点相关的元数据。</p> 
<h3><a id="_103"></a>终止作业</h3> 
<h4><a id="_104"></a>正常停止作业以创建最终保存点</h4> 
<p>停止作业的另一个操作是 stop。这是一种在停止从源流向接收器时停止正在运行的流式处理作业的更优雅方式。当用户请求停止作业时，将请求所有源发送将触发保存点的最后一个检查点屏障，并且在该保存点成功完成后，它们将通过调用 cancel（） 方法完成。</p> 
<pre><code class="prism language-bash">$ ./bin/flink stop <span class="token punctuation">\</span>
      --savepointPath /tmp/flink-savepoints <span class="token punctuation">\</span>
      <span class="token variable">$JOB_ID</span>
</code></pre> 
<pre><code class="prism language-bash">Suspending job <span class="token string">"cca7bc1061d61cf15238e92312c2fc20"</span> with a savepoint.
Savepoint completed. Path: file:/tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab
</code></pre> 
<p>如果未设置 state.savepoints.dir，我们必须使用 --savepointPath 指定 savepoint 文件夹。</p> 
<p>如果指定了 --drain 标志，则在最后一个检查点屏障之前将发出MAX_WATERMARK。这将使所有已注册的事件时间计时器都处于触发状态，从而清除正在等待特定水印的任何状态，例如窗口。作业将继续运行，直到所有源正确关闭。这允许作业完成对所有动态数据的处理，这可以生成一些记录，以便在停止时获取的保存点之后进行处理。</p> 
<p>如果要永久终止作业，请使用 --drain 标志。如果要在以后的某个时间点恢复作业，请不要排空管道，因为在恢复作业时可能会导致不正确的结果。</p> 
<p>最后，您可以选择提供保存点的<a href="https://nightlies.apache.org/flink/flink-docs-release-1.15/zh/docs/ops/state/savepoints/#savepoint-format" rel="nofollow">二进制</a>格式。</p> 
<h4><a id="_126"></a>非优雅的停止作业</h4> 
<p>取消作业可以通过取消操作实现：</p> 
<pre><code class="prism language-bash">$ ./bin/flink cancel <span class="token variable">$JOB_ID</span>
</code></pre> 
<pre><code class="prism language-bash">Cancelling job cca7bc1061d61cf15238e92312c2fc20.
Cancelled job cca7bc1061d61cf15238e92312c2fc20.
</code></pre> 
<p>相应作业的状态将从“正在运行”转换为“已取消”。任何计算都将停止。</p> 
<p>–withSavepoint 标志允许在作业取消过程中创建保存点。此功能已弃用。请改用停止操作。</p> 
<h3><a id="_143"></a>从保存点启动作业</h3> 
<p>可以使用运行（和运行应用程序）操作从保存点启动作业。</p> 
<pre><code class="prism language-bash">$ ./bin/flink run <span class="token punctuation">\</span>
      --detached <span class="token punctuation">\</span> 
      --fromSavepoint /tmp/flink-savepoints/savepoint-cca7bc-bb1e257f0dab <span class="token punctuation">\</span>
      ./examples/streaming/StateMachineExample.jar
</code></pre> 
<pre><code class="prism language-bash">Usage with built-in data generator: StateMachineExample <span class="token punctuation">[</span>--error-rate <span class="token operator">&lt;</span>probability-of-invalid-transition<span class="token operator">&gt;</span><span class="token punctuation">]</span> <span class="token punctuation">[</span>--sleep <span class="token operator">&lt;</span>sleep-per-record-in-ms<span class="token operator">&gt;</span><span class="token punctuation">]</span>
Usage with Kafka: StateMachineExample --kafka-topic <span class="token operator">&lt;</span>topic<span class="token operator">&gt;</span> <span class="token punctuation">[</span>--brokers <span class="token operator">&lt;</span>brokers<span class="token operator">&gt;</span><span class="token punctuation">]</span>
Options <span class="token keyword">for</span> both the above setups:
        <span class="token punctuation">[</span>--backend <span class="token operator">&lt;</span>file<span class="token operator">|</span>rocks<span class="token operator">&gt;</span><span class="token punctuation">]</span>
        <span class="token punctuation">[</span>--checkpoint-dir <span class="token operator">&lt;</span>filepath<span class="token operator">&gt;</span><span class="token punctuation">]</span>
        <span class="token punctuation">[</span>--async-checkpoints <span class="token operator">&lt;</span>true<span class="token operator">|</span>false<span class="token operator">&gt;</span><span class="token punctuation">]</span>
        <span class="token punctuation">[</span>--incremental-checkpoints <span class="token operator">&lt;</span>true<span class="token operator">|</span>false<span class="token operator">&gt;</span><span class="token punctuation">]</span>
        <span class="token punctuation">[</span>--output <span class="token operator">&lt;</span>filepath<span class="token operator">&gt;</span> OR null <span class="token keyword">for</span> stdout<span class="token punctuation">]</span>

Using standalone <span class="token builtin class-name">source</span> with error rate <span class="token number">0.000000</span> and <span class="token function">sleep</span> delay <span class="token number">1</span> millis

Job has been submitted with JobID 97b20a0a8ffd5c1d656328b0cd6436a6
</code></pre> 
<p>查看该命令如何等于初始 run 命令，但 --fromSavepoint 参数除外，该参数用于引用以前停止的作业的状态。将生成一个新的作业 ID，可用于维护作业。</p> 
<p>默认情况下，我们尝试将整个保存点状态与正在提交的作业进行匹配。如果要允许跳过无法使用新作业还原的保存点状态，可以设置 --allowNonRestoredState 标志。如果从程序中删除了在触发保存点时属于程序一部分的算子，并且您仍希望使用保存点，则需要允许此操作。</p> 
<pre><code class="prism language-bash">$ ./bin/flink run <span class="token punctuation">\</span>
      --fromSavepoint <span class="token operator">&lt;</span>savepointPath<span class="token operator">&gt;</span> <span class="token punctuation">\</span>
      --allowNonRestoredState <span class="token punctuation">..</span>.
</code></pre> 
<p>如果您的程序删除了作为保存点一部分的算子，这将非常有用。</p> 
<p>您还可以选择应用于保存点的恢复模式。该模式控制谁将取得指定保存点文件的所有权。</p> 
<h2><a id="CLI_Actions_181"></a>CLI Actions</h2> 
<p>以下是 Flink 的 CLI 工具支持的操作概述：</p> 
<table><thead><tr><th>Action</th><th>Purpose（目的）</th></tr></thead><tbody><tr><td>run</td><td>此操作将执行作业。它至少需要包含作业的jar。如有必要，可以传递与 Flink 或作业相关的参数。</td></tr><tr><td>run-application</td><td>此操作在<a href="https://nightlies.apache.org/flink/flink-docs-release-1.15/zh/docs/deployment/overview/#application-mode" rel="nofollow">Application Mode</a>下执行作业。除此之外，它需要与运行操作相同的参数。</td></tr><tr><td>info</td><td>此操作可用于打印已传递作业的优化执行图。同样，需要传递包含作业的 jar。</td></tr><tr><td>list</td><td>此操作将列出所有正在运行的作业或计划的作业。</td></tr><tr><td>savepoint</td><td>此操作可用于为给定作业创建或释放保存点。如果 conf/flink-conf.yaml 中未指定 state.savepoints.dir 参数，则可能需要指定 JobID 之外的 savepoint 目录。</td></tr><tr><td>cancel</td><td>此操作可用于根据其作业 ID 取消正在运行的作业。</td></tr><tr><td>stop</td><td>此操作将取消和保存点操作组合在一起以停止正在运行的作业，同时还会创建一个保存点以再次开始。</td></tr></tbody></table> 
<p>所有操作及其参数的更细粒度的描述可以通过 bin/flink --help 或每个单独的操作 bin/flink --help 的使用信息进行访问。</p> 
<h2><a id="_196"></a>高级命令行界面</h2> 
<h3><a id="REST_API_197"></a>REST API</h3> 
<p>Flink 集群也可以使用 REST API 进行管理。前面几节中描述的命令是 Flink 的 REST 端点所提供命令的子集。因此，像curl这样的工具可以用来从Flink中获得更多。</p> 
<h3><a id="_200"></a>选择部署目标</h3> 
<p>Flink 与多个集群管理框架（如 Kubernetes 或 YARN）兼容，这些框架在资源提供程序部分有更详细的描述。可以在不同的部署模式下提交作业。作业提交的参数化因基础框架和部署模式而异。</p> 
<p>bin/flink 提供了一个参数 --target 来处理不同的选项。除此之外，还必须使用run（对于Session and Per-Job Mode）或run-application（对于 Application Mode）提交作业。请参阅以下参数组合摘要：</p> 
<ul><li>YARN<br> ./bin/flink run --target yarn-session: 提交到已在 YARN 集群上运行的 Flink<br> ./bin/flink run --target yarn-per-job: 提交在per-job模式下在 YARN 群集上启动 Flink<br> ./bin/flink run-application --target yarn-application: 提交在应用程序模式下在 YARN 集群上启动 Flink</li><li>Kubernetes<br> ./bin/flink run --target kubernetes-session: 提交到 Kubernetes 集群上已经运行的 Flink<br> ./bin/flink run-application --target kubernetes-application: 提交在应用程序中的 Kubernetes 集群上启动 Flink</li><li>Standalone:<br> ./bin/flink run --target local:在会话模式下使用微型集群进行本地提交<br> ./bin/flink run --target remote: 提交到已在运行的 Flink 集群</li></ul> 
<p>–target 将覆盖 conf/flink-conf.yaml 中指定的 execute.target。</p> 
<p>有关命令和可用选项的更多详细信息，请参阅文档的资源提供程序特定页面。</p> 
<h3><a id="_PyFlink__219"></a>提交 PyFlink 作业</h3> 
<p>目前，用户可以通过 CLI 提交 PyFlink 作业。它不需要指定 JAR 文件路径或条目 main 类，这与 Java 作业提交不同。</p> 
<p>当通过 flink run 提交 Python 作业时，Flink 将运行命令 “python”。请运行以下命令以确认当前环境中的 python 可执行文件指向受支持的 Python 版本 3.6+。</p> 
<pre><code class="prism language-bash">$ python --version
<span class="token comment"># the version printed here must be 3.6+</span>
</code></pre> 
<p>以下命令显示了不同的 PyFlink 作业提交用例：</p> 
<ul><li>Run a PyFlink job:</li></ul> 
<pre><code class="prism language-bash">$ ./bin/flink run --python examples/python/table/word_count.py
</code></pre> 
<p>使用其他源文件和资源文件运行 PyFlink 作业。在 --pyFiles 中指定的文件将被添加到 PYTHONPATH 中，因此在 Python 代码中可用。</p> 
<pre><code class="prism language-bash">$ ./bin/flink run <span class="token punctuation">\</span>
      --python examples/python/table/word_count.py <span class="token punctuation">\</span>
      --pyFiles file:///user.txt,hdfs:///<span class="token variable">$namenode_address</span>/username.txt
</code></pre> 
<ul><li>运行将引用 Java UDF 或外部连接器的 PyFlink 作业。–jarfile 中指定的 JAR 文件将上传到集群。</li></ul> 
<pre><code class="prism language-bash">$ ./bin/flink run <span class="token punctuation">\</span>
      --python examples/python/table/word_count.py <span class="token punctuation">\</span>
      --jarfile <span class="token operator">&lt;</span>jarFile<span class="token operator">&gt;</span>
</code></pre> 
<p>使用 pyFiles 和 --pyModule 中指定的主入口模块运行 PyFlink 作业：</p> 
<pre><code class="prism language-bash">$ ./bin/flink run <span class="token punctuation">\</span>
      --pyModule table.word_count <span class="token punctuation">\</span>
      --pyFiles examples/python/table
</code></pre> 
<p>在主机上运行的特定 JobManager 上提交 PyFlink 作业（相应地调整命令）：</p> 
<pre><code class="prism language-bash">$ ./bin/flink run <span class="token punctuation">\</span>
      --jobmanager <span class="token operator">&lt;</span>jobmanagerHost<span class="token operator">&gt;</span>:8081 <span class="token punctuation">\</span>
      --python examples/python/table/word_count.py

</code></pre> 
<ul><li>Run a PyFlink job using a YARN cluster in Per-Job Mode:</li></ul> 
<pre><code class="prism language-bash">$ ./bin/flink run <span class="token punctuation">\</span>
      --target yarn-per-job
      --python examples/python/table/word_count.py

</code></pre> 
<ul><li>Run a PyFlink job using a YARN cluster in Application Mode:</li></ul> 
<pre><code class="prism language-bash">$ ./bin/flink run-application -t yarn-application <span class="token punctuation">\</span>
      -Djobmanager.memory.process.size<span class="token operator">=</span>1024m <span class="token punctuation">\</span>
      -Dtaskmanager.memory.process.size<span class="token operator">=</span>1024m <span class="token punctuation">\</span>
      -Dyarn.application.name<span class="token operator">=</span><span class="token operator">&lt;</span>ApplicationName<span class="token operator">&gt;</span> <span class="token punctuation">\</span>
      -Dyarn.ship-files<span class="token operator">=</span>/path/to/shipfiles <span class="token punctuation">\</span>
      -pyarch shipfiles/venv.zip <span class="token punctuation">\</span>
      -pyclientexec venv.zip/venv/bin/python3 <span class="token punctuation">\</span>
      -pyexec venv.zip/venv/bin/python3 <span class="token punctuation">\</span>
      -py shipfiles/word_count.py

</code></pre> 
<p>注意 它假定执行作业所需的 Python 依赖项已放置在 /path/to/shipfiles 目录中。例如，对于上面的示例，它应该包含 venv.zip 和 word_count.py。</p> 
<p>注意：当它在 YARN 应用程序模式下执行 JobManager 上的作业时，-pyarch 和 -py 中指定的路径是相对于 shipfile 的路径，shipfile 是已交付文件的目录名称。</p> 
<p>注意：通过 -pyarch 指定的存档文件将通过 Blob 服务器分发到任务管理器，其中文件大小限制为 2 GB。如果归档文件的大小超过 2 GB，则可以将其上载到分布式文件系统，然后使用命令行选项 -pyarch 中的路径。</p> 
<ul><li>在具有集群ID的本机Kubernetes集群上运行PyFlink应用程序，它需要一个安装了PyFlink的Docker映像，请参阅在docker中启用PyFlink：</li></ul> 
<pre><code class="prism language-bash">$ ./bin/flink run-application <span class="token punctuation">\</span>
      --target kubernetes-application <span class="token punctuation">\</span>
      --parallelism <span class="token number">8</span> <span class="token punctuation">\</span>
      -Dkubernetes.cluster-id<span class="token operator">=</span><span class="token operator">&lt;</span>ClusterId<span class="token operator">&gt;</span> <span class="token punctuation">\</span>
      -Dtaskmanager.memory.process.size<span class="token operator">=</span>4096m <span class="token punctuation">\</span>
      -Dkubernetes.taskmanager.cpu<span class="token operator">=</span><span class="token number">2</span> <span class="token punctuation">\</span>
      -Dtaskmanager.numberOfTaskSlots<span class="token operator">=</span><span class="token number">4</span> <span class="token punctuation">\</span>
      -Dkubernetes.container.image<span class="token operator">=</span><span class="token operator">&lt;</span>PyFlinkImageName<span class="token operator">&gt;</span> <span class="token punctuation">\</span>
      --pyModule word_count <span class="token punctuation">\</span>
      --pyFiles /opt/flink/examples/python/table/word_count.py

</code></pre> 
<p>要了解更多可用选项，请参阅资源提供程序部分中更详细地介绍的 <a href="https://nightlies.apache.org/flink/flink-docs-release-1.15/zh/docs/deployment/resource-providers/native_kubernetes/" rel="nofollow">Kubernetes</a> 或 <a href="https://nightlies.apache.org/flink/flink-docs-release-1.15/zh/docs/deployment/resource-providers/yarn/" rel="nofollow">YARN</a>。</p> 
<p>除了上面提到的 --pyFiles、–pyModule 和 --python 之外，还有其他一些与 Python 相关的选项。以下是 Flink 的 CLI 工具支持的操作运行和运行应用程序的所有 Python 相关选项的概述：</p> 
<table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td>-py,–python</td><td>具有程序入口点的 Python 脚本。可以使用 --pyFiles 选项配置依赖资源。</td></tr><tr><td>-pym,–pyModule</td><td>具有程序入口点的 Python 模块。此选项必须与 --pyFiles 结合使用。</td></tr><tr><td>-pyfs,–pyFiles</td><td>附加作业的自定义文件。标准资源文件后缀，如.py/.egg/.zip/.whl 或目录。这些文件将被添加到本地客户端和远程 python UDF 工作线程的 PYTHONPATH 中。后缀为.zip的文件将被提取并添加到PYTHONPATH中。逗号（‘，’）可以用作指定多个文件的分隔符（例如，–pyFiles file：///tmp/myresource.zip，hdfs：///$namenode_address/myresource2.zip）。</td></tr><tr><td>-pyarch,–pyArchives</td><td>为作业添加 python 存档文件。归档文件将被解压缩到 python UDF worker 的工作目录中。对于每个归档文件，指定一个目标目录。如果指定了目标目录名，则归档文件将被提取到具有指定名称的目录中。否则，存档文件将被提取到与存档文件同名的目录中。通过此选项上传的文件可通过相对路径访问。“#”可用作归档文件路径和目标目录名称的分隔符。逗号 （‘，’） 可用作指定多个存档文件的分隔符。此选项可用于上传虚拟环境，Python UDF中使用的数据文件（例如，–pyArchives file:///tmp/py37.zip,file:///tmp/data.zip#data --pyExecutable py37.zip/bin/python）。数据文件可以在Python UDF中访问，例如：f = open（‘data/data.txt’， ‘r’）。</td></tr><tr><td>-pyclientexec,–pyClientExecutable</td><td>Python 解释器的路径，用于在通过 “flink run”提交 Python 作业或编译包含 Python UDF 的 Java/Scala 作业时启动 Python 进程。（例如，–pyArchives file:///tmp/py37.zip --pyClientExecutable py37.zip/py37/python）</td></tr><tr><td>-pyexec,–pyExecutable</td><td>指定用于执行 python UDF worker 的 python 解释器的路径（例如：–pyExecutable /usr/local/bin/python3）。python UDF worker依赖于Python 3.6+，Apache Beam（版本== 2.27.0），Pip（版本&gt;= 7.1.0）和SetupTools（版本&gt;= 37.0.0）。请确保指定的环境符合上述要求。</td></tr><tr><td>-pyreq,–pyRequirements</td><td>指定要求.txt定义第三方依赖项的文件。这些依赖项将被安装并添加到 python UDF worker 的 PYTHONPATH 中。可以选择指定包含这些依赖项的安装包的目录。如果可选参数存在，请使用“#”作为分隔符（例如，–pyRequirements file:///tmp/requirements.txt#file:///tmp/cached_dir）。</td></tr></tbody></table> 
<p>除了提交作业期间的命令行选项外，它还支持通过代码中的配置或 Python API 指定依赖项。有关更多详细信息，请参阅依赖项管理。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/472172964e13af06054a339578b9f1d0/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">windows安装paddlepaddle遇到的问题及解决方案</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9893bd17839e75ea0d711e8737d95189/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">VSCode中值得推荐的常用的33个高效前端插件「效率篇」（一）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>