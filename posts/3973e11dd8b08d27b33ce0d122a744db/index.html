<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【头歌实训】PySpark Streaming 入门 - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【头歌实训】PySpark Streaming 入门" />
<meta property="og:description" content="文章目录 第1关：SparkStreaming 基础 与 套接字流任务描述相关知识Spark Streaming 简介Python 与 Spark StreamingPython Spark Streaming APISpark Streaming 初体验（套接字流） 编程要求测试说明答案代码 第2关：文件流任务描述相关知识文件流概述Python 与 Spark Streaming 文件流Spark Streaming 文件流初体验 编程要求测试说明答案代码 第3关：RDD 队列流任务描述相关知识队列流概述Python 与 Spark Streaming 队列流Spark Streaming 队列流初体验 编程要求测试说明答案代码 第1关：SparkStreaming 基础 与 套接字流 任务描述 本关任务：使用 Spark Streaming 实现词频统计。
相关知识 为了完成本关任务，你需要掌握：
Spark Streaming 简介；Python 与 Spark Streaming；Python Spark Streaming API；Spark Streaming 初体验（套接字流）。 Spark Streaming 简介 Spark Streaming 是 Spark 的核心组件之一，为 Spark 提供了可拓展、高吞吐、容错的流计算能力。如下图所示，Spark Streaming 可整合多种输入数据源，如 Kafka、Flume、HDFS，甚至是普通的 TCP 套接字。经处理后的数据可存储至文件系统、数据库，或显示在仪表盘里。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/3973e11dd8b08d27b33ce0d122a744db/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-27T17:48:16+08:00" />
<meta property="article:modified_time" content="2023-12-27T17:48:16+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【头歌实训】PySpark Streaming 入门</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#1SparkStreaming____1" rel="nofollow">第1关：SparkStreaming 基础 与 套接字流</a></li><li><ul><li><a href="#_3" rel="nofollow">任务描述</a></li><li><a href="#_7" rel="nofollow">相关知识</a></li><li><ul><li><a href="#Spark_Streaming__16" rel="nofollow">Spark Streaming 简介</a></li><li><a href="#Python__Spark_Streaming_28" rel="nofollow">Python 与 Spark Streaming</a></li><li><a href="#Python_Spark_Streaming_API_76" rel="nofollow">Python Spark Streaming API</a></li><li><a href="#Spark_Streaming__133" rel="nofollow">Spark Streaming 初体验（套接字流）</a></li></ul> 
   </li><li><a href="#_267" rel="nofollow">编程要求</a></li><li><a href="#_299" rel="nofollow">测试说明</a></li><li><a href="#_303" rel="nofollow">答案代码</a></li></ul> 
  </li><li><a href="#2_381" rel="nofollow">第2关：文件流</a></li><li><ul><li><a href="#_383" rel="nofollow">任务描述</a></li><li><a href="#_387" rel="nofollow">相关知识</a></li><li><ul><li><a href="#_395" rel="nofollow">文件流概述</a></li><li><a href="#Python__Spark_Streaming__407" rel="nofollow">Python 与 Spark Streaming 文件流</a></li><li><a href="#Spark_Streaming__431" rel="nofollow">Spark Streaming 文件流初体验</a></li></ul> 
   </li><li><a href="#_535" rel="nofollow">编程要求</a></li><li><a href="#_563" rel="nofollow">测试说明</a></li><li><a href="#_567" rel="nofollow">答案代码</a></li></ul> 
  </li><li><a href="#3RDD__641" rel="nofollow">第3关：RDD 队列流</a></li><li><ul><li><a href="#_643" rel="nofollow">任务描述</a></li><li><a href="#_647" rel="nofollow">相关知识</a></li><li><ul><li><a href="#_655" rel="nofollow">队列流概述</a></li><li><a href="#Python__Spark_Streaming__663" rel="nofollow">Python 与 Spark Streaming 队列流</a></li><li><a href="#Spark_Streaming__691" rel="nofollow">Spark Streaming 队列流初体验</a></li></ul> 
   </li><li><a href="#_747" rel="nofollow">编程要求</a></li><li><a href="#_762" rel="nofollow">测试说明</a></li><li><a href="#_766" rel="nofollow">答案代码</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="1SparkStreaming____1"></a>第1关：SparkStreaming 基础 与 套接字流</h2> 
<h3><a id="_3"></a>任务描述</h3> 
<p>本关任务：使用 Spark Streaming 实现词频统计。</p> 
<h3><a id="_7"></a>相关知识</h3> 
<p>为了完成本关任务，你需要掌握：</p> 
<ol><li>Spark Streaming 简介；</li><li>Python 与 Spark Streaming；</li><li>Python Spark Streaming API；</li><li>Spark Streaming 初体验（套接字流）。</li></ol> 
<h4><a id="Spark_Streaming__16"></a>Spark Streaming 简介</h4> 
<p>Spark Streaming 是 Spark 的核心组件之一，为 Spark 提供了可拓展、高吞吐、容错的流计算能力。如下图所示，Spark Streaming 可整合多种输入数据源，如 Kafka、Flume、HDFS，甚至是普通的 TCP 套接字。经处理后的数据可存储至文件系统、数据库，或显示在仪表盘里。</p> 
<p><img src="https://images2.imgbox.com/17/e0/ouTJKj0X_o.png" alt="img"></p> 
<p>Spark Streaming 的基本原理是将实时输入数据流以时间片（秒级）为单位进行拆分，然后经 Spark 引擎以类似批处理的方式处理每个时间片数据，执行流程如下图所示。</p> 
<p><img src="https://images2.imgbox.com/6b/a0/jiTCHOKI_o.png" alt="img"></p> 
<p>Spark Streaming 最主要的抽象是 DStream（Discretized Stream，离散化数据流），表示连续不断的数据流。在内部实现上，Spark Streaming 的输入数据按照时间片（如 1 秒）分成一段一段的 DStream，每一段数据转换为 Spark 中的 RDD，并且对 DStream 的操作都最终转变为对相应的 RDD 的操作。例如，下图展示了进行单词统计时，每个时间片的数据（存储句子的 RDD）经 flatMap 操作，生成了存储单词的 RDD。整个流式计算可根据业务的需求对这些中间的结果进一步处理，或者存储到外部设备中。</p> 
<h4><a id="Python__Spark_Streaming_28"></a>Python 与 Spark Streaming</h4> 
<p>在 Python 中使用 Spark Streaming 只需要下载 <code>pyspark</code> 扩展库即可，命令如下：</p> 
<pre><code class="prism language-sh">pip <span class="token function">install</span> pyspark
</code></pre> 
<p>键入命令后，等待下载完成。</p> 
<p><img src="https://images2.imgbox.com/f2/3a/9FPyhzRP_o.png" alt="img"></p> 
<p>出现如上图所示，则表示安装完成。</p> 
<p><strong>创建 Spark Streaming 的上下文对象：</strong></p> 
<p><strong>方式一</strong></p> 
<pre><code class="prism language-py"><span class="token comment"># 导入包</span>
<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkContext<span class="token punctuation">,</span> SparkConf
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>streaming <span class="token keyword">import</span> StreamingContext
<span class="token comment"># 通过 SparkConf() 设置配置参数</span>
sparkConf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span>
sparkConf<span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">'demo'</span><span class="token punctuation">)</span>
sparkConf<span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">'local[*]'</span><span class="token punctuation">)</span>
<span class="token comment"># 创建 spark 上下文对象</span>
sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>sparkConf<span class="token punctuation">)</span>
<span class="token comment"># 创建 Spark Streaming 上下文对象</span>
<span class="token comment"># 参数1：spark 上下文对象</span>
<span class="token comment"># 参数2：读取间隔时间（秒）</span>
ssc <span class="token operator">=</span> StreamingContext<span class="token punctuation">(</span>sc<span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>方式二</strong></p> 
<pre><code class="prism language-py"><span class="token comment"># 导入包</span>
<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkContext
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>streaming <span class="token keyword">import</span> StreamingContext
<span class="token comment"># 创建 spark 上下文对象</span>
sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">,</span> <span class="token string">"demo"</span><span class="token punctuation">)</span>
<span class="token comment"># 创建 Spark Streaming 上下文对象</span>
<span class="token comment"># 参数1：spark 上下文对象</span>
<span class="token comment"># 参数2：读取间隔时间（秒）</span>
ssc <span class="token operator">=</span> StreamingContext<span class="token punctuation">(</span>sc<span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="Python_Spark_Streaming_API_76"></a>Python Spark Streaming API</h4> 
<p>在 <code>pyspark</code> 库中有很多丰富的 API 提供使用，下面将介绍常用的一些 API。</p> 
<p><strong>Spark Streaming 核心 API</strong></p> 
<table><thead><tr><th>名称</th><th>释义</th></tr></thead><tbody><tr><td>StreamingContext(sparkContext[, …])</td><td>Spark Streaming 功能的主要入口点。</td></tr><tr><td>DStream（jdstream、ssc、jrdd_deserializer）</td><td>离散流 (DStream) 是 Spark Streaming 中的基本抽象，是表示连续数据流的 RDD 的连续序列（相同类型）。</td></tr></tbody></table> 
<p><strong>Spark Streaming 操作 API</strong></p> 
<table><thead><tr><th>名称</th><th>释义</th></tr></thead><tbody><tr><td>StreamingContext.addStreamingListener（…）</td><td>添加一个 [[org.apache.spark.streaming.scheduler.StreamingListener]] 对象，用于接收与流相关的系统事件。</td></tr><tr><td>StreamingContext.awaitTermination（[timeout]）</td><td>等待执行停止。</td></tr><tr><td>StreamingContext.awaitTerminationOrTimeout（[timeout]）</td><td>等待执行停止。</td></tr><tr><td>StreamingContext.checkpoint（directory）</td><td>设置上下文以定期检查 DStream 操作以实现主控容错。</td></tr><tr><td>StreamingContext.getActive()</td><td>返回当前活动的 StreamingContext 或无。</td></tr><tr><td>StreamingContext.getActiveOrCreate（……）</td><td>要么返回活动的 StreamingContext（即当前已启动但未停止），要么从检查点数据重新创建 StreamingContext 或使用提供的 setupFunc 函数创建新的 StreamingContext。</td></tr><tr><td>StreamingContext.remember（duration）</td><td>在此上下文中设置每个 DStreams 以记住它在最后给定持续时间内生成的 RDD。</td></tr><tr><td>StreamingContext.sparkContext</td><td>返回与此 StreamingContext 关联的 SparkContext。</td></tr><tr><td>StreamingContext.start()</td><td>开始执行流。</td></tr><tr><td>StreamingContext.stop（[stopSparkContext，…]）</td><td>停止流的执行，可选择确保所有接收到的数据都已处理。</td></tr><tr><td>StreamingContext.transform（dstreams，……）</td><td>创建一个新的 DStream，其中每个 RDD 都是通过在 DStream 的 RDD 上应用函数来生成的。</td></tr><tr><td>StreamingContext.union(*dstreams)</td><td>从多个相同类型和相同幻灯片时长的 DStream 创建一个统一的 DStream。</td></tr></tbody></table> 
<p><strong>输入与输出 API</strong></p> 
<table><thead><tr><th>名称</th><th>释义</th></tr></thead><tbody><tr><td>StreamingContext.binaryRecordsStream（……）</td><td>创建一个输入流，用于监控与 Hadoop 兼容的文件系统中的新文件，并将它们作为具有固定长度记录的平面二进制文件读取。</td></tr><tr><td>StreamingContext.queueStream(rdds[, …])</td><td>从 RDD 或列表的队列中创建输入流。</td></tr><tr><td>StreamingContext.socketTextStream（hostname, port）</td><td>从 TCP 源主机名创建输入：端口。</td></tr><tr><td>StreamingContext.textFileStream（directory）</td><td>创建一个输入流，用于监视与 Hadoop 兼容的文件系统中的新文件并将它们作为文本文件读取。</td></tr><tr><td>DStream.pprint（[num]）</td><td>打印此 DStream 中生成的每个 RDD 的前 num 个元素。</td></tr><tr><td>DStream.saveAsTextFiles（prefix[, suffix]）</td><td>将此 DStream 中的每个 RDD 保存为文本文件，使用元素的字符串表示。</td></tr></tbody></table> 
<p><strong>常用的转换与操作 API</strong></p> 
<table><thead><tr><th>名称</th><th>释义</th></tr></thead><tbody><tr><td>DStream.count()</td><td>返回一个新的 DStream，其中每个 RDD 都有一个元素，该元素是通过计算此 DStream 的每个 RDD 生成的。</td></tr><tr><td>DStream.countByValue()</td><td>返回一个新的 DStream，其中每个 RDD 包含此 DStream 的每个 RDD 中每个不同值的计数。</td></tr><tr><td>DStream.filter（F）</td><td>返回一个新的 DStream，仅包含满足条件的元素。</td></tr><tr><td>DStream.flatMap(f[,preservesPartitioning])</td><td>通过对该 DStream 的所有元素应用一个函数，然后将结果展平，返回一个新的 DStream。</td></tr><tr><td>DStream.flatMapValues（F）</td><td>通过将 flatmap 函数应用于此 DStream 中每个键值对的值而不更改键，返回一个新的 DStream。</td></tr><tr><td>DStream.foreachRDD（func）</td><td>对这个 DStream 中的每个 RDD 应用一个函数。</td></tr><tr><td>DStream.groupByKey([numPartitions])</td><td>通过在每个 RDD 上应用 groupByKey 返回一个新的 DStream。</td></tr><tr><td>DStream.join（other[，numPartitions]）</td><td>通过在这个 DStream 和其他DStream 的 RDD 之间应用 ‘join’ 返回一个新的DStream。</td></tr><tr><td>DStream.map(f[,preservesPartitioning])</td><td>通过对 DStream 的每个元素应用一个函数来返回一个新的 DStream。</td></tr><tr><td>DStream.mapValues（F）</td><td>通过对该 DStream 中每个键值对的值应用映射函数返回一个新的 DStream，而不更改键。</td></tr><tr><td>DStream.reduce（func）</td><td>返回一个新的 DStream，其中每个 RDD 具有通过减少此 DStream 的每个 RDD 生成的单个元素。</td></tr><tr><td>DStream.reduceByKey(func[,numPartitions])</td><td>通过对每个 RDD 应用 reduceByKey 来返回一个新的 DStream。</td></tr><tr><td>DStream.updateStateByKey(updateFunc[, …])</td><td>返回一个新的“状态” DStream，其中每个键的状态通过对键的先前状态和键的新值应用给定函数来更新。</td></tr></tbody></table> 
<h4><a id="Spark_Streaming__133"></a>Spark Streaming 初体验（套接字流）</h4> 
<p>下面让我们快速了解一下简单的 Spark Streaming 程序是什么样的，假设我们要计算从侦听 TCP 套接字的数据服务器接收到的文本数据中的字数，实现一个流式的 WordCount 计算程序。</p> 
<p><strong>第一步，导入包</strong></p> 
<p>打开右侧<strong>命令行窗口</strong>，等待连接后，在主目录下创建文件 <code>test.py</code>，导入 Spark Streaming 所需要的包。</p> 
<pre><code class="prism language-py">touch test<span class="token punctuation">.</span>py
<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkContext
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>streaming <span class="token keyword">import</span> StreamingContext
</code></pre> 
<p><strong>第二步，创建上下文对象</strong></p> 
<p>首先，我们导入StreamingContext，它是所有流功能的主要入口点。创建一个具有多个执行线程的本地 StreamingContext，批处理间隔为 20 秒。</p> 
<pre><code class="prism language-py">sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">,</span> <span class="token string">"demo"</span><span class="token punctuation">)</span>
<span class="token comment"># 每 20 秒读取一次</span>
ssc <span class="token operator">=</span> StreamingContext<span class="token punctuation">(</span>sc<span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span> 
</code></pre> 
<p><strong>第三步，指定数据流</strong></p> 
<p>使用这个上下文，我们可以创建一个表示来自 TCP 源的流数据的 DStream，指定为主机名（例如：localhost）和端口（例如：7777）。</p> 
<pre><code class="prism language-py">lines <span class="token operator">=</span> ssc<span class="token punctuation">.</span>socketTextStream<span class="token punctuation">(</span><span class="token string">"localhost"</span><span class="token punctuation">,</span> <span class="token number">7777</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>第四步，分词统计与输出</strong></p> 
<p>接下来，我们要按空格（根据数据流的情况来）将行拆分为单词。</p> 
<pre><code class="prism language-py">words <span class="token operator">=</span> lines<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span><span class="token keyword">lambda</span> line<span class="token punctuation">:</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>在这种情况下，每一行将被拆分为多个单词，单词流表示为 wordsDStream。接下来，我们要计算这些单词，输出结果到屏幕。</p> 
<pre><code class="prism language-py">pairs <span class="token operator">=</span> words<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> word<span class="token punctuation">:</span> <span class="token punctuation">(</span>word<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
wordCounts <span class="token operator">=</span> pairs<span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">,</span> y<span class="token punctuation">:</span> x <span class="token operator">+</span> y<span class="token punctuation">)</span>
wordCounts<span class="token punctuation">.</span>pprint<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>第五步，启动与停止</strong></p> 
<p>请注意，当执行这些行时，Spark Streaming 仅设置它在启动时将执行的计算，并且尚未开始真正的处理。在所有转换设置完成后才开始处理，所以我们最后调用。</p> 
<pre><code class="prism language-py"><span class="token comment"># 开始执行流</span>
ssc<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 等待计算终止</span>
ssc<span class="token punctuation">.</span>awaitTermination<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>启动前我们需要先新开一个命令行窗口用于创建数据流服务器发送端</strong>。点击右侧 <code>+</code> 号，新增一个命令行窗口，启动数据流服务器。</p> 
<pre><code class="prism language-py">nc <span class="token operator">-</span>l <span class="token operator">-</span>p <span class="token number">7777</span>
</code></pre> 
<blockquote> 
 <p>必须先启动数据流服务器，然后再开始执行程序。</p> 
</blockquote> 
<p>回到刚刚的代码窗口，启动程序，开始监听。启动后，我们切换到数据流服务器窗口，输入如下单词：</p> 
<pre><code class="prism language-py">hello python
hello spark
hello spark streaming
</code></pre> 
<p>代码窗口界面结果输出如下：</p> 
<p><img src="https://images2.imgbox.com/d0/7d/W5WxPdZt_o.png" alt=","></p> 
<p>当我们在数据流服务器窗口再次输入和上面一样的单词时，发现结果没有进行累加，如下所示：</p> 
<p><img src="https://images2.imgbox.com/02/63/d5ZEteay_o.png" alt=","></p> 
<p>这是由于我们并没有实现更新的操作，我们需要使用 <code>updateStateByKey(func)</code> 方法对其进行累加统计，其参数为一个函数，也就是根据传入的这个函数来实现状态更新功能。</p> 
<p>当我们使用累加器时还需借助 <code>checkpoint()</code> 方法设置检查点，告知累加器其检查区域，其参数为一个字符串，指定为保存检查点的目录，如果指定目录未存在，则会自动创建。</p> 
<p>具体实现方式如下代码所示：</p> 
<pre><code class="prism language-py"><span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkContext
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>streaming <span class="token keyword">import</span> StreamingContext
<span class="token comment"># 累加器(状态更新)</span>
<span class="token keyword">def</span> <span class="token function">updateFunction</span><span class="token punctuation">(</span>newValues<span class="token punctuation">,</span> runningCount<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> runningCount <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        runningCount <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">return</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>newValues<span class="token punctuation">,</span> runningCount<span class="token punctuation">)</span>
sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">,</span> <span class="token string">"demo"</span><span class="token punctuation">)</span>
<span class="token comment"># 设置输入日志等级</span>
sc<span class="token punctuation">.</span>setLogLevel<span class="token punctuation">(</span><span class="token string">"ERROR"</span><span class="token punctuation">)</span>
ssc <span class="token operator">=</span> StreamingContext<span class="token punctuation">(</span>sc<span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span>
<span class="token comment"># 设置检查点</span>
ssc<span class="token punctuation">.</span>checkpoint<span class="token punctuation">(</span><span class="token string">"file:///usr/local/word_log"</span><span class="token punctuation">)</span>
<span class="token comment"># 指定监听端口</span>
lines <span class="token operator">=</span> ssc<span class="token punctuation">.</span>socketTextStream<span class="token punctuation">(</span><span class="token string">"localhost"</span><span class="token punctuation">,</span> <span class="token number">7777</span><span class="token punctuation">)</span>
<span class="token comment"># 进行词频统计</span>
words <span class="token operator">=</span> lines<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span><span class="token keyword">lambda</span> line<span class="token punctuation">:</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
pairs <span class="token operator">=</span> words<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> word<span class="token punctuation">:</span> <span class="token punctuation">(</span>word<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 调用累加器</span>
wordCounts <span class="token operator">=</span> pairs<span class="token punctuation">.</span>updateStateByKey<span class="token punctuation">(</span>updateFunction<span class="token punctuation">)</span>
<span class="token comment"># 输出到屏幕</span>
wordCounts<span class="token punctuation">.</span>pprint<span class="token punctuation">(</span><span class="token punctuation">)</span>
ssc<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>
ssc<span class="token punctuation">.</span>awaitTermination<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>**运行程序，注意，先启动数据流服务器。**输入如下数据两次，请在第一次数据输出到屏幕上后再输入第二次：</p> 
<pre><code class="prism language-py">hello python
hello spark
hello spark streaming
</code></pre> 
<p>第一次结果如下：</p> 
<p><img src="https://images2.imgbox.com/0a/c2/8sgG4sOe_o.png" alt=","></p> 
<p>第二次结果如下：</p> 
<p><img src="https://images2.imgbox.com/14/ee/9IvpPKSp_o.png" alt=","></p> 
<p>从结果中可以看出，我们已经实现了从套接字流中读取数据并完成词频统计。</p> 
<h3><a id="_267"></a>编程要求</h3> 
<p>打开右侧代码文件窗口，在 <strong>Begin</strong> 至 <strong>End</strong> 区域补充代码，执行程序，读取套接字流数据，按空格进行分词，完成词频统计。补充代码，将词频统计的输出内容存储到 <code>/data/workspace/myshixun/project/step1/result</code> 文件中。</p> 
<p><strong>代码文件目录：</strong> <code>/data/workspace/myshixun/project/step1/work.py</code></p> 
<p><strong>套接字流相关信息：</strong></p> 
<ul><li>地址：<code>localhost</code></li><li>端口：<code>8888</code></li><li>输入数据：</li></ul> 
<p>程序启动后（5s），<strong>请在 60 秒内写入数据</strong>，如果需要调整时间，你可以通过修改代码文件中 <code>ssc.awaitTermination(timeout=60) </code> 的 <code>timeout</code> 指定时间。</p> 
<pre><code class="prism language-py">It <span class="token keyword">is</span> believed that the computer <span class="token keyword">is</span> bringing the world into a brand new era<span class="token punctuation">.</span> 
At the time the computer was invented<span class="token punctuation">,</span> scientists<span class="token punctuation">,</span> marveling at its calculating speed<span class="token punctuation">,</span> 
felt that they had created a miracle<span class="token punctuation">.</span>
Nowadays<span class="token punctuation">,</span> the function of the computer <span class="token keyword">is</span> no longer confined to calculation<span class="token punctuation">;</span> 
It permeates peoples daily lives <span class="token keyword">and</span> has become an inseparable part of human society<span class="token punctuation">.</span>
</code></pre> 
<blockquote> 
 <p>输入内容后，注意按回车。</p> 
</blockquote> 
<p>检查点存放本地目录：<code>/root/mylog/</code></p> 
<p><strong>请在程序运行完成后再点击评测，否则会影响评测结果。</strong></p> 
<p>小贴士：</p> 
<ul><li><code>pprint()</code> 方法中可以设置数据输出显示的数量。</li></ul> 
<h3><a id="_299"></a>测试说明</h3> 
<p>平台将对你编写的代码进行评测，如果与预期结果一致，则通关，否则测试失败。</p> 
<h3><a id="_303"></a>答案代码</h3> 
<p>先写入代码</p> 
<pre><code class="prism language-py"><span class="token comment">#!/usr/local/bin/python</span>
<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkContext
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>streaming <span class="token keyword">import</span> StreamingContext


<span class="token comment"># 累加器(状态更新)</span>
<span class="token keyword">def</span> <span class="token function">updateFunction</span><span class="token punctuation">(</span>newValues<span class="token punctuation">,</span> runningCount<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> runningCount <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        runningCount <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">return</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>newValues<span class="token punctuation">,</span> runningCount<span class="token punctuation">)</span>


sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">,</span> <span class="token string">"work"</span><span class="token punctuation">)</span>

ssc <span class="token operator">=</span> StreamingContext<span class="token punctuation">(</span>sc<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>

<span class="token comment">###################### Begin ######################</span>
<span class="token comment"># 设置检查点</span>
ssc<span class="token punctuation">.</span>checkpoint<span class="token punctuation">(</span><span class="token string">"/root/mylog/"</span><span class="token punctuation">)</span>
<span class="token comment"># 指定监听端口</span>
lines <span class="token operator">=</span> ssc<span class="token punctuation">.</span>socketTextStream<span class="token punctuation">(</span><span class="token string">"localhost"</span><span class="token punctuation">,</span> <span class="token number">8888</span><span class="token punctuation">)</span>
<span class="token comment"># 进行词频统计</span>
words <span class="token operator">=</span> lines<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span><span class="token keyword">lambda</span> line<span class="token punctuation">:</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
pairs <span class="token operator">=</span> words<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> word<span class="token punctuation">:</span> <span class="token punctuation">(</span>word<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 调用累加器</span>
wordCounts <span class="token operator">=</span> pairs<span class="token punctuation">.</span>updateStateByKey<span class="token punctuation">(</span>updateFunction<span class="token punctuation">)</span>
<span class="token comment"># 输出到屏幕</span>
wordCounts<span class="token punctuation">.</span>pprint<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 保存输出内容到指定文件中</span>
wordCounts<span class="token punctuation">.</span>saveAsTextFiles<span class="token punctuation">(</span><span class="token string">"/data/workspace/myshixun/project/step1/result"</span><span class="token punctuation">,</span><span class="token string">"txt"</span><span class="token punctuation">)</span>

<span class="token comment">###################### End ######################</span>

ssc<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>

ssc<span class="token punctuation">.</span>awaitTermination<span class="token punctuation">(</span>timeout<span class="token operator">=</span><span class="token number">60</span><span class="token punctuation">)</span>

 
</code></pre> 
<p>在第一个命令行窗口执行，启动数据流服务器</p> 
<pre><code class="prism language-sh"><span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /root/mylog/
<span class="token builtin class-name">cd</span> /root/mylog/
<span class="token function">nc</span> <span class="token parameter variable">-l</span> <span class="token parameter variable">-p</span> <span class="token number">8888</span>
</code></pre> 
<p>启动程序，开始监听后，打开另一个命令行窗口执行</p> 
<pre><code class="prism language-sh"><span class="token builtin class-name">cd</span> /data/workspace/myshixun/project/step1/
<span class="token function">chmod</span> <span class="token number">777</span> work.py
python work.py <span class="token comment"># 现在开始运行代码文件，请在 60 秒内写入下面数据</span>
</code></pre> 
<p>回到第一个命令行窗口下把下面数据粘贴上去，再打一个回车</p> 
<pre><code class="prism language-py">It <span class="token keyword">is</span> believed that the computer <span class="token keyword">is</span> bringing the world into a brand new era<span class="token punctuation">.</span> 
At the time the computer was invented<span class="token punctuation">,</span> scientists<span class="token punctuation">,</span> marveling at its calculating speed<span class="token punctuation">,</span> 
felt that they had created a miracle<span class="token punctuation">.</span>
Nowadays<span class="token punctuation">,</span> the function of the computer <span class="token keyword">is</span> no longer confined to calculation<span class="token punctuation">;</span> 
It permeates peoples daily lives <span class="token keyword">and</span> has become an inseparable part of human society<span class="token punctuation">.</span>
</code></pre> 
<p>再去另一个命令行窗口就可以看到正在统计词频了</p> 
<h2><a id="2_381"></a>第2关：文件流</h2> 
<h3><a id="_383"></a>任务描述</h3> 
<p>本关任务：使用 Spark Streaming 实现文件目录监听，完成词频统计。</p> 
<h3><a id="_387"></a>相关知识</h3> 
<p>为了完成本关任务，你需要掌握：</p> 
<ol><li>文件流概述；</li><li>Python 与 Spark Streaming 文件流；</li><li>Spark Streaming 文件流初体验。</li></ol> 
<h4><a id="_395"></a>文件流概述</h4> 
<p>文件流就是数据从一个地方流到另一个地方，像一块大蛋糕一样把一个大的文件分成一块一块的流过去就叫文件流。其中流分为输入流与输出流，输入流指从外界向我们的程序中移动的方向，因此是用来获取数据的流，作用就是读操作。输出流与之相反，从程序向外界移动的方向,用来输出数据的流,作用就是写操作。流是单向的,输入用来读，输出用来写。</p> 
<p><img src="https://images2.imgbox.com/62/46/Hy0F7Swj_o.png" alt="img"></p> 
<p>那么我们为什么需要流呢？</p> 
<ul><li>当外部设备与内存中的数据规模不一致，内存小，外部设备大，如果内存大小只有 1G ，但从磁盘读 2G，不能一次读完，这时就需要流。</li><li>当外部设备与内存处理数据的能力不一致，内存处理数据快，外部设备慢，内存给磁盘写了 1G ，磁盘可能需要 5 秒去处理写数据，其他事件就会受到影响，这时就需要流。</li><li>当读取或者写入大文件时数据会推挤在内存中，导致效率低（内存数据多，导致执行时间变长），这时就需要流。</li></ul> 
<h4><a id="Python__Spark_Streaming__407"></a>Python 与 Spark Streaming 文件流</h4> 
<p>Spark 支持从兼容 HDFS API 的文件系统中读取数据，创建数据流。在 Python 中使用 Spark Streaming 文件流十分简单，通过 <code>textFileStream()</code> 方法就可以对创建文件流。</p> 
<p>在 Python 中创建 Spark Streaming 文件流：</p> 
<pre><code class="prism language-py"><span class="token comment"># 导入包</span>
<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkContext<span class="token punctuation">,</span> SparkConf
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>streaming <span class="token keyword">import</span> StreamingContext
<span class="token comment"># 通过 SparkConf() 配置参数</span>
sparkConf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span>
sparkConf<span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">'demo'</span><span class="token punctuation">)</span>
sparkConf<span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">'local[*]'</span><span class="token punctuation">)</span>
<span class="token comment"># 创建 spark 上下文对象</span>
sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>sparkConf<span class="token punctuation">)</span>
<span class="token comment"># 创建 Spark Streaming 上下文对象</span>
<span class="token comment"># 参数1：spark 上下文对象</span>
<span class="token comment"># 参数2：读取间隔时间（秒）</span>
ssc <span class="token operator">=</span> StreamingContext<span class="token punctuation">(</span>sc<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
<span class="token comment"># 指定目录或文件，创建文件流</span>
ssc<span class="token punctuation">.</span>textFileStream<span class="token punctuation">(</span><span class="token string">"xxxxxx"</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="Spark_Streaming__431"></a>Spark Streaming 文件流初体验</h4> 
<p>通过对文件流及其创建方法的了解，我们现在通过实际的文件流案例来学习 Spark Streaming 读取文件流的具体实现。</p> 
<p>打开右侧命令行窗口，创建一个目录 <code>test</code>，并在里面创建两个子文件 <code>log1.txt</code>、<code>log2.txt</code>，用于模拟数据。</p> 
<pre><code class="prism language-sh"><span class="token builtin class-name">cd</span> /root
<span class="token function">mkdir</span> <span class="token builtin class-name">test</span>
<span class="token builtin class-name">cd</span> /root/test
<span class="token function">touch</span> log1.txt log2.txt
</code></pre> 
<p>创建完成后，我们在新建的两个文件 <code>log1.txt</code> 和 <code>log2.txt</code> 中任意写入一些数据。</p> 
<pre><code class="prism language-sh"><span class="token builtin class-name">echo</span> <span class="token parameter variable">-e</span> <span class="token string">"hello python <span class="token entity" title="\n">\n</span>hello spark streaming <span class="token entity" title="\n">\n</span>I love big data!"</span> <span class="token operator">&gt;</span> /root/test/log1.txt
<span class="token builtin class-name">echo</span> <span class="token parameter variable">-e</span> <span class="token string">"hello python <span class="token entity" title="\n">\n</span>hello spark streaming <span class="token entity" title="\n">\n</span>I love big data!"</span> <span class="token operator">&gt;</span> /root/test/log2.txt
</code></pre> 
<p>下面我们就进入 <code>python shell</code> 界面，创建文件流。</p> 
<pre><code class="prism language-py">python
</code></pre> 
<p>进入后，出现如下界面：</p> 
<p><img src="https://images2.imgbox.com/8b/b2/m7x8QZEO_o.png" alt=","></p> 
<p><strong>第一步，指定监听目录 <code>/root/test</code>，创建 Spark Streaming 文件流</strong></p> 
<pre><code class="prism language-py"><span class="token comment"># 导入包</span>
<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkContext
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>streaming <span class="token keyword">import</span> StreamingContext
<span class="token comment"># 创建 spark 上下文对象</span>
sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">,</span><span class="token string">"demo"</span><span class="token punctuation">)</span>
<span class="token comment"># 创建 Spark Streaming 上下文对象</span>
ssc <span class="token operator">=</span> StreamingContext<span class="token punctuation">(</span>sc<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
<span class="token comment"># 指定 /root/test 目录，创建文件流</span>
lines <span class="token operator">=</span> ssc<span class="token punctuation">.</span>textFileStream<span class="token punctuation">(</span><span class="token string">"/root/test"</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>第二步，数据处理</strong></p> 
<p>完成对文件流中相关数据的处理。</p> 
<pre><code class="prism language-py">lines<span class="token punctuation">.</span>pprint<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>第三步，启动与停止</strong></p> 
<pre><code class="prism language-py">ssc<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>
ssc<span class="token punctuation">.</span>awaitTermination<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>运行后发现，并没有输出我们之前写入到文件 <code>log1.txt</code> 和 <code>log2.txt</code>中的内容。</p> 
<p><img src="https://images2.imgbox.com/66/6a/htLGrElE_o.png" alt=","></p> 
<p>原因是，程序启动后只会只监听 <code>/root/test</code> 目录下在程序启动后新增的文件，不会去处理历史上已经存在的文件，即使你对其进行更新操作。</p> 
<p>现在我们点击 <code>+</code> 号新增一个命令行窗口，验证是否真的如此。打开新窗口后，切换到监听目录中，创建一个新文件 <code>log3.txt</code>，任意写入一些数据。</p> 
<pre><code class="prism language-sh"><span class="token builtin class-name">cd</span> /root/test
<span class="token function">vi</span> log3.txt
hello python
hello spark
</code></pre> 
<p>此时我们返回程序运行窗口，稍作等待，查看输出内容，发现刚刚创建的文件 <code>log3.txt</code> 其中的内容输出到了屏幕上。</p> 
<p><img src="https://images2.imgbox.com/fb/39/lx9qtkwD_o.png" alt=","></p> 
<p>现在我们来测试更新操作是否会被读取到，对程序运行前创建的文件 <code>log2.txt</code> 进行更新操作，任意追加一些内容。</p> 
<pre><code class="prism language-sh"><span class="token builtin class-name">cd</span> /root/test
<span class="token function">vi</span> log2.txt
I like ping<span class="token operator">!</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/1e/ec/4BeQOoJG_o.png" alt=","></p> 
<p>此时我们返回程序运行窗口，稍作等待，查看输出内容，发现对 <code>log2.txt</code> 文件追加的内容并没有输出到屏幕上。</p> 
<p><img src="https://images2.imgbox.com/14/1d/wbhuISC0_o.png" alt=","></p> 
<p>说明程序运行前监听目录下的文件并不会被识别。</p> 
<p>文件流的扩展知识：</p> 
<ul><li>可以提供 <code>POSIX glob</code> 模式，例如：<code>hdfs://namenode:8040/logs/2017/*</code>，在这里，DStream 将包含与该模式匹配的目录中的所有文件。也就是说：它是目录的模式，而不是目录中的文件。</li><li>所有文件必须采用相同的数据格式。</li><li>文件根据其修改时间而非创建时间被视为时间段的一部分。</li><li>一旦处理完毕，在当前窗口中对文件的更改不会导致文件被重新读取，即：更新被忽略。</li><li>目录下的文件越多，扫描更改所需的时间就越长——即使没有文件被修改。</li><li>如果使用通配符来标识目录，例如：<code>hdfs://namenode:8040/logs/2016-*</code>，重命名整个目录以匹配路径，则会将该目录添加到受监视目录列表中。只有目录中修改时间在当前窗口内的文件才会包含在流中。</li><li>调用FileSystem.setTimes() 修复时间戳是一种在以后的窗口中拾取文件的方法，即使它的内容没有改变。</li></ul> 
<h3><a id="_535"></a>编程要求</h3> 
<p>打开右侧代码文件窗口，在 <strong>Begin</strong> 至 <strong>End</strong> 区域补充代码，执行程序，读取文件流数据，按空格进行分词，完成词频统计。补充代码，将词频统计的输出内容存储到 <code>/data/workspace/myshixun/project/step2/result</code> 文件中。</p> 
<p><strong>代码文件目录：</strong> <code>/data/workspace/myshixun/project/step2/work.py</code></p> 
<p><strong>文件流相关信息：</strong></p> 
<ul><li>监听目录：<code>/root/file_stream</code> (需要自行创建)</li><li>文件名称：<code>words.txt</code> (需要自行创建)</li><li>文件内的数据：</li></ul> 
<p>程序启动后（5s），<strong>请在 60 秒内创建文件并写入数据</strong>，如果需要调整时间，你可以通过修改代码文件中 <code>ssc.awaitTermination(timeout=60) </code>的 <code>timeout</code> 指定时间。</p> 
<pre><code class="prism language-py">Hiding behind the loose dusty curtain<span class="token punctuation">,</span> a teenager packed up his overcoat into the suitcase<span class="token punctuation">.</span>
He planned to leave home at dusk though there was thunder <span class="token keyword">and</span> lightning outdoors<span class="token punctuation">.</span>
As a result<span class="token punctuation">,</span> his score <span class="token keyword">in</span> each exam never added up to over <span class="token number">60</span><span class="token punctuation">,</span> his name <span class="token keyword">is</span> LiMing<span class="token punctuation">.</span>
</code></pre> 
<blockquote> 
 <p>输入内容后，注意保存退出。</p> 
</blockquote> 
<p>检查点存放本地目录：<code>/root/mylog2/</code></p> 
<p>小贴士：</p> 
<ul><li><code>pprint()</code> 方法中可以设置数据输出显示的数量。</li></ul> 
<h3><a id="_563"></a>测试说明</h3> 
<p>平台将对你编写的代码进行评测，如果与预期结果一致，则通关，否则测试失败。</p> 
<h3><a id="_567"></a>答案代码</h3> 
<pre><code class="prism language-py"><span class="token comment">#!/usr/local/bin/python</span>
<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkContext
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>streaming <span class="token keyword">import</span> StreamingContext


<span class="token comment"># 累加器(状态更新)</span>
<span class="token keyword">def</span> <span class="token function">updateFunction</span><span class="token punctuation">(</span>newValues<span class="token punctuation">,</span> runningCount<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> runningCount <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        runningCount <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">return</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>newValues<span class="token punctuation">,</span> runningCount<span class="token punctuation">)</span>


sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">,</span> <span class="token string">"work"</span><span class="token punctuation">)</span>

ssc <span class="token operator">=</span> StreamingContext<span class="token punctuation">(</span>sc<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>

<span class="token comment">###################### Begin ######################</span>
<span class="token comment"># 设置检查点</span>
ssc<span class="token punctuation">.</span>checkpoint<span class="token punctuation">(</span><span class="token string">"/root/mylog2/"</span><span class="token punctuation">)</span>
<span class="token comment"># 指定监听端口</span>
lines <span class="token operator">=</span> ssc<span class="token punctuation">.</span>textFileStream<span class="token punctuation">(</span><span class="token string">"/root/test"</span><span class="token punctuation">)</span>
<span class="token comment"># 进行词频统计</span>
words <span class="token operator">=</span> lines<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span><span class="token keyword">lambda</span> line<span class="token punctuation">:</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
pairs <span class="token operator">=</span> words<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> word<span class="token punctuation">:</span> <span class="token punctuation">(</span>word<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 调用累加器</span>
wordCounts <span class="token operator">=</span> pairs<span class="token punctuation">.</span>updateStateByKey<span class="token punctuation">(</span>updateFunction<span class="token punctuation">)</span>
<span class="token comment"># 输出到屏幕</span>
wordCounts<span class="token punctuation">.</span>pprint<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 保存输出内容到指定文件中</span>
wordCounts<span class="token punctuation">.</span>saveAsTextFiles<span class="token punctuation">(</span><span class="token string">"/data/workspace/myshixun/project/step2/result"</span><span class="token punctuation">,</span><span class="token string">"txt"</span><span class="token punctuation">)</span>

<span class="token comment">###################### End ######################</span>

ssc<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>

ssc<span class="token punctuation">.</span>awaitTermination<span class="token punctuation">(</span>timeout<span class="token operator">=</span><span class="token number">60</span><span class="token punctuation">)</span>

 
</code></pre> 
<p>在第一个命令行窗口执行</p> 
<pre><code class="prism language-sh"><span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /root/test/
<span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /root/mylog2/
<span class="token builtin class-name">cd</span> /data/workspace/myshixun/project/step2/
<span class="token function">chmod</span> <span class="token number">777</span> work.py
python work.py <span class="token comment"># 现在开始运行代码文件，请在 60 秒内创建文件并写入下面数据</span>
</code></pre> 
<p>再打开一个命令行窗口创建文件并写入下面数据</p> 
<pre><code class="prism language-sh"><span class="token function">vim</span> /root/test/words.txt
</code></pre> 
<p>把下面数据粘贴上去</p> 
<pre><code class="prism language-py">Hiding behind the loose dusty curtain<span class="token punctuation">,</span> a teenager packed up his overcoat into the suitcase<span class="token punctuation">.</span>
He planned to leave home at dusk though there was thunder <span class="token keyword">and</span> lightning outdoors<span class="token punctuation">.</span>
As a result<span class="token punctuation">,</span> his score <span class="token keyword">in</span> each exam never added up to over <span class="token number">60</span><span class="token punctuation">,</span> his name <span class="token keyword">is</span> LiMing<span class="token punctuation">.</span>
</code></pre> 
<p>再去另一个命令行窗口就可以看到正在统计词频了</p> 
<h2><a id="3RDD__641"></a>第3关：RDD 队列流</h2> 
<h3><a id="_643"></a>任务描述</h3> 
<p>本关任务：使用 Spark Streaming 实现队列流，完成词频统计。</p> 
<h3><a id="_647"></a>相关知识</h3> 
<p>为了完成本关任务，你需要掌握：</p> 
<ol><li>队列流概述；</li><li>Python 与 Spark Streaming 队列流；</li><li>Spark Streaming 队列流初体验。</li></ol> 
<h4><a id="_655"></a>队列流概述</h4> 
<p>队列是无须的或共享的消息。使用队列消息传递，可以创建多个消费者来从点对点消息传递通道接收消息。当通道传递消息时，任何消费者都可能收到消息。消息传递系统的实现确定哪个消费者实际接收消息。Queuing 通常与无状态应用程序一起使用。无状态应用程序不关心顺序，但它们确实需要识别或删除单个消息的能力，以及尽可能扩展并行消耗的能力。</p> 
<p><img src="https://images2.imgbox.com/e2/06/EXYXIXKe_o.png" alt="img"></p> 
<p>相比之下，流是严格有序的或独占的消息传递。使用流消息传递，始终只有一个消费者使用消息传递通道。消费者接收从通道发送的消息，其顺序与消息的写入顺序一致。Streaming 通常与有状态的应用程序一起使用。有状态应用程序关心消息顺序及其状态。消息的顺序决定有状态应用程序的状态。当发生无序消费时，排序将影响应用程序，需要处理逻辑的正确性。</p> 
<h4><a id="Python__Spark_Streaming__663"></a>Python 与 Spark Streaming 队列流</h4> 
<p>为了使用测试数据测试 Spark Streaming 应用程序，还可以基于 RDD 队列创建 DStream，使用 <code>streamingContext.queueStream(queueOfRDDs)</code>. 每个推入队列的 RDD 都会被视为 DStream 中的一批数据，像流一样处理。</p> 
<p><img src="https://images2.imgbox.com/0f/2d/V55ESwzk_o.png" alt="img"></p> 
<p>在 Python 中创建 Spark Streaming 队列流：</p> 
<pre><code class="prism language-py"><span class="token comment"># 导入包</span>
<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkContext<span class="token punctuation">,</span> SparkConf
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>streaming <span class="token keyword">import</span> StreamingContext
<span class="token comment"># 通过 SparkConf() 配置参数</span>
sparkConf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span>
sparkConf<span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">'demo'</span><span class="token punctuation">)</span>
sparkConf<span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">'local[*]'</span><span class="token punctuation">)</span>
<span class="token comment"># 创建 spark 上下文对象</span>
sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf<span class="token operator">=</span>sparkConf<span class="token punctuation">)</span>
<span class="token comment"># 创建 Spark Streaming 上下文对象</span>
<span class="token comment"># 参数1：spark 上下文对象</span>
<span class="token comment"># 参数2：读取间隔时间（秒）</span>
ssc <span class="token operator">=</span> StreamingContext<span class="token punctuation">(</span>sc<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
<span class="token comment"># 创建列表（RDD）</span>
rddQueue <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"Hello python"</span><span class="token punctuation">,</span> <span class="token string">"Hello spark"</span><span class="token punctuation">,</span> <span class="token string">"Hello spark streaming"</span><span class="token punctuation">]</span>
<span class="token comment"># 创建队列流</span>
inputStream <span class="token operator">=</span> ssc<span class="token punctuation">.</span>queueStream<span class="token punctuation">(</span>rddQueue<span class="token punctuation">)</span>
</code></pre> 
<h4><a id="Spark_Streaming__691"></a>Spark Streaming 队列流初体验</h4> 
<p>通过对队列流及其创建方法的了解，我们现在通过一个案例来学习 Spark Streaming 读取队列流的具体实现。</p> 
<p><strong>打开右侧命令行窗口</strong>，等待连接后，进入 <code>python shell</code> 界面，创建队列流。</p> 
<pre><code class="prism language-py">python
</code></pre> 
<p>进入后，出现如下界面：</p> 
<p><img src="https://images2.imgbox.com/93/60/MoNJ5YzW_o.png" alt=","></p> 
<p><strong>第一步，创建 Spark Streaming 队列流</strong></p> 
<pre><code class="prism language-py"><span class="token comment"># 导入包</span>
<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkContext
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>streaming <span class="token keyword">import</span> StreamingContext
<span class="token comment"># 创建 spark 上下文对象</span>
sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">,</span><span class="token string">"demo"</span><span class="token punctuation">)</span>
<span class="token comment"># 创建 Spark Streaming 上下文对象</span>
ssc <span class="token operator">=</span> StreamingContext<span class="token punctuation">(</span>sc<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
<span class="token comment"># 创建列表（RDD）</span>
rddQueue <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"Hello python"</span><span class="token punctuation">,</span> <span class="token string">"Hello spark"</span><span class="token punctuation">,</span> <span class="token string">"Hello spark streaming"</span><span class="token punctuation">]</span>
<span class="token comment"># 创建队列流</span>
inputStream <span class="token operator">=</span> ssc<span class="token punctuation">.</span>queueStream<span class="token punctuation">(</span>rddQueue<span class="token punctuation">)</span>
</code></pre> 
<p><strong>第二步，数据处理</strong></p> 
<p>完成对队列流中相关数据的处理。</p> 
<pre><code class="prism language-py">inputStream<span class="token punctuation">.</span>pprint<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>第三步，启动与停止</strong></p> 
<pre><code class="prism language-py">ssc<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 检测到没有数据流输入后就会停止</span>
ssc<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>运行后发现，并没有一次输出所有的数据，而是依次的进行输出处理。</p> 
<p><img src="https://images2.imgbox.com/7d/83/yAqAnoM0_o.png" alt="img"></p> 
<p><img src="https://images2.imgbox.com/a6/da/odiZ6bCd_o.png" alt="img"></p> 
<p><strong>…</strong></p> 
<p>这就是 Spark Streaming 队列流的特性，我们在使用时需要注意。</p> 
<h3><a id="_747"></a>编程要求</h3> 
<p>打开右侧代码文件窗口，在 <strong>Begin</strong> 至 <strong>End</strong> 区域补充代码，根据所给出的 <code>rdd</code> 列表，创建队列流，按空格进行分词，完成词频统计，使用 <code>pprint()</code> 输出结果。</p> 
<p><strong>词频统计要求：</strong></p> 
<ul><li>对数据按照 26 个字母进行扁平化统计，例如：<code>('g', 10)</code>。</li><li>过滤掉所有为 <code>''</code> 的值。</li></ul> 
<p>检查点存放本地目录：<code>/root/mylog3/</code></p> 
<p>小贴士：</p> 
<ul><li><code>pprint()</code> 方法中可以设置数据输出显示的数量。</li></ul> 
<h3><a id="_762"></a>测试说明</h3> 
<p>平台将对你编写的代码进行评测，如果与预期结果一致，则通关，否则测试失败。</p> 
<h3><a id="_766"></a>答案代码</h3> 
<pre><code class="prism language-sh"><span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /root/mylog3/
</code></pre> 
<pre><code class="prism language-py"><span class="token keyword">import</span> time
<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkContext
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>streaming <span class="token keyword">import</span> StreamingContext


<span class="token comment"># 累加器(状态更新)</span>
<span class="token keyword">def</span> <span class="token function">updateFunction</span><span class="token punctuation">(</span>newValues<span class="token punctuation">,</span> runningCount<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> runningCount <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        runningCount <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">return</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>newValues<span class="token punctuation">,</span> runningCount<span class="token punctuation">)</span>


sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">,</span> <span class="token string">"work"</span><span class="token punctuation">)</span>

ssc <span class="token operator">=</span> StreamingContext<span class="token punctuation">(</span>sc<span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>

<span class="token comment"># rdd 列表</span>
rdd <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"My father is a basketball fan, he watches the NBA match when he is free."</span><span class="token punctuation">,</span>
            <span class="token string">"Because of the effect from my father, I fell in love with basketball when I was very small."</span><span class="token punctuation">,</span>
            <span class="token string">" So when I go to middle school, I join the basketball team in my class"</span><span class="token punctuation">,</span>
            <span class="token string">" I meet many friends who have the same love for basketball."</span><span class="token punctuation">,</span>
            <span class="token string">" We will play basketball after class or sometimes in the weekend, we will play the match with other team."</span><span class="token punctuation">]</span>


<span class="token comment">###################### Begin ######################</span>
<span class="token comment"># 设置检查点</span>
ssc<span class="token punctuation">.</span>checkpoint<span class="token punctuation">(</span><span class="token string">"/root/mylog3/"</span><span class="token punctuation">)</span>

<span class="token comment"># 创建队列流</span>
inputStream <span class="token operator">=</span> ssc<span class="token punctuation">.</span>queueStream<span class="token punctuation">(</span><span class="token punctuation">[</span>sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span>line<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> line <span class="token keyword">in</span> rdd<span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 按空格进行分词</span>
words <span class="token operator">=</span> inputStream<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span><span class="token keyword">lambda</span> line<span class="token punctuation">:</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 过滤掉空字符串</span>
words_filter <span class="token operator">=</span> words<span class="token punctuation">.</span><span class="token builtin">filter</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> word<span class="token punctuation">:</span> word <span class="token operator">!=</span> <span class="token string">''</span><span class="token punctuation">)</span>

<span class="token comment"># 按字母进行扁平化统计</span>
words_flatMap <span class="token operator">=</span> words_filter<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span><span class="token keyword">lambda</span> word<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>letter<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">for</span> letter <span class="token keyword">in</span> word<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 使用 updateStateByKey 进行状态更新</span>
wordCnt <span class="token operator">=</span> words_flatMap<span class="token punctuation">.</span>updateStateByKey<span class="token punctuation">(</span>updateFunction<span class="token punctuation">)</span>

<span class="token comment"># 输出结果</span>
wordCnt<span class="token punctuation">.</span>pprint<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment">###################### End ######################</span>

ssc<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>

time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">30</span><span class="token punctuation">)</span>

ssc<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/303414ebf854be7456c989c7119f8e6c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Android 配置不同应用ID</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/fde557a9c54770388cb1fbee0ae8d564/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Jupyter Notebook 开启远程登录</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>