<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>手把手教你用线性回归预测二手房房价 - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="手把手教你用线性回归预测二手房房价" />
<meta property="og:description" content="导读
在机器学习中，线性模型是一种形式简单但包含机器学习主要建模思想的模型。
线性回归是线性模型的一种典型方法，比如“双十一”中某款产品的销量预测、某数据岗位的薪资水平预测、二手房房价的预测都可以用线性回归来拟合模型。
今天我们就以杭州的二手房的房屋单价为例，看看如何用线性回归进行预测。
2021年杭州二手房挂牌数量累计超过16万套，在新房数量少和排队摇号的限制下，购买二手房已成为杭州人买房更重要的方式。下图是某二手房网站公开的杭州部分地区的二手房房价。
图 | 链家
二手房的市场价格是多种因素综合作用的结果，主要因素有：面积、户型、朝向、是否精装、楼层、建筑形态、所属地段、所属城区和附近是否有地铁等。
我们的目标是预测二手房的均价，输入自变量包括上述特征，因为二手房房屋单价是一个连续数值，所以可以直接建立起由自变量到因变量的线性回归模型。
线性回归的原理推导
给定一组由输入x和输出y构成的数据集D={(x₁,y₁),(x₂,y₂),…,(xₘ,yₘ)}，其中xᵢ=(xᵢ1,xᵢ2,…,xᵢd)，yᵢ∈R。
线性回归就是通过训练学习得到一个线性模型来最大限度地根据输入x拟合输出y。
线性回归试图用上文提到的对房价的影响因素作为输入xᵢ，以房屋单价yᵢ作为输出，学习得到y=wxᵢ&#43;b。
线性回归学习的关键问题在于确定参数w和b，使得拟合输出y与真实输出yᵢ尽可能接近。
在回归任务中，我们通常使用均方误差来度量预测与标签之间的损失，所以回归任务的优化目标就是使得拟合输出和真实输出之间的均方误差最小化，所以有：
为求得w和b的最小化参数w*和b*，可基于式(2-1)分别对w和b求一阶导数并令其为0，对w求导的推导过程如式(2-2)所示：
同理，对参数b求导的推导过程如式(2-3)所示：
基于式(2-2)和式(2-3)，分别令其为0，可解得w和b的最优解表达式为：
这种基于均方误差最小化求解线性回归参数的方法就是著名的最小二乘法。最小二乘法的简单图示如下所示。
下面我们将上述推导过程进行矩阵化以适应多元线性回归问题。
所谓多元问题，就是输入有多个变量，如前述影响薪资水平的因素包括城市、学历、年龄和经验等。为方便矩阵化的最小二乘法的推导，可将参数w和b合并为向量表达形式：
训练集D的输入部分可表示为一个m×d 维的矩阵X，其中d为输入变量的个数。则矩阵X可表示为式（2-6）：
输出y的向量表达式为y=(y1,y2,…,ym)，类似于式（2-1），参数优化目标函数的矩阵化表达式（2-7）为：
根据矩阵微分公式：
可得：
令矩阵XᐪX为满秩矩阵或者正定矩阵，令式（2-13）等于0，可解得参数为：
但有些时候，矩阵XᐪX并不是满秩矩阵，我们通过对XᐪX添加正则化项来使得该矩阵可逆。一个典型的表达式如下：
其中λΙ即为添加的正则化项。在线性回归模型的迭代训练时，基于式（14）直接求解参数的方法并不常用，通常可以使用梯度下降之类的优化算法来求得的最优估计。
线性回归的代码实现
基于一个完整机器学习模型实现的视角，我们从整体编写思路到具体分步实现，使用NumPy实现一个线性回归模型。
按照机器学习三要素——模型、策略和算法的原则，逐步搭建线性回归代码框架。
一、编写思路
线性回归模型的主体较为简单，即y=wTx&#43;b，在具体编写过程中，基于均方损失最小化的优化策略和梯度下降的寻优算法非常关键。一个线性回归模型代码的编写思路如图2-3所示。
图2-3 线性回归模型代码的编写思路
可以看到，图3提供了两种实现方式。一种是基于Numpy的手动实现，也是本文的重点所在。另一种是调用sklearn机器学习库的实现方式，旨在提供对比参考。
二、基于Numpy的代码实现
首先尝试实现线性回归模型的主体部分，包括回归模型公式、均方损失函数和参数求偏导。线性回归模型主体部分的实现如代码清单2-1所示。
在代码清单2-1中，我们尝试将线性回归模型的主体部分定义为linear_loss函数。该函数的输入参数包括训练数据和权重系数，输出为线性回归模型预测值、均方损失、权重参数一阶偏导和偏置一阶偏导。
在给定模型初始参数的情况下，线性回归模型根据训练数据和参数计算出当前均方损失和参数一阶梯度。
然后在linear_loss函数的基础上，定义线性回归模型的训练过程。主要包括参数初始化、迭代训练和梯度下降寻优。
我们可以先定义一个参数初始化函数initialize_params，再基于linear_loss函数和initialize_params函数来定义包含迭代训练和梯度下降寻优的线性回归拟合过程。
参数初始化函数initialize_params如代码清单2-2所示。
在代码清单2-2中，我们输入训练数据的变量维度，即对于线性回归而言，每一个变量都有一个权重系数。输出为初始化为零向量的权重系数和初始化为零的偏置参数。
最后，我们尝试结合linear_loss和initialize_params函数定义线性回归模型训练过程的函数linear_train，如代码清单2-3所示。
在代码清单2-3中，我们首先初始化模型参数，然后对遍历设置训练迭代过程。在每一次迭代过程中，基于linear_loss函数计算当前迭代的预测值、均方损失和梯度，并根据梯度下降法不断更新系数。
在训练过程中记录每一步的损失、每10000次迭代打印当前损失信息、保存更新后的模型参数字典和梯度字典。这样，一个完整的线性回归模型就基本完成了。
基于上述代码实现，我们使用sklearn的diabetes数据集进行测试，其具体信息如表2-1所示。
从sklearn中导入该数据集并将其划分为训练集和测试集，如代码清单2-4所示。
代码清单2-4首先导入sklearn的diabetes公开数据集，获取数据输入和标签并打乱顺序后划分数据集，输出为：
然后我们使用代码清单2-3定义的linear_train函数训练划分后的数据集，如代码清单2-5所示。
在学习率为0.01、迭代次数为200000的条件下，我们得到上述训练参数。训练中的均方损失下降过程如图2-4所示。
图2-4 训练中的均方损失下降过程
基于前述训练参数，我们可以定义一个预测函数对测试集进行预测，如代码清单2-6所示。
代码清单2-6定义了回归模型的预测函数，输入参数为测试集和模型训练参数，然后通过回归表达式即可进行回归预测。
如何衡量预测结果的好坏呢？
除均方损失外，回归模型的一个重要评估指标是R2系数，用来判断模型拟合水平。我们尝试自定义一个R2系数计算方法，并基于该系数计算代码清单2-6预测结果的拟合水平，具体如代码清单2-7所示。
代码清单2-7给出了回归模型R2系数的计算方式。根据总离差平方和、残差平方和以及R2计算公式，我们计算测试集的R2系数。代码清单2-7的输出如下：
0.5334188457463576
可以看到，我们自定义并训练的线性回归模型在该测试集上的R2系数为0.53，结果并不算太好，除了模型的一些超参数需要做一些调整和优化外，可能线性回归模型本身对该数据集拟合效果有限。
三、基于sklearn的模型实现
作为参考对比，这里同样基于sklearn的LinearRegression类给出对于该数据集的拟合效果。LinearRegression函数位于sklearn的linear_model模块下，定义该类的一个线性回归实例后，直接调用其fit方法拟合训练集即可。
参考实现如代码清单2-8所示。
输出如下：
可以看到，在不做任何特征处理的情况下，基于sklearn的线性回归模型在同样的数据集上与我们基于NumPy手写的模型表现差异并不大，这也验证了手写算法的有效性。
本文摘编自《机器学习：公式推导与代码实现》" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/43e7db6d6310e7e1d81228415f2f6807/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-03-01T17:51:00+08:00" />
<meta property="article:modified_time" content="2022-03-01T17:51:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">手把手教你用线性回归预测二手房房价</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p><span title=""></span></p> 
 <p>导读</p> 
 <p>在机器学习中，<strong>线性模型</strong>是一种形式简单但包含机器学习主要建模思想的模型。</p> 
 <p><strong>线性回归</strong>是线性模型的一种典型方法，比如“双十一”中某款产品的销量预测、某数据岗位的薪资水平预测、二手房房价的预测都可以用线性回归来拟合模型。</p> 
 <p>今天我们就以杭州的二手房的房屋单价为例，看看如何用线性回归进行预测。</p> 
 <p>2021年杭州二手房挂牌数量累计超过16万套，在新房数量少和排队摇号的限制下，购买二手房已成为杭州人买房更重要的方式。下图是某二手房网站公开的杭州部分地区的二手房房价。</p> 
 <p><img src="https://images2.imgbox.com/15/39/qlRHaNnY_o.png" alt="46aa89f583ad0440bf6784ddb8ac847f.png"></p> 
 <p>图 | 链家</p> 
 <p>二手房的市场价格是多种因素综合作用的结果，主要因素有：<strong>面积、户型、朝向、是否精装、楼层、建筑形态、所属地段、所属城区和附近是否有地铁等</strong>。</p> 
 <p>我们的目标是<strong>预测二手房的均价</strong>，输入自变量包括上述特征，因为二手房房屋单价是一个连续数值，所以可以直接建立起<strong>由自变量到因变量的线性回归模型</strong>。</p> 
 <p><strong>线性回归的原理推导</strong></p> 
 <p style="text-align:left;">给定一组由输入x和输出y构成的数据集D={(x₁,y₁),(x₂,y₂),…,(xₘ,yₘ)}，其中xᵢ=(xᵢ1,xᵢ2,…,xᵢd)，yᵢ∈R。</p> 
 <p>线性回归就是<strong>通过训练学习得到一个线性模型来最大限度地根据输入x拟合输出y</strong>。</p> 
 <p>线性回归试图用上文提到的对房价的影响因素作为输入xᵢ，以房屋单价yᵢ作为输出，学习得到y=wxᵢ+b。</p> 
 <p>线性回归学习的关键问题在于<strong>确定参数w和b，使得拟合输出y与真实输出yᵢ尽可能接近。</strong></p> 
 <p>在回归任务中，我们通常使用均方误差来度量预测与标签之间的损失，所以回归任务的优化目标就是使得拟合输出和真实输出之间的均方误差最小化，所以有：</p> 
 <p><img src="https://images2.imgbox.com/19/4d/BVASRdcx_o.png" alt="082b1a9af4043c1ddf6767afb01fc6f9.png"></p> 
 <p>为求得w和b的最小化参数w*和b*，可基于式(2-1)分别对w和b求一阶导数并令其为0，对w求导的推导过程如式(2-2)所示：<br></p> 
 <p><img src="https://images2.imgbox.com/db/99/KLdrj2Wf_o.png" alt="802837b7cee133bc4233516ab892a920.png"></p> 
 <p>同理，对参数b求导的推导过程如式(2-3)所示：</p> 
 <p><img src="https://images2.imgbox.com/c8/b4/uVNngKek_o.png" alt="915ea74b65d3bb2023adb48cf7e9d48e.png"></p> 
 <p>基于式(2-2)和式(2-3)，分别令其为0，可解得w和b的最优解表达式为：</p> 
 <p><img src="https://images2.imgbox.com/f9/58/Pfba6Umc_o.png" alt="dc8e3830acb8ac1612b3e815ae24a344.png"></p> 
 <p><img src="https://images2.imgbox.com/2e/ea/LYqoaNQH_o.png" alt="00e7ff8388672d061b74304dbdf15c2a.png"></p> 
 <p>这种基于均方误差最小化求解线性回归参数的方法就是著名的<strong>最小二乘法</strong>。最小二乘法的简单图示如下所示。<br></p> 
 <p><img src="https://images2.imgbox.com/21/c3/2YapAHLY_o.png" alt="18dab3ba777e375aeb02018c8bd6a97f.png"></p> 
 <p>下面我们将上述推导过程进行矩阵化以适应多元线性回归问题。</p> 
 <p>所谓多元问题，就是输入有多个变量，如前述影响薪资水平的因素包括城市、学历、年龄和经验等。为方便矩阵化的最小二乘法的推导，可将参数w和b合并为向量表达形式：</p> 
 <p><img src="https://images2.imgbox.com/80/0b/bZHzTEGB_o.png" alt="d4c51573830c9c8e6830a8941f5a17c8.png"></p> 
 <p>训练集D的输入部分可表示为一个m×d 维的矩阵X，其中d为输入变量的个数。则矩阵X可表示为式（2-6）：</p> 
 <p><img src="https://images2.imgbox.com/6c/34/ombApWgZ_o.png" alt="1e570d0e2a1e1ac04aeec7d328b723f6.png"></p> 
 <p>输出y的向量表达式为y=(y<sub>1</sub>,y<sub>2</sub>,…,y<sub>m</sub>)，类似于式（2-1），参数优化目标函数的矩阵化表达式（2-7）为：</p> 
 <p><img src="https://images2.imgbox.com/5e/8b/4AgflHjV_o.png" alt="c29aa0e4a490bdd67e7a5fe6061ad36e.png"></p> 
 <p><img src="https://images2.imgbox.com/8b/f1/iTsQr9WR_o.png" alt="bb97d9144f9226eac3dfd0d259797005.png"></p> 
 <p>根据矩阵微分公式：</p> 
 <p><img src="https://images2.imgbox.com/1b/ab/JWh6rkP6_o.png" alt="27f69bd249d706f78fe3ca80e872db79.png"></p> 
 <p>可得：<br></p> 
 <p><img src="https://images2.imgbox.com/2c/7a/50oeQFUb_o.png" alt="031887a1e0b110f3427122a0e857c9ef.png"></p> 
 <p>令矩阵XᐪX为满秩矩阵或者正定矩阵，令式（2-13）等于0，可解得参数为：<br></p> 
 <p><img src="https://images2.imgbox.com/c8/c1/5C8p7DGX_o.png" alt="37310cad9d564b9205edf824a3de9044.png"></p> 
 <p>但有些时候，矩阵XᐪX并不是满秩矩阵，我们通过对XᐪX添加正则化项来使得该矩阵可逆。一个典型的表达式如下：<br></p> 
 <p><img src="https://images2.imgbox.com/11/bb/vw7HObUv_o.png" alt="e9d6216a960eaf82f43dfb3944f51270.png"></p> 
 <p>其中λΙ即为添加的正则化项。在线性回归模型的迭代训练时，基于式（14）直接求解参数的方法并不常用，通常可以使用梯度下降之类的优化算法来求得的最优估计。<br></p> 
 <p><strong>线性回归的代码实现</strong></p> 
 <p>基于一个完整机器学习模型实现的视角，我们从整体编写思路到具体分步实现，使用NumPy实现一个线性回归模型。</p> 
 <p>按照机器学习三要素——<strong>模型、策略和算法</strong>的原则，逐步搭建线性回归代码框架。</p> 
 <p><strong>一、编写思路</strong></p> 
 <p>线性回归模型的主体较为简单，即<strong>y=<strong>w<sup>T</sup></strong>x+b</strong>，在具体编写过程中，基于均方损失最小化的优化策略和梯度下降的寻优算法非常关键。一个线性回归模型代码的编写思路如图2-3所示。<br></p> 
 <p><img src="https://images2.imgbox.com/ac/75/MXwtnHGA_o.png" alt="d8f270a4bab7df95f9164607364f73a6.png"></p> 
 <p style="text-align:center;">图2-3 线性回归模型代码的编写思路<br></p> 
 <p>可以看到，图3提供了两种实现方式。一种是基于Numpy的手动实现，也是本文的重点所在。另一种是调用sklearn机器学习库的实现方式，旨在提供对比参考。</p> 
 <p><strong>二、基于Numpy的代码实现</strong></p> 
 <p>首先尝试实现线性回归模型的<strong>主体部分</strong>，包括回归模型公式、均方损失函数和参数求偏导。线性回归模型主体部分的实现如代码清单2-1所示。<br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/54/5c/aacP75Co_o.png" alt="b31da648bb04af890d383684e2cbebe2.png"></p> 
 <p>在代码清单2-1中，我们尝试将线性回归模型的主体部分定义为linear_loss函数。该函数的输入参数包括训练数据和权重系数，输出为线性回归模型预测值、均方损失、权重参数一阶偏导和偏置一阶偏导。</p> 
 <p>在给定模型初始参数的情况下，线性回归模型根据训练数据和参数计算出<strong>当前均方损失和参数一阶梯度</strong>。</p> 
 <p>然后在linear_loss函数的基础上，定义线性回归模型的<strong>训练过程</strong>。主要包括参数初始化、迭代训练和梯度下降寻优。</p> 
 <p>我们可以先定义一个参数初始化函数initialize_params，再基于linear_loss函数和initialize_params函数来定义包含迭代训练和梯度下降寻优的线性回归拟合过程。</p> 
 <p>参数初始化函数initialize_params如代码清单2-2所示。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/55/bc/pghNodPo_o.png" alt="548f410b31831f640e8918df02f7e72b.png"></p> 
 <p>在代码清单2-2中，我们输入训练数据的变量维度，即对于线性回归而言，每一个变量都有一个权重系数。输出为初始化为零向量的权重系数和初始化为零的偏置参数。<br></p> 
 <p>最后，我们尝试结合linear_loss和initialize_params函数定义线性回归模型训练过程的函数linear_train，如代码清单2-3所示。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/3f/33/bChsNUZK_o.png" alt="22278c45faabc5a5a07d6dbf99d5692b.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/0d/29/idWRyqDc_o.png" alt="37fdc0bfbebca82b45a30ceebd2ce02d.png"></p> 
 <p>在代码清单2-3中，我们首先初始化模型参数，然后对遍历设置训练迭代过程。在每一次迭代过程中，基于linear_loss函数计算当前迭代的预测值、均方损失和梯度，并根据梯度下降法不断更新系数。</p> 
 <p>在训练过程中记录每一步的损失、每10000次迭代打印当前损失信息、保存更新后的模型参数字典和梯度字典。这样，一个完整的线性回归模型就基本完成了。<br></p> 
 <p>基于上述代码实现，我们使用sklearn的diabetes数据集进行测试，其具体信息如表2-1所示。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/dd/29/EjMuHKdt_o.png" alt="5e3e46a5208f1986c5c1472387d27fd1.png"></p> 
 <p>从sklearn中导入该数据集并将其划分为训练集和测试集，如代码清单2-4所示。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/4f/ec/OGgOBtdF_o.png" alt="52734c197dce74ee08cfaf5f6ba0e9af.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/8b/8a/Kg1EYcLX_o.png" alt="85446299c8be4d0c842bb5ecb8faff5d.png"></p> 
 <p>代码清单2-4首先导入sklearn的diabetes公开数据集，获取数据输入和标签并打乱顺序后划分数据集，输出为：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/76/97/tqE6frmN_o.png" alt="e631566be6afea8640d7a72911673ef1.png"></p> 
 <p>然后我们使用代码清单2-3定义的linear_train函数训练划分后的数据集，如代码清单2-5所示。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/a4/c2/nsYjFZhl_o.png" alt="18b10a45362aa9287b7e78dfff443174.png"></p> 
 <p>在学习率为0.01、迭代次数为200000的条件下，我们得到上述训练参数。训练中的均方损失下降过程如图2-4所示。</p> 
 <p><img src="https://images2.imgbox.com/6d/b7/LeLqhc4O_o.png" alt="aaa8265192f50ecd97fdeee78eb397ea.png"></p> 
 <p style="text-align:center;">图2-4  训练中的均方损失下降过程<br></p> 
 <p>基于前述训练参数，我们可以定义一个预测函数对测试集进行预测，如代码清单2-6所示。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/4e/7f/CpqxBLgP_o.png" alt="3be825c7ab70fda46cce482fd0e577cc.png"></p> 
 <p>代码清单2-6定义了回归模型的预测函数，输入参数为测试集和模型训练参数，然后通过回归表达式即可进行回归预测。</p> 
 <p>如何衡量预测结果的好坏呢？</p> 
 <p>除均方损失外，回归模型的一个重要评估指标是R2系数，用来判断模型拟合水平。我们尝试自定义一个R2系数计算方法，并基于该系数计算代码清单2-6预测结果的拟合水平，具体如代码清单2-7所示。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/d7/93/j8pNKAik_o.png" alt="36be65ba4c18a8b3ab3270209190b8ab.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/96/83/cLbfG7d2_o.png" alt="5cd91fa5f0876d7d5f8edf9085267693.png"></p> 
 <p>代码清单2-7给出了回归模型R2系数的计算方式。根据总离差平方和、残差平方和以及R2计算公式，我们计算测试集的R2系数。代码清单2-7的输出如下：</p> 
 <p style="text-align:center;">0.5334188457463576</p> 
 <p>可以看到，我们自定义并训练的线性回归模型在该测试集上的R2系数为0.53，结果并不算太好，除了模型的一些超参数需要做一些调整和优化外，<strong>可能线性回归模型本身对该数据集拟合效果有限</strong>。<br></p> 
 <p><strong>三、基于sklearn的模型实现</strong></p> 
 <p>作为参考对比，这里同样基于sklearn的LinearRegression类给出对于该数据集的拟合效果。LinearRegression函数位于sklearn的linear_model模块下，定义该类的一个线性回归实例后，直接调用其fit方法拟合训练集即可。</p> 
 <p>参考实现如代码清单2-8所示。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/5f/98/vRuluREE_o.png" alt="4aa7374bb90b1cf00d642e68671e61b8.png"></p> 
 <p>输出如下：<br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/6b/77/54sjjmTo_o.png" alt="66225e59b1aa47c9cc8765216363a377.png"></p> 
 <p>可以看到，在不做任何特征处理的情况下，基于sklearn的线性回归模型在同样的数据集上与我们基于NumPy手写的模型表现差异并不大，这也验证了手写算法的有效性。</p> 
 <p><strong>本文摘编自《机器学习：公式推导与代码实现》</strong></p> 
 <p>题图来源 | iStock<strong><br></strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/14/1f/x5hGBjJN_o.png" alt="30b594a4c3acf2aab71308051b9ec50d.png"></p> 
 <p>图书简介：作为一门应用型学科，机器学习植根于数学理论，落地于代码实现。这就意味着，掌握公式推导和代码编写，方能更加深入地理解机器学习算法的内在逻辑和运行机制。<strong>本书在对全部机器学习算法进行分类梳理的基础之上，分别对监督学习单模型、监督学习集成模型、无监督学习模型、概率模型四个大类共26个经典算法进行了细致的公式推导和代码实现</strong>，旨在帮助机器学习学习者和研究者完整地掌握算法细节、实现方法以及内在逻辑。</p> 
</div>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b5fe5f583ec4b7ce40b95206ff6eb697/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">focal loss 多分类问题MNIST手写字学习</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/bc9032696c0f242ae2f3412bb7170668/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Java中时间格式 yyyyMMdd和yyyy-MM-dd相互转换</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>