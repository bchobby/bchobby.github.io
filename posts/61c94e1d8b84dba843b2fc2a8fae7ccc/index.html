<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>大数据技术之SparkCore - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="大数据技术之SparkCore" />
<meta property="og:description" content="文章开篇先简单介绍一下SparkCore： Spark Core是spark的核心与基础，实现了Spark的基本功能，包含任务调度，内存管理，错误恢复与存储系统交互等模块
Spark Core中包含了对Spark核心API——RDD API(弹性分布式数据集)的定义：RDD表示分布在多个计算节点上可以并行操作的元素集合，是spark的核心抽象。
第1章 RDD概述 1.1 什么是RDD
RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象。代码中是一个抽象类，它代表一个不可变、可分区、里面的元素可并行计算的集合。
1.2 RDD的属性
1)一组分区（Partition），即数据集的基本组成单位;
2)一个计算每个分区的函数;
3)RDD之间的依赖关系;
4)一个Partitioner，即RDD的分片函数;
5)一个列表，存储存取每个Partition的优先位置（preferred location）。
1.3 RDD特点
RDD表示只读的分区的数据集，对RDD进行改动，只能通过RDD的转换操作，由一个RDD得到一个新的RDD，新的RDD包含了从其他RDD衍生所必需的信息。RDDs之间存在依赖，RDD的执行是按照血缘关系延时计算的。如果血缘关系较长，可以通过持久化RDD来切断血缘关系。
1.3.1 分区
RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute函数是执行转换逻辑将其他RDD的数据进行转换。
1.3.2 只读
如下图所示，RDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。
由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了，如下图所示。
RDD的操作算子包括两类，一类叫做transformations，它是用来将RDD进行转化，构建RDD的血缘关系；另一类叫做actions，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。下图是RDD所支持的操作算子列表。
1.3.3 依赖
RDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。如下图所示，依赖包括两种，一种是窄依赖，RDDs之间分区是一一对应的，另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。
1.3.4 缓存
如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。如下图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。
在这里插入图片描述
1.3.5 CheckPoint
虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间迭代型应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道它的父RDDs了，它可以从checkpoint处拿到数据。
第2章 RDD编程
2.1 编程模型
在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的transformations定义RDD之后，就可以调用actions触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。
要使用Spark，开发者需要编写一个Driver程序，它被提交到集群以调度运行Worker，如下图所示。Driver中定义了一个或多个RDD，并调用RDD上的action，Worker则执行RDD分区计算任务。
2.2 RDD的创建
在Spark中创建RDD的创建方式可以分为三种：从集合中创建RDD；从外部存储创建RDD；从其他RDD创建。
2.2.1 从集合中创建
从集合中创建RDD，Spark主要提供了两种函数：parallelize和makeRDD
1）使用parallelize()从集合创建
scala&gt; val rdd = sc.parallelize(Array(1,2,3,4,5,6,7,8))
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at :24" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/61c94e1d8b84dba843b2fc2a8fae7ccc/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-12-25T13:50:46+08:00" />
<meta property="article:modified_time" content="2022-12-25T13:50:46+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">大数据技术之SparkCore</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h5><a id="SparkCore_0"></a>文章开篇先简单介绍一下<mark>SparkCore</mark>：</h5> 
<p>Spark Core是spark的核心与基础，实现了Spark的基本功能，包含任务调度，内存管理，错误恢复与存储系统交互等模块<br> Spark Core中包含了对Spark核心API——RDD API(弹性分布式数据集)的定义：RDD表示分布在多个计算节点上可以并行操作的元素集合，是spark的核心抽象。</p> 
<h4><a id="1_RDD_3"></a>第1章 RDD概述</h4> 
<p><strong>1.1 什么是RDD</strong><br> RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象。代码中是一个抽象类，它代表一个不可变、可分区、里面的元素可并行计算的集合。<br> <strong>1.2 RDD的属性</strong><br> <img src="https://images2.imgbox.com/0d/f6/g9KQ0RH2_o.png" alt="在这里插入图片描述"><br> 1)一组分区（Partition），即数据集的基本组成单位;<br> 2)一个计算每个分区的函数;<br> 3)RDD之间的依赖关系;<br> 4)一个Partitioner，即RDD的分片函数;<br> 5)一个列表，存储存取每个Partition的优先位置（preferred location）。<br> <strong>1.3 RDD特点</strong><br> RDD表示只读的分区的数据集，对RDD进行改动，只能通过RDD的转换操作，由一个RDD得到一个新的RDD，新的RDD包含了从其他RDD衍生所必需的信息。RDDs之间存在依赖，RDD的执行是按照血缘关系延时计算的。如果血缘关系较长，可以通过持久化RDD来切断血缘关系。<br> <strong>1.3.1 分区</strong><br> RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute函数是执行转换逻辑将其他RDD的数据进行转换。<img src="https://images2.imgbox.com/52/28/U7jTuHPv_o.png" alt="在这里插入图片描述"><br> <strong>1.3.2 只读</strong><br> 如下图所示，RDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。<img src="https://images2.imgbox.com/bc/e0/kPqY74o7_o.png" alt="在这里插入图片描述"><br> 由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了，如下图所示。<img src="https://images2.imgbox.com/37/04/y5mylmc5_o.png" alt="在这里插入图片描述"><br> RDD的操作算子包括两类，一类叫做transformations，它是用来将RDD进行转化，构建RDD的血缘关系；另一类叫做actions，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。下图是RDD所支持的操作算子列表。<br> <strong>1.3.3 依赖</strong><br> RDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。如下图所示，依赖包括两种，一种是窄依赖，RDDs之间分区是一一对应的，另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。<img src="https://images2.imgbox.com/85/45/jk6LaVxX_o.png" alt="在这里插入图片描述"><br> <strong>1.3.4 缓存</strong><br> 如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。如下图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。<br> 在这里插入图片描述<br> <strong>1.3.5 CheckPoint</strong><br> 虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间迭代型应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道它的父RDDs了，它可以从checkpoint处拿到数据。<br> 第2章 RDD编程<br> <strong>2.1 编程模型</strong><br> 在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的transformations定义RDD之后，就可以调用actions触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。<br> 要使用Spark，开发者需要编写一个Driver程序，它被提交到集群以调度运行Worker，如下图所示。Driver中定义了一个或多个RDD，并调用RDD上的action，Worker则执行RDD分区计算任务。<br> <img src="https://images2.imgbox.com/70/90/uOpvEzdp_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/28/f9/456b0Uxl_o.png" alt="在这里插入图片描述"><br> <strong>2.2 RDD的创建</strong><br> 在Spark中创建RDD的创建方式可以分为三种：从集合中创建RDD；从外部存储创建RDD；从其他RDD创建。<br> 2.2.1 从集合中创建<br> 从集合中创建RDD，Spark主要提供了两种函数：parallelize和makeRDD<br> 1）使用parallelize()从集合创建<br> scala&gt; val rdd = sc.parallelize(Array(1,2,3,4,5,6,7,8))<br> rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at :24<br> 2）使用makeRDD()从集合创建<br> scala&gt; val rdd1 = sc.makeRDD(Array(1,2,3,4,5,6,7,8))<br> rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at :24</p> 
<p><strong>2.2.2 由外部存储系统的数据集创建</strong><br> 包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等，我们会在第4章详细介绍。<br> scala&gt; val rdd2= sc.textFile(“hdfs://hadoop102:9000/RELEASE”)<br> rdd2: org.apache.spark.rdd.RDD[String] = hdfs:// hadoop102:9000/RELEASE MapPartitionsRDD[4] at textFile at :24<br> <strong>2.2.3 从其他RDD创建</strong><br> 详见2.3节<br> <strong>2.3 RDD的转换（面试开发重点）</strong><br> RDD整体上分为Value类型和Key-Value类型<br> <strong>2.3.1 Value类型</strong><br> <strong>2.3.1.1 map(func)案例</strong></p> 
<ol><li>作用：返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成</li><li>需求：创建一个1-10数组的RDD，将所有元素<em>2形成新的RDD<br> （1）创建<br> scala&gt; var source = sc.parallelize(1 to 10)<br> source: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[8] at parallelize at :24<br> （2）打印<br> scala&gt; source.collect()<br> res7: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)<br> （3）将所有元素</em>2<br> scala&gt; val mapadd = source.map(_ * 2)<br> mapadd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[9] at map at :26<br> （4）打印最终结果<br> scala&gt; mapadd.collect()<br> res8: Array[Int] = Array(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)<br> <strong>2.3.1.2 mapPartitions(func) 案例</strong></li><li>作用：类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U]。假设有N个元素，有M个分区，那么map的函数的将被调用N次,而mapPartitions被调用M次,一个函数一次处理所有分区。</li><li>需求：创建一个RDD，使每个元素*2组成新的RDD<br> （1）创建一个RDD</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>Array<span class="token punctuation">(</span><span class="token number">1,2</span>,3,4<span class="token punctuation">))</span>
rdd: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）使每个元素*2组成新的RDD</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd.mapPartitions<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token operator">&gt;</span>x.map<span class="token punctuation">(</span>_*2<span class="token punctuation">))</span>
res3: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> MapPartitionsRDD<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">]</span> at mapPartitions at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:27
</code></pre> 
<p>（3）打印新的RDD</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> res3.collect
res4: Array<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span><span class="token number">2</span>, <span class="token number">4</span>, <span class="token number">6</span>, <span class="token number">8</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>2.3.1.3 mapPartitionsWithIndex(func) 案例</strong></p> 
<ol><li>作用：类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U]；</li><li>需求：创建一个RDD，使每个元素跟所在分区形成一个元组组成一个新的RDD<br> （1）创建一个RDD</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>Array<span class="token punctuation">(</span><span class="token number">1,2</span>,3,4<span class="token punctuation">))</span>
rdd: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）使每个元素跟所在分区形成一个元组组成一个新的RDD</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val indexRdd <span class="token operator">=</span> rdd.mapPartitionsWithIndex<span class="token variable"><span class="token punctuation">((</span>index<span class="token punctuation">,</span>items<span class="token punctuation">)</span><span class="token operator">=</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>items.map<span class="token punctuation">((</span>index<span class="token punctuation">,</span>_<span class="token punctuation">))</span></span><span class="token punctuation">))</span>
indexRdd: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> MapPartitionsRDD<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span> at mapPartitionsWithIndex at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:26
</code></pre> 
<p>（3）打印新的RDD</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> indexRdd.collect
res2: Array<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token variable"><span class="token punctuation">((</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">))</span></span>
</code></pre> 
<p><strong>2.3.1.4 flatMap(func) 案例</strong></p> 
<ol><li>作用：类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）</li><li>需求：创建一个元素为1-5的RDD，运用flatMap创建一个新的RDD，新的RDD为原RDD的每个元素的2倍（2，4，6，8，10）<br> （1）创建</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val sourceFlat <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span><span class="token number">1</span> to <span class="token number">5</span><span class="token punctuation">)</span>
sourceFlat: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">12</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）打印</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> sourceFlat.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res11: Array<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">2</span>, <span class="token number">3</span>, <span class="token number">4</span>, <span class="token number">5</span><span class="token punctuation">)</span>
</code></pre> 
<p>（3）根据原RDD创建新RDD（1-&gt;1,2-&gt;1,2……5-&gt;1,2,3,4,5）</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val flatMap <span class="token operator">=</span> sourceFlat.flatMap<span class="token punctuation">(</span><span class="token number">1</span> to _<span class="token punctuation">)</span>
flatMap: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> MapPartitionsRDD<span class="token punctuation">[</span><span class="token number">13</span><span class="token punctuation">]</span> at flatMap at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:26
</code></pre> 
<p>（4）打印新RDD</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> flatMap.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res12: Array<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">1</span>, <span class="token number">2</span>, <span class="token number">1</span>, <span class="token number">2</span>, <span class="token number">3</span>, <span class="token number">1</span>, <span class="token number">2</span>, <span class="token number">3</span>, <span class="token number">4</span>, <span class="token number">1</span>, <span class="token number">2</span>, <span class="token number">3</span>, <span class="token number">4</span>, <span class="token number">5</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>2.3.1.5 map()和mapPartition()的区别</strong></p> 
<ol><li>map()：每次处理一条数据。</li><li>mapPartition()：每次处理一个分区的数据，这个分区的数据处理完后，原RDD中分区的数据才能释放，可能导致OOM。</li><li>开发指导：当内存空间较大的时候建议使用mapPartition()，以提高处理效率。<br> 2.3.1.6 glom案例</li><li>作用：将每一个分区形成一个数组，形成新的RDD类型时RDD[Array[T]]</li><li>需求：创建一个4个分区的RDD，并将每个分区的数据放到一个数组<br> （1）创建</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span><span class="token number">1</span> to <span class="token number">16,4</span><span class="token punctuation">)</span>
rdd: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">65</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）将每个分区的数据放到一个数组并收集到Driver端打印</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd.glom<span class="token punctuation">(</span><span class="token punctuation">)</span>.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res25: Array<span class="token punctuation">[</span>Array<span class="token punctuation">[</span>Int<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span>Array<span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">2</span>, <span class="token number">3</span>, <span class="token number">4</span><span class="token punctuation">)</span>, Array<span class="token punctuation">(</span><span class="token number">5</span>, <span class="token number">6</span>, <span class="token number">7</span>, <span class="token number">8</span><span class="token punctuation">)</span>, Array<span class="token punctuation">(</span><span class="token number">9</span>, <span class="token number">10</span>, <span class="token number">11</span>, <span class="token number">12</span><span class="token punctuation">)</span>, Array<span class="token punctuation">(</span><span class="token number">13</span>, <span class="token number">14</span>, <span class="token number">15</span>, <span class="token number">16</span><span class="token punctuation">))</span>
</code></pre> 
<p>2.3.1.7 groupBy(func)案例</p> 
<ol><li>作用：分组，按照传入函数的返回值进行分组。将相同的key对应的值放入一个迭代器。</li><li>需求：创建一个RDD，按照元素模以2的值进行分组。<br> （1）创建</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span><span class="token number">1</span> to <span class="token number">4</span><span class="token punctuation">)</span>
rdd: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">65</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）按照元素模以2的值进行分组</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val group <span class="token operator">=</span> rdd.groupBy<span class="token punctuation">(</span>_%2<span class="token punctuation">)</span>
group: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, Iterable<span class="token punctuation">[</span>Int<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> ShuffledRDD<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> at groupBy at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:26
</code></pre> 
<p>（3）打印结果</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> group.collect
res0: Array<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, Iterable<span class="token punctuation">[</span>Int<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token variable"><span class="token punctuation">((</span><span class="token number">0</span><span class="token punctuation">,</span>CompactBuffer<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">))</span></span>, <span class="token punctuation">(</span><span class="token number">1</span>,CompactBuffer<span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">3</span><span class="token punctuation">))</span><span class="token punctuation">)</span>
</code></pre> 
<p>2.3.1.8 filter(func) 案例</p> 
<ol><li>作用：过滤。返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成。</li><li>需求：创建一个RDD（由字符串组成），过滤出一个新RDD（包含”xiao”子串）<br> （1）创建</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> var sourceFilter <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>Array<span class="token punctuation">(</span><span class="token string">"xiaoming"</span>,<span class="token string">"xiaojiang"</span>,<span class="token string">"xiaohe"</span>,<span class="token string">"dazhi"</span><span class="token punctuation">))</span>
sourceFilter: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）打印</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> sourceFilter.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res9: Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span>xiaoming, xiaojiang, xiaohe, dazhi<span class="token punctuation">)</span>
</code></pre> 
<p>（3）过滤出含” xiao”子串的形成一个新的RDD</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val filter <span class="token operator">=</span> sourceFilter.filter<span class="token punctuation">(</span>_.contains<span class="token punctuation">(</span><span class="token string">"xiao"</span><span class="token punctuation">))</span>
filter: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> MapPartitionsRDD<span class="token punctuation">[</span><span class="token number">11</span><span class="token punctuation">]</span> at filter at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:26
</code></pre> 
<p>（4）打印新RDD</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> filter.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res10: Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span>xiaoming, xiaojiang, xiaohe<span class="token punctuation">)</span>
</code></pre> 
<p><strong>2.3.1.9 sample(withReplacement, fraction, seed) 案例</strong></p> 
<ol><li>作用：以指定的随机种子随机抽样出数量为fraction的数据，withReplacement表示是抽出的数据是否放回，true为有放回的抽样，false为无放回的抽样，seed用于指定随机数生成器种子。</li><li>需求：创建一个RDD（1-10），从中选择放回和不放回抽样<br> （1）创建RDD</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span><span class="token number">1</span> to <span class="token number">10</span><span class="token punctuation">)</span>
rdd: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">20</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）打印</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res15: Array<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">2</span>, <span class="token number">3</span>, <span class="token number">4</span>, <span class="token number">5</span>, <span class="token number">6</span>, <span class="token number">7</span>, <span class="token number">8</span>, <span class="token number">9</span>, <span class="token number">10</span><span class="token punctuation">)</span>
</code></pre> 
<p>（3）放回抽样</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> var sample1 <span class="token operator">=</span> rdd.sample<span class="token punctuation">(</span>true,0.4,2<span class="token punctuation">)</span>
sample1: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> PartitionwiseSampledRDD<span class="token punctuation">[</span><span class="token number">21</span><span class="token punctuation">]</span> at sample at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:26
</code></pre> 
<p>（4）打印放回抽样结果</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> sample1.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res16: Array<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">2</span>, <span class="token number">2</span>, <span class="token number">7</span>, <span class="token number">7</span>, <span class="token number">8</span>, <span class="token number">9</span><span class="token punctuation">)</span>
</code></pre> 
<p>（5）不放回抽样</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> var sample2 <span class="token operator">=</span> rdd.sample<span class="token punctuation">(</span>false,0.2,3<span class="token punctuation">)</span>
sample2: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> PartitionwiseSampledRDD<span class="token punctuation">[</span><span class="token number">22</span><span class="token punctuation">]</span> at sample at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:26
</code></pre> 
<p>（6）打印不放回抽样结果</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> sample2.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res17: Array<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">9</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>2.3.1.10 distinct([numTasks])) 案例</strong></p> 
<ol><li>作用：对源RDD进行去重后返回一个新的RDD。默认情况下，只有8个并行任务来操作，但是可以传入一个可选的numTasks参数改变它。</li><li>需求：创建一个RDD，使用distinct()对其去重。<br> （1）创建一个RDD</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val distinctRdd <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>List<span class="token punctuation">(</span><span class="token number">1,2</span>,1,5,2,9,6,1<span class="token punctuation">))</span>
distinctRdd: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">34</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）对RDD进行去重（不指定并行度）</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val unionRDD <span class="token operator">=</span> distinctRdd.distinct<span class="token punctuation">(</span><span class="token punctuation">)</span>
unionRDD: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> MapPartitionsRDD<span class="token punctuation">[</span><span class="token number">37</span><span class="token punctuation">]</span> at distinct at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:26
</code></pre> 
<p>（3）打印去重后生成的新RDD</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> unionRDD.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res20: Array<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">9</span>, <span class="token number">5</span>, <span class="token number">6</span>, <span class="token number">2</span><span class="token punctuation">)</span>
</code></pre> 
<p>（4）对RDD（指定并行度为2）</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val unionRDD <span class="token operator">=</span> distinctRdd.distinct<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
unionRDD: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> MapPartitionsRDD<span class="token punctuation">[</span><span class="token number">40</span><span class="token punctuation">]</span> at distinct at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:26
</code></pre> 
<p>（5）打印去重后生成的新RDD</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> unionRDD.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res21: Array<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span><span class="token number">6</span>, <span class="token number">2</span>, <span class="token number">1</span>, <span class="token number">9</span>, <span class="token number">5</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>2.3.1.11 coalesce(numPartitions) 案例</strong></p> 
<ol><li>作用：缩减分区数，用于大数据集过滤后，提高小数据集的执行效率。</li><li>需求：创建一个4个分区的RDD，对其缩减分区<br> （1）创建一个RDD</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span><span class="token number">1</span> to <span class="token number">16,4</span><span class="token punctuation">)</span>
rdd: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">54</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）查看RDD的分区数</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd.partitions.size
res20: Int <span class="token operator">=</span> <span class="token number">4</span>
</code></pre> 
<p>（3）对RDD重新分区</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val coalesceRDD <span class="token operator">=</span> rdd.coalesce<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>
coalesceRDD: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> CoalescedRDD<span class="token punctuation">[</span><span class="token number">55</span><span class="token punctuation">]</span> at coalesce at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:2
</code></pre> 
<p>6<br> （4）查看新RDD的分区数</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> coalesceRDD.partitions.size
res21: Int <span class="token operator">=</span> <span class="token number">3</span>
</code></pre> 
<p><strong>2.3.1.12 repartition(numPartitions) 案例</strong></p> 
<ol><li>作用：根据分区数，重新通过网络随机洗牌所有数据。</li><li>需求：创建一个4个分区的RDD，对其重新分区<br> （1）创建一个RDD</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span><span class="token number">1</span> to <span class="token number">16,4</span><span class="token punctuation">)</span>
rdd: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">56</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）查看RDD的分区数</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd.partitions.size
res22: Int <span class="token operator">=</span> <span class="token number">4</span>
</code></pre> 
<p>（3）对RDD重新分区</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rerdd <span class="token operator">=</span> rdd.repartition<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
rerdd: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> MapPartitionsRDD<span class="token punctuation">[</span><span class="token number">60</span><span class="token punctuation">]</span> at repartition at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:26
</code></pre> 
<p>（4）查看新RDD的分区数</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rerdd.partitions.size
res23: Int <span class="token operator">=</span> <span class="token number">2</span>
</code></pre> 
<p><strong>2.3.1.13 coalesce和repartition的区别</strong></p> 
<ol><li>coalesce重新分区，可以选择是否进行shuffle过程。由参数shuffle: Boolean = false/true决定。</li><li>repartition实际上是调用的coalesce，默认是进行shuffle的。源码如下：<br> def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope {<!-- --><br> coalesce(numPartitions, shuffle = true)<br> }<br> <strong>2.3.1.14 sortBy(func,[ascending], [numTasks]) 案例</strong></li><li>作用；使用func先对数据进行处理，按照处理后的数据比较结果排序，默认为正序。</li><li>需求：创建一个RDD，按照不同的规则进行排序<br> （1）创建一个RDD</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>List<span class="token punctuation">(</span><span class="token number">2,1</span>,3,4<span class="token punctuation">))</span>
rdd: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">21</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）按照自身大小排序</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd.sortBy<span class="token punctuation">(</span>x <span class="token operator">=</span><span class="token operator">&gt;</span> x<span class="token punctuation">)</span>.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res11: Array<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">2</span>, <span class="token number">3</span>, <span class="token number">4</span><span class="token punctuation">)</span>
</code></pre> 
<p>（3）按照与3余数的大小排序</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd.sortBy<span class="token punctuation">(</span>x <span class="token operator">=</span><span class="token operator">&gt;</span> x%3<span class="token punctuation">)</span>.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res12: Array<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span><span class="token number">3</span>, <span class="token number">4</span>, <span class="token number">1</span>, <span class="token number">2</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>2.3.1.15 pipe(command, [envVars]) 案例</strong></p> 
<ol><li>作用：管道，针对每个分区，都执行一个shell脚本，返回输出的RDD。<br> 注意：脚本需要放在Worker节点可以访问到的位置</li><li>需求：编写一个脚本，使用管道将脚本作用于RDD上。<br> （1）编写一个脚本<br> Shell脚本</li></ol> 
<pre><code class="prism language-bash"><span class="token shebang important">#!/bin/sh</span>
<span class="token builtin class-name">echo</span> <span class="token string">"AA"</span>
<span class="token keyword">while</span> <span class="token builtin class-name">read</span> LINE<span class="token punctuation">;</span> <span class="token keyword">do</span>
   <span class="token builtin class-name">echo</span> <span class="token string">"&gt;&gt;&gt;"</span><span class="token variable">${LINE}</span>
<span class="token keyword">done</span>
</code></pre> 
<p>（2）创建一个只有一个分区的RDD</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>List<span class="token punctuation">(</span><span class="token string">"hi"</span>,<span class="token string">"Hello"</span>,<span class="token string">"how"</span>,<span class="token string">"are"</span>,<span class="token string">"you"</span><span class="token punctuation">)</span>,1<span class="token punctuation">)</span>
rdd: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">50</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（3）将脚本作用该RDD并打印</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd.pipe<span class="token punctuation">(</span><span class="token string">"/opt/module/spark/pipe.sh"</span><span class="token punctuation">)</span>.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res18: Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span>AA, <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>hi, <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>Hello, <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>how, <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>are, <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>you<span class="token punctuation">)</span>
</code></pre> 
<p>（4）创建一个有两个分区的RDD</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>List<span class="token punctuation">(</span><span class="token string">"hi"</span>,<span class="token string">"Hello"</span>,<span class="token string">"how"</span>,<span class="token string">"are"</span>,<span class="token string">"you"</span><span class="token punctuation">)</span>,2<span class="token punctuation">)</span>
rdd: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">52</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（5）将脚本作用该RDD并打印</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd.pipe<span class="token punctuation">(</span><span class="token string">"/opt/module/spark/pipe.sh"</span><span class="token punctuation">)</span>.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res19: Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span>AA, <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>hi, <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>Hello, AA, <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>how, <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>are, <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>you<span class="token punctuation">)</span>
</code></pre> 
<p><strong>2.3.2 双Value类型交互</strong><br> 2.3.2.1 union(otherDataset) 案例</p> 
<ol><li>作用：对源RDD和参数RDD求并集后返回一个新的RDD</li><li>需求：创建两个RDD，求并集<br> （1）创建第一个RDD</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd1 <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span><span class="token number">1</span> to <span class="token number">5</span><span class="token punctuation">)</span>
rdd1: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">23</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）创建第二个RDD</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd2 <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span><span class="token number">5</span> to <span class="token number">10</span><span class="token punctuation">)</span>
rdd2: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">24</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（3）计算两个RDD的并集</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd3 <span class="token operator">=</span> rdd1.union<span class="token punctuation">(</span>rdd2<span class="token punctuation">)</span>
rdd3: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> UnionRDD<span class="token punctuation">[</span><span class="token number">25</span><span class="token punctuation">]</span> at union at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:28
</code></pre> 
<p>（4）打印并集结果</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd3.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res18: Array<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">2</span>, <span class="token number">3</span>, <span class="token number">4</span>, <span class="token number">5</span>, <span class="token number">5</span>, <span class="token number">6</span>, <span class="token number">7</span>, <span class="token number">8</span>, <span class="token number">9</span>, <span class="token number">10</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>2.3.2.2 subtract (otherDataset) 案例</strong></p> 
<ol><li>作用：计算差的一种函数，去除两个RDD中相同的元素，不同的RDD将保留下来</li><li>需求：创建两个RDD，求第一个RDD与第二个RDD的差集<br> （1）创建第一个RDD</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span><span class="token number">3</span> to <span class="token number">8</span><span class="token punctuation">)</span>
rdd: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">70</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）创建第二个RDD</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd1 <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span><span class="token number">1</span> to <span class="token number">5</span><span class="token punctuation">)</span>
rdd1: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">71</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（3）计算第一个RDD与第二个RDD的差集并打印</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd.subtract<span class="token punctuation">(</span>rdd1<span class="token punctuation">)</span>.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res27: Array<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span><span class="token number">8</span>, <span class="token number">6</span>, <span class="token number">7</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>2.3.2.3 intersection(otherDataset) 案例</strong></p> 
<ol><li>作用：对源RDD和参数RDD求交集后返回一个新的RDD</li><li>需求：创建两个RDD，求两个RDD的交集<br> （1）创建第一个RDD</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd1 <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span><span class="token number">1</span> to <span class="token number">7</span><span class="token punctuation">)</span>
rdd1: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">26</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）创建第二个RDD</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd2 <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span><span class="token number">5</span> to <span class="token number">10</span><span class="token punctuation">)</span>
rdd2: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">27</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（3）计算两个RDD的交集</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd3 <span class="token operator">=</span> rdd1.intersection<span class="token punctuation">(</span>rdd2<span class="token punctuation">)</span>
rdd3: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> MapPartitionsRDD<span class="token punctuation">[</span><span class="token number">33</span><span class="token punctuation">]</span> at intersection at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:28
</code></pre> 
<p>（4）打印计算结果</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd3.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res19: Array<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span><span class="token number">5</span>, <span class="token number">6</span>, <span class="token number">7</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>2.3.2.4 cartesian(otherDataset) 案例</strong></p> 
<ol><li>作用：笛卡尔积（尽量避免使用）</li><li>需求：创建两个RDD，计算两个RDD的笛卡尔积<br> （1）创建第一个RDD</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd1 <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span><span class="token number">1</span> to <span class="token number">3</span><span class="token punctuation">)</span>
rdd1: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">47</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）创建第二个RDD</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd2 <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span><span class="token number">2</span> to <span class="token number">5</span><span class="token punctuation">)</span>
rdd2: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">48</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（3）计算两个RDD的笛卡尔积并打印</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd1.cartesian<span class="token punctuation">(</span>rdd2<span class="token punctuation">)</span>.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res17: Array<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token variable"><span class="token punctuation">((</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">))</span></span>
</code></pre> 
<p><strong>2.3.2.5 zip(otherDataset)案例</strong></p> 
<ol><li>作用：将两个RDD组合成Key/Value形式的RDD,这里默认两个RDD的partition数量以及元素数量都相同，否则会抛出异常。</li><li>需求：创建两个RDD，并将两个RDD组合到一起形成一个(k,v)RDD<br> （1）创建第一个RDD</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd1 <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>Array<span class="token punctuation">(</span><span class="token number">1,2</span>,3<span class="token punctuation">)</span>,3<span class="token punctuation">)</span>
rdd1: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）创建第二个RDD（与1分区数相同）</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd2 <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>Array<span class="token punctuation">(</span><span class="token string">"a"</span>,<span class="token string">"b"</span>,<span class="token string">"c"</span><span class="token punctuation">)</span>,3<span class="token punctuation">)</span>
rdd2: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（3）第一个RDD组合第二个RDD并打印</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd1.zip<span class="token punctuation">(</span>rdd2<span class="token punctuation">)</span>.collect
res1: Array<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, String<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token variable"><span class="token punctuation">((</span><span class="token number">1</span><span class="token punctuation">,</span>a<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span>b<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span>c<span class="token punctuation">))</span></span>
</code></pre> 
<p>（4）第二个RDD组合第一个RDD并打印</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd2.zip<span class="token punctuation">(</span>rdd1<span class="token punctuation">)</span>.collect
res2: Array<span class="token punctuation">[</span><span class="token punctuation">(</span>String, Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token variable"><span class="token punctuation">((</span>a<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>b<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>c<span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">))</span></span>
</code></pre> 
<p>（5）创建第三个RDD（与1,2分区数不同）</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd3 <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>Array<span class="token punctuation">(</span><span class="token string">"a"</span>,<span class="token string">"b"</span>,<span class="token string">"c"</span><span class="token punctuation">)</span>,2<span class="token punctuation">)</span>
rdd3: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（6）第一个RDD组合第三个RDD并打印</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd1.zip<span class="token punctuation">(</span>rdd3<span class="token punctuation">)</span>.collect
java.lang.IllegalArgumentException: Can't <span class="token function">zip</span> RDDs with unequal numbers of partitions: List<span class="token punctuation">(</span><span class="token number">3</span>, <span class="token number">2</span><span class="token punctuation">)</span>
  at org.apache.spark.rdd.ZippedPartitionsBaseRDD.getPartitions<span class="token punctuation">(</span>ZippedPartitionsRDD.scala:57<span class="token punctuation">)</span>
  at org.apache.spark.rdd.RDD<span class="token variable">$$</span>anonfun<span class="token variable">$partitions</span><span class="token variable">$2</span>.apply<span class="token punctuation">(</span>RDD.scala:252<span class="token punctuation">)</span>
  at org.apache.spark.rdd.RDD<span class="token variable">$$</span>anonfun<span class="token variable">$partitions</span><span class="token variable">$2</span>.apply<span class="token punctuation">(</span>RDD.scala:250<span class="token punctuation">)</span>
  at scala.Option.getOrElse<span class="token punctuation">(</span>Option.scala:121<span class="token punctuation">)</span>
  at org.apache.spark.rdd.RDD.partitions<span class="token punctuation">(</span>RDD.scala:250<span class="token punctuation">)</span>
  at org.apache.spark.SparkContext.runJob<span class="token punctuation">(</span>SparkContext.scala:1965<span class="token punctuation">)</span>
  at org.apache.spark.rdd.RDD<span class="token variable">$$</span>anonfun<span class="token variable">$collect</span><span class="token variable">$1</span>.apply<span class="token punctuation">(</span>RDD.scala:936<span class="token punctuation">)</span>
  at org.apache.spark.rdd.RDDOperationScope$.withScope<span class="token punctuation">(</span>RDDOperationScope.scala:151<span class="token punctuation">)</span>
  at org.apache.spark.rdd.RDDOperationScope$.withScope<span class="token punctuation">(</span>RDDOperationScope.scala:112<span class="token punctuation">)</span>
  at org.apache.spark.rdd.RDD.withScope<span class="token punctuation">(</span>RDD.scala:362<span class="token punctuation">)</span>
  at org.apache.spark.rdd.RDD.collect<span class="token punctuation">(</span>RDD.scala:935<span class="token punctuation">)</span>
  <span class="token punctuation">..</span>. <span class="token number">48</span> elided
</code></pre> 
<p><strong>2.3.3 Key-Value类型</strong><br> 2.3.3.1 partitionBy案例</p> 
<ol><li>作用：对pairRDD进行分区操作，如果原有的partionRDD和现有的partionRDD是一致的话就不进行分区， 否则会生成ShuffleRDD，即会产生shuffle过程。</li><li>需求：创建一个4个分区的RDD，对其重新分区<br> （1）创建一个RDD</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>Array<span class="token variable"><span class="token punctuation">((</span><span class="token number">1</span><span class="token punctuation">,</span>"aaa"<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span>"bbb"<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span>"ccc"<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span>"ddd"<span class="token punctuation">))</span></span>,4<span class="token punctuation">)</span>
rdd: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, String<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">44</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）查看RDD的分区数</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd.partitions.size
res24: Int <span class="token operator">=</span> <span class="token number">4</span>
</code></pre> 
<p>（3）对RDD重新分区</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> var rdd2 <span class="token operator">=</span> rdd.partitionBy<span class="token punctuation">(</span>new org.apache.spark.HashPartitioner<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">))</span>
rdd2: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, String<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> ShuffledRDD<span class="token punctuation">[</span><span class="token number">45</span><span class="token punctuation">]</span> at partitionBy at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:26
</code></pre> 
<p>（4）查看新RDD的分区数</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd2.partitions.size
res25: Int <span class="token operator">=</span> <span class="token number">2</span>
</code></pre> 
<p><strong>2.3.3.2 groupByKey案例</strong></p> 
<ol><li>作用：groupByKey也是对每个key进行操作，但只生成一个sequence。</li><li>需求：创建一个pairRDD，将相同key对应值聚合到一个sequence中，并计算相同key对应值的相加结果。<br> （1）创建一个pairRDD</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val words <span class="token operator">=</span> Array<span class="token punctuation">(</span><span class="token string">"one"</span>, <span class="token string">"two"</span>, <span class="token string">"two"</span>, <span class="token string">"three"</span>, <span class="token string">"three"</span>, <span class="token string">"three"</span><span class="token punctuation">)</span>
words: Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span>one, two, two, three, three, three<span class="token punctuation">)</span>

scala<span class="token operator">&gt;</span> val wordPairsRDD <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>words<span class="token punctuation">)</span>.map<span class="token punctuation">(</span>word <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">(</span>word, <span class="token number">1</span><span class="token punctuation">))</span>
wordPairsRDD: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>String, Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> MapPartitionsRDD<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span> at map at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:26
</code></pre> 
<p>（2）将相同key对应值聚合到一个sequence中</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val group <span class="token operator">=</span> wordPairsRDD.groupByKey<span class="token punctuation">(</span><span class="token punctuation">)</span>
group: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>String, Iterable<span class="token punctuation">[</span>Int<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> ShuffledRDD<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span> at groupByKey at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:28
</code></pre> 
<p>（3）打印结果</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> group.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res1: Array<span class="token punctuation">[</span><span class="token punctuation">(</span>String, Iterable<span class="token punctuation">[</span>Int<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token variable"><span class="token punctuation">((</span>two<span class="token punctuation">,</span>CompactBuffer<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">))</span></span>, <span class="token punctuation">(</span>one,CompactBuffer<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">))</span>, <span class="token punctuation">(</span>three,CompactBuffer<span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">1</span>, <span class="token number">1</span><span class="token punctuation">))</span><span class="token punctuation">)</span>
</code></pre> 
<p>（4）计算相同key对应值的相加结果</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> group.map<span class="token punctuation">(</span>t <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">(</span>t._1, t._2.sum<span class="token punctuation">))</span>
res2: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>String, Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> MapPartitionsRDD<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">]</span> at map at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:31
</code></pre> 
<p>（5）打印结果</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> res2.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res3: Array<span class="token punctuation">[</span><span class="token punctuation">(</span>String, Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token variable"><span class="token punctuation">((</span>two<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>one<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>three<span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">))</span></span>
</code></pre> 
<p><strong>2.3.3.3 reduceByKey(func, [numTasks]) 案例</strong></p> 
<ol><li>在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，reduce任务的个数可以通过第二个可选的参数来设置。</li><li>需求：创建一个pairRDD，计算相同key对应值的相加结果<br> （1）创建一个pairRDD</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>List<span class="token variable"><span class="token punctuation">((</span>"female"<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span>"male"<span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span>"female"<span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span>"male"<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">))</span></span><span class="token punctuation">)</span>
rdd: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>String, Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">46</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）计算相同key对应值的相加结果</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val reduce <span class="token operator">=</span> rdd.reduceByKey<span class="token punctuation">((</span>x,y<span class="token punctuation">)</span> <span class="token operator">=</span><span class="token operator">&gt;</span> x+y<span class="token punctuation">)</span>
reduce: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>String, Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> ShuffledRDD<span class="token punctuation">[</span><span class="token number">47</span><span class="token punctuation">]</span> at reduceByKey at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:26
</code></pre> 
<p>（3）打印结果</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> reduce.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res29: Array<span class="token punctuation">[</span><span class="token punctuation">(</span>String, Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token variable"><span class="token punctuation">((</span>female<span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>male<span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">))</span></span>
</code></pre> 
<p><strong>2.3.3.4 reduceByKey和groupByKey的区别</strong></p> 
<ol><li>reduceByKey：按照key进行聚合，在shuffle之前有combine（预聚合）操作，返回结果是RDD[k,v].</li><li>groupByKey：按照key进行分组，直接进行shuffle。</li><li>开发指导：reduceByKey比groupByKey，建议使用。但是需要注意是否会影响业务逻辑。<br> <strong>2.3.3.5 aggregateByKey案例</strong><br> 参数：(zeroValue:U,[partitioner: Partitioner]) (seqOp: (U, V) =&gt; U,combOp: (U, U) =&gt; U)</li><li>作用：在kv对的RDD中，，按key将value进行分组合并，合并时，将每个value和初始值作为seq函数的参数，进行计算，返回的结果作为一个新的kv对，然后再将结果按照key进行合并，最后将每个分组的value传递给combine函数进行计算（先将前两个value进行计算，将返回结果和下一个value传给combine函数，以此类推），将key与计算结果作为一个新的kv对输出。</li><li>参数描述：<br> （1）zeroValue：给每一个分区中的每一个key一个初始值；<br> （2）seqOp：函数用于在每一个分区中用初始值逐步迭代value；<br> （3）combOp：函数用于合并每个分区中的结果。</li><li>需求：创建一个pairRDD，取出每个分区相同key对应值的最大值，然后相加</li><li>需求分析<img src="https://images2.imgbox.com/15/09/wpLzAc9w_o.png" alt="在这里插入图片描述"><br> （1）创建一个pairRDD</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val input <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>Array<span class="token variable"><span class="token punctuation">((</span>"a"<span class="token punctuation">,</span> <span class="token number">88</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>"b"<span class="token punctuation">,</span> <span class="token number">95</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>"a"<span class="token punctuation">,</span> <span class="token number">91</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>"b"<span class="token punctuation">,</span> <span class="token number">93</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>"a"<span class="token punctuation">,</span> <span class="token number">95</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>"b"<span class="token punctuation">,</span> <span class="token number">98</span><span class="token punctuation">))</span></span>,2<span class="token punctuation">)</span>
input: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>String, Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">52</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:26
</code></pre> 
<p>（2）将相同key对应的值相加，同时记录该key出现的次数，放入一个二元组</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val combine <span class="token operator">=</span> input.combineByKey<span class="token variable"><span class="token punctuation">((</span>_<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span>acc<span class="token operator">:</span><span class="token punctuation">(</span>Int<span class="token punctuation">,</span>Int<span class="token punctuation">)</span><span class="token punctuation">,</span>v<span class="token punctuation">)</span><span class="token operator">=</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>acc._1<span class="token operator">+</span>v<span class="token punctuation">,</span>acc._2<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span>acc1<span class="token operator">:</span><span class="token punctuation">(</span>Int<span class="token punctuation">,</span>Int<span class="token punctuation">)</span><span class="token punctuation">,</span>acc2<span class="token operator">:</span><span class="token punctuation">(</span>Int<span class="token punctuation">,</span>Int<span class="token punctuation">))</span></span><span class="token operator">=</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>acc1._1+acc2._1,acc1._2+acc2._2<span class="token punctuation">))</span>
combine: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>String, <span class="token punctuation">(</span>Int, Int<span class="token punctuation">))</span><span class="token punctuation">]</span> <span class="token operator">=</span> ShuffledRDD<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span> at combineByKey at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:28
</code></pre> 
<p>（3）打印合并后的结果</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> combine.collect
res5: Array<span class="token punctuation">[</span><span class="token punctuation">(</span>String, <span class="token punctuation">(</span>Int, Int<span class="token punctuation">))</span><span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token variable"><span class="token punctuation">((</span>b<span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">286</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">))</span></span>, <span class="token punctuation">(</span>a,<span class="token punctuation">(</span><span class="token number">274,3</span><span class="token punctuation">))</span><span class="token punctuation">)</span>
</code></pre> 
<p>（4）计算平均值</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val result <span class="token operator">=</span> combine.map<span class="token punctuation">{<!-- --></span>case <span class="token punctuation">(</span>key,value<span class="token punctuation">)</span> <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">(</span>key,value._1/value._2.toDouble<span class="token punctuation">)</span><span class="token punctuation">}</span>
result: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>String, Double<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> MapPartitionsRDD<span class="token punctuation">[</span><span class="token number">54</span><span class="token punctuation">]</span> at map at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:30
</code></pre> 
<p>（5）打印结果</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> result.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res33: Array<span class="token punctuation">[</span><span class="token punctuation">(</span>String, Double<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token variable"><span class="token punctuation">((</span>b<span class="token punctuation">,</span><span class="token number">95.33333333333333</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token number">91.33333333333333</span><span class="token punctuation">))</span></span>
</code></pre> 
<p><strong>2.3.3.8 sortByKey([ascending], [numTasks]) 案例</strong></p> 
<ol><li>作用：在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD</li><li>需求：创建一个pairRDD，按照key的正序和倒序进行排序<br> （1）创建一个pairRDD</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>Array<span class="token variable"><span class="token punctuation">((</span><span class="token number">3</span><span class="token punctuation">,</span>"aa"<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span>"cc"<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span>"bb"<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>"dd"<span class="token punctuation">))</span></span><span class="token punctuation">)</span>
rdd: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, String<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">14</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）按照key的正序</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd.sortByKey<span class="token punctuation">(</span>true<span class="token punctuation">)</span>.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res9: Array<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, String<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token variable"><span class="token punctuation">((</span><span class="token number">1</span><span class="token punctuation">,</span>dd<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span>bb<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span>aa<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span>cc<span class="token punctuation">))</span></span>
</code></pre> 
<p>（3）按照key的倒序</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd.sortByKey<span class="token punctuation">(</span>false<span class="token punctuation">)</span>.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res10: Array<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, String<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token variable"><span class="token punctuation">((</span><span class="token number">6</span><span class="token punctuation">,</span>cc<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span>aa<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span>bb<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>dd<span class="token punctuation">))</span></span>
</code></pre> 
<p><strong>2.3.3.9 mapValues案例</strong></p> 
<ol><li>针对于(K,V)形式的类型只对V进行操作</li><li>需求：创建一个pairRDD，并将value添加字符串"|||"<br> （1）创建一个pairRDD</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd3 <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>Array<span class="token variable"><span class="token punctuation">((</span><span class="token number">1</span><span class="token punctuation">,</span>"a"<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>"d"<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span>"b"<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span>"c"<span class="token punctuation">))</span></span><span class="token punctuation">)</span>
rdd3: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, String<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">67</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）对value添加字符串"|||"</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd3.mapValues<span class="token punctuation">(</span>_+<span class="token string">"|||"</span><span class="token punctuation">)</span>.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res26: Array<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, String<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token variable"><span class="token punctuation">((</span><span class="token number">1</span><span class="token punctuation">,</span>a<span class="token operator">||</span><span class="token operator">|</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>d<span class="token operator">||</span><span class="token operator">|</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span>b<span class="token operator">||</span><span class="token operator">|</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span>c<span class="token operator">||</span><span class="token operator">|</span><span class="token punctuation">))</span></span>
</code></pre> 
<p>2.3.3.10 join(otherDataset, [numTasks]) 案例</p> 
<ol><li>作用：在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD</li><li>需求：创建两个pairRDD，并将key相同的数据聚合到一个元组。<br> （1）创建第一个pairRDD</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>Array<span class="token variable"><span class="token punctuation">((</span><span class="token number">1</span><span class="token punctuation">,</span>"a"<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span>"b"<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span>"c"<span class="token punctuation">))</span></span><span class="token punctuation">)</span>
rdd: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, String<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">32</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）创建第二个pairRDD</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd1 <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>Array<span class="token variable"><span class="token punctuation">((</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">))</span></span><span class="token punctuation">)</span>
rdd1: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">33</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（3）join操作并打印结果</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd.join<span class="token punctuation">(</span>rdd1<span class="token punctuation">)</span>.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res13: Array<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, <span class="token punctuation">(</span>String, Int<span class="token punctuation">))</span><span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token variable"><span class="token punctuation">((</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">))</span></span>, <span class="token punctuation">(</span><span class="token number">2</span>,<span class="token punctuation">(</span>b,5<span class="token punctuation">))</span>, <span class="token punctuation">(</span><span class="token number">3</span>,<span class="token punctuation">(</span>c,6<span class="token punctuation">))</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>2.3.3.11 cogroup(otherDataset, [numTasks]) 案例</strong></p> 
<ol><li>作用：在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable,Iterable))类型的RDD</li><li>需求：创建两个pairRDD，并将key相同的数据聚合到一个迭代器。<br> （1）创建第一个pairRDD</li></ol> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>Array<span class="token variable"><span class="token punctuation">((</span><span class="token number">1</span><span class="token punctuation">,</span>"a"<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span>"b"<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span>"c"<span class="token punctuation">))</span></span><span class="token punctuation">)</span>
rdd: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, String<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">37</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）创建第二个pairRDD</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd1 <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>Array<span class="token variable"><span class="token punctuation">((</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">))</span></span><span class="token punctuation">)</span>
rdd1: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">38</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（3）cogroup两个RDD并打印结果</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd.cogroup<span class="token punctuation">(</span>rdd1<span class="token punctuation">)</span>.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res14: Array<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, <span class="token punctuation">(</span>Iterable<span class="token punctuation">[</span>String<span class="token punctuation">]</span>, Iterable<span class="token punctuation">[</span>Int<span class="token punctuation">]</span><span class="token punctuation">))</span><span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token variable"><span class="token punctuation">((</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">(</span>CompactBuffer<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">,</span>CompactBuffer<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">))</span></span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token number">2</span>,<span class="token punctuation">(</span>CompactBuffer<span class="token punctuation">(</span>b<span class="token punctuation">)</span>,CompactBuffer<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">))</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token number">3</span>,<span class="token punctuation">(</span>CompactBuffer<span class="token punctuation">(</span>c<span class="token punctuation">)</span>,CompactBuffer<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">))</span><span class="token punctuation">))</span>
</code></pre> 
<p>2.3.4 案例实操</p> 
<ol><li>数据结构：时间戳，省份，城市，用户，广告，中间字段使用空格分割。</li></ol> 
<p>样本如下：<br> 1516609143867 6 7 64 16<br> 1516609143869 9 4 75 18<br> 1516609143869 1 7 87 12<br> 2. 需求：统计出每一个省份广告被点击次数的TOP3<br> 3. 实现过程：</p> 
<pre><code class="prism language-bash">package com.package.practice

<span class="token function">import</span> org.apache.spark.rdd.RDD
<span class="token function">import</span> org.apache.spark.<span class="token punctuation">{<!-- --></span>SparkConf, SparkContext<span class="token punctuation">}</span>

//需求：统计出每一个省份广告被点击次数的TOP3
object Practice <span class="token punctuation">{<!-- --></span>

  def main<span class="token punctuation">(</span>args: Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span>: Unit <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>

   //1.初始化spark配置信息并建立与spark的连接
    val sparkConf <span class="token operator">=</span> new SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span>.setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>.setAppName<span class="token punctuation">(</span><span class="token string">"Practice"</span><span class="token punctuation">)</span>
    val sc <span class="token operator">=</span> new SparkContext<span class="token punctuation">(</span>sparkConf<span class="token punctuation">)</span>

   //2.读取数据生成RDD：TS，Province，City，User，AD
    val line <span class="token operator">=</span> sc.textFile<span class="token punctuation">(</span><span class="token string">"E:<span class="token entity" title="\\">\\</span>IDEAWorkSpace<span class="token entity" title="\\">\\</span>SparkTest<span class="token entity" title="\\">\\</span>src<span class="token entity" title="\\">\\</span>main<span class="token entity" title="\\">\\</span>resources<span class="token entity" title="\\">\\</span>agent.log"</span><span class="token punctuation">)</span>

   //3.按照最小粒度聚合：<span class="token variable"><span class="token punctuation">((</span>Province<span class="token punctuation">,</span>AD<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
    val provinceAdToOne <span class="token operator">=</span> line.map { x <span class="token operator">=</span><span class="token operator">&gt;</span>
      val fields<span class="token operator">:</span> Array[String] <span class="token operator">=</span> x.split<span class="token punctuation">(</span>" "<span class="token punctuation">)</span>
      <span class="token punctuation">((</span>fields<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> fields<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">))</span></span>, <span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token punctuation">}</span>

   //4.计算每个省中每个广告被点击的总数：<span class="token variable"><span class="token punctuation">((</span>Province<span class="token punctuation">,</span>AD<span class="token punctuation">)</span><span class="token punctuation">,</span>sum<span class="token punctuation">)</span>
    val provinceAdToSum <span class="token operator">=</span> provinceAdToOne.reduceByKey<span class="token punctuation">(</span>_ <span class="token operator">+</span> _<span class="token punctuation">)</span>

   <span class="token operator">/</span><span class="token operator">/</span><span class="token number">5.</span>将省份作为key，广告加点击数为value：<span class="token punctuation">(</span>Province<span class="token punctuation">,</span><span class="token punctuation">(</span>AD<span class="token punctuation">,</span>sum<span class="token punctuation">))</span></span>
    val provinceToAdSum <span class="token operator">=</span> provinceAdToSum.map<span class="token punctuation">(</span>x <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">(</span>x._1._1, <span class="token punctuation">(</span>x._1._2, x._2<span class="token punctuation">))</span><span class="token punctuation">)</span>

   //6.将同一个省份的所有广告进行聚合<span class="token punctuation">(</span>Province,List<span class="token variable"><span class="token punctuation">((</span>AD1<span class="token punctuation">,</span>sum1<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span>AD2<span class="token punctuation">,</span>sum2<span class="token punctuation">)</span>...<span class="token punctuation">))</span></span>
    val provinceGroup <span class="token operator">=</span> provinceToAdSum.groupByKey<span class="token punctuation">(</span><span class="token punctuation">)</span>

   //7.对同一个省份所有广告的集合进行排序并取前3条，排序规则为广告点击总数
    val provinceAdTop3 <span class="token operator">=</span> provinceGroup.mapValues <span class="token punctuation">{<!-- --></span> x <span class="token operator">=</span><span class="token operator">&gt;</span>
      x.toList.sortWith<span class="token punctuation">((</span>x, y<span class="token punctuation">)</span> <span class="token operator">=</span><span class="token operator">&gt;</span> x._2 <span class="token operator">&gt;</span> y._2<span class="token punctuation">)</span>.take<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>
    <span class="token punctuation">}</span>

   //8.将数据拉取到Driver端并打印
    provinceAdTop3.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>.foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>

   //9.关闭与spark的连接
    sc.stop<span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token punctuation">}</span>
  
<span class="token punctuation">}</span>
</code></pre> 
<p>2.4 Action<br> 2.4.1 reduce(func)案例</p> 
<ol><li>作用：通过func函数聚集RDD中的所有元素，先聚合分区内数据，再聚合分区间数据。</li><li>需求：创建一个RDD，将所有元素聚合得到结果。<br> （1）创建一个RDD[Int]<br> scala&gt; val rdd1 = sc.makeRDD(1 to 10,2)<br> rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[85] at makeRDD at :24<br> （2）聚合RDD[Int]所有元素<br> scala&gt; rdd1.reduce(<em>+</em>)<br> res50: Int = 55<br> （3）创建一个RDD[String]<br> scala&gt; val rdd2 = sc.makeRDD(Array((“a”,1),(“a”,3),(“c”,3),(“d”,5)))<br> rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[86] at makeRDD at :24<br> （4）聚合RDD[String]所有数据<br> scala&gt; rdd2.reduce((x,y)=&gt;(x._1 + y._1,x._2 + y._2))<br> res51: (String, Int) = (adca,12)<br> 2.4.2 collect()案例</li><li>作用：在驱动程序中，以数组的形式返回数据集的所有元素。</li><li>需求：创建一个RDD，并将RDD内容收集到Driver端打印<br> （1）创建一个RDD<br> scala&gt; val rdd = sc.parallelize(1 to 10)<br> rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at :24<br> （2）将结果收集到Driver端<br> scala&gt; rdd.collect<br> res0: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)<br> 2.4.3 count()案例</li><li>作用：返回RDD中元素的个数</li><li>需求：创建一个RDD，统计该RDD的条数<br> （1）创建一个RDD<br> scala&gt; val rdd = sc.parallelize(1 to 10)<br> rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at :24<br> （2）统计该RDD的条数<br> scala&gt; rdd.count<br> res1: Long = 10<br> 2.4.4 first()案例</li><li>作用：返回RDD中的第一个元素</li><li>需求：创建一个RDD，返回该RDD中的第一个元素<br> （1）创建一个RDD<br> scala&gt; val rdd = sc.parallelize(1 to 10)<br> rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at :24<br> （2）统计该RDD的条数<br> scala&gt; rdd.first<br> res2: Int = 1<br> 2.4.5 take(n)案例</li><li>作用：返回一个由RDD的前n个元素组成的数组</li><li>需求：创建一个RDD，统计该RDD的条数<br> （1）创建一个RDD<br> scala&gt; val rdd = sc.parallelize(Array(2,5,4,6,8,3))<br> rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at :24<br> （2）统计该RDD的条数<br> scala&gt; rdd.take(3)<br> res10: Array[Int] = Array(2, 5, 4)<br> 2.4.6 takeOrdered(n)案例</li><li>作用：返回该RDD排序后的前n个元素组成的数组</li><li>需求：创建一个RDD，统计该RDD的条数<br> （1）创建一个RDD<br> scala&gt; val rdd = sc.parallelize(Array(2,5,4,6,8,3))<br> rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at :24<br> （2）统计该RDD的条数<br> scala&gt; rdd.takeOrdered(3)<br> res18: Array[Int] = Array(2, 3, 4)<br> 2.4.7 aggregate案例</li><li>参数：(zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)</li><li>作用：aggregate函数将每个分区里面的元素通过seqOp和初始值进行聚合，然后用combine函数将每个分区的结果和初始值(zeroValue)进行combine操作。这个函数最终返回的类型不需要和RDD中元素类型一致。</li><li>需求：创建一个RDD，将所有元素相加得到结果<br> （1）创建一个RDD<br> scala&gt; var rdd1 = sc.makeRDD(1 to 10,2)<br> rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[88] at makeRDD at :24<br> （2）将该RDD所有元素相加得到结果<br> scala&gt; rdd.aggregate(0)(<em>+</em>,<em>+</em>)<br> res22: Int = 55<br> 2.4.8 fold(num)(func)案例</li><li>作用：折叠操作，aggregate的简化操作，seqop和combop一样。</li><li>需求：创建一个RDD，将所有元素相加得到结果<br> （1）创建一个RDD<br> scala&gt; var rdd1 = sc.makeRDD(1 to 10,2)<br> rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[88] at makeRDD at :24<br> （2）将该RDD所有元素相加得到结果<br> scala&gt; rdd.fold(0)(<em>+</em>)<br> res24: Int = 55<br> 2.4.9 saveAsTextFile(path)<br> 作用：将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本<br> 2.4.10 saveAsSequenceFile(path)<br> 作用：将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。<br> 2.4.11 saveAsObjectFile(path)<br> 作用：用于将RDD中的元素序列化成对象，存储到文件中。<br> 2.4.12 countByKey()案例</li><li>作用：针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。</li><li>需求：创建一个PairRDD，统计每种key的个数<br> （1）创建一个PairRDD<br> scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),3)<br> rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[95] at parallelize at :24<br> （2）统计每种key的个数<br> scala&gt; rdd.countByKey<br> res63: scala.collection.Map[Int,Long] = Map(3 -&gt; 2, 1 -&gt; 3, 2 -&gt; 1)<br> 2.4.13 foreach(func)案例</li><li>作用：在数据集的每一个元素上，运行函数func进行更新。</li><li>需求：创建一个RDD，对每个元素进行打印<br> （1）创建一个RDD<br> scala&gt; var rdd = sc.makeRDD(1 to 5,2)<br> rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[107] at makeRDD at :24<br> （2）对该RDD每个元素进行打印<br> scala&gt; rdd.foreach(println(_))<br> 3<br> 4<br> 5<br> 1<br> 2<br> 2.5 RDD中的函数传递<br> 在实际开发中我们往往需要自己定义一些对于RDD的操作，那么此时需要主要的是，初始化工作是在Driver端进行的，而实际运行程序是在Executor端进行的，这就涉及到了跨进程通信，是需要序列化的。下面我们看几个例子：<br> 2.5.1 传递一个方法<br> 1．创建一个类</li></ol> 
<pre><code class="prism language-bash">class Search<span class="token punctuation">(</span>s:String<span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>

//过滤出包含字符串的数据
  def isMatch<span class="token punctuation">(</span>s: String<span class="token punctuation">)</span>: Boolean <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    s.contains<span class="token punctuation">(</span>query<span class="token punctuation">)</span>
  <span class="token punctuation">}</span>

//过滤出包含字符串的RDD
  def getMatch1 <span class="token punctuation">(</span>rdd: RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span>: RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    rdd.filter<span class="token punctuation">(</span>isMatch<span class="token punctuation">)</span>
  <span class="token punctuation">}</span>

  //过滤出包含字符串的RDD
  def getMatche2<span class="token punctuation">(</span>rdd: RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span>: RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    rdd.filter<span class="token punctuation">(</span>x <span class="token operator">=</span><span class="token operator">&gt;</span> x.contains<span class="token punctuation">(</span>query<span class="token punctuation">))</span>
  <span class="token punctuation">}</span>

<span class="token punctuation">}</span>
<span class="token number">2</span>．创建Spark主程序
object SeriTest <span class="token punctuation">{<!-- --></span>

  def main<span class="token punctuation">(</span>args: Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span>: Unit <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>

   //1.初始化配置信息及SparkContext
    val sparkConf: SparkConf <span class="token operator">=</span> new SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span>.setAppName<span class="token punctuation">(</span><span class="token string">"WordCount"</span><span class="token punctuation">)</span>.setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    val sc <span class="token operator">=</span> new SparkContext<span class="token punctuation">(</span>sparkConf<span class="token punctuation">)</span>

//2.创建一个RDD
    val rdd: RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>Array<span class="token punctuation">(</span><span class="token string">"hadoop"</span>, <span class="token string">"spark"</span>, <span class="token string">"hive"</span>, <span class="token string">"atguigu"</span><span class="token punctuation">))</span>

//3.创建一个Search对象
    val search <span class="token operator">=</span> new Search<span class="token punctuation">(</span><span class="token punctuation">)</span>

//4.运用第一个过滤函数并打印结果
    val match1: RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> search.getMatche1<span class="token punctuation">(</span>rdd<span class="token punctuation">)</span>
    match1.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>.foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>3．运行程序</p> 
<pre><code class="prism language-bash">Exception <span class="token keyword">in</span> thread <span class="token string">"main"</span> org.apache.spark.SparkException: Task not serializable
    at org.apache.spark.util.ClosureCleaner$.ensureSerializable<span class="token punctuation">(</span>ClosureCleaner.scala:298<span class="token punctuation">)</span>
    at org.apache.spark.util.ClosureCleaner$.org<span class="token variable">$apache</span><span class="token variable">$spark</span><span class="token variable">$util</span><span class="token variable">$ClosureCleaner</span><span class="token variable">$$</span>clean<span class="token punctuation">(</span>ClosureCleaner.scala:288<span class="token punctuation">)</span>
    at org.apache.spark.util.ClosureCleaner$.clean<span class="token punctuation">(</span>ClosureCleaner.scala:108<span class="token punctuation">)</span>
    at org.apache.spark.SparkContext.clean<span class="token punctuation">(</span>SparkContext.scala:2101<span class="token punctuation">)</span>
    at org.apache.spark.rdd.RDD<span class="token variable">$$</span>anonfun<span class="token variable">$filter</span><span class="token variable">$1</span>.apply<span class="token punctuation">(</span>RDD.scala:387<span class="token punctuation">)</span>
    at org.apache.spark.rdd.RDD<span class="token variable">$$</span>anonfun<span class="token variable">$filter</span><span class="token variable">$1</span>.apply<span class="token punctuation">(</span>RDD.scala:386<span class="token punctuation">)</span>
    at org.apache.spark.rdd.RDDOperationScope$.withScope<span class="token punctuation">(</span>RDDOperationScope.scala:151<span class="token punctuation">)</span>
    at org.apache.spark.rdd.RDDOperationScope$.withScope<span class="token punctuation">(</span>RDDOperationScope.scala:112<span class="token punctuation">)</span>
    at org.apache.spark.rdd.RDD.withScope<span class="token punctuation">(</span>RDD.scala:362<span class="token punctuation">)</span>
    at org.apache.spark.rdd.RDD.filter<span class="token punctuation">(</span>RDD.scala:386<span class="token punctuation">)</span>
    at com.atguigu.Search.getMatche1<span class="token punctuation">(</span>SeriTest.scala:39<span class="token punctuation">)</span>
    at com.atguigu.SeriTest$.main<span class="token punctuation">(</span>SeriTest.scala:18<span class="token punctuation">)</span>
    at com.atguigu.SeriTest.main<span class="token punctuation">(</span>SeriTest.scala<span class="token punctuation">)</span>
Caused by: java.io.NotSerializableException: com.atguigu.Search
</code></pre> 
<p>4．问题说明</p> 
<pre><code class="prism language-bash">//过滤出包含字符串的RDD
  def getMatch1 <span class="token punctuation">(</span>rdd: RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span>: RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    rdd.filter<span class="token punctuation">(</span>isMatch<span class="token punctuation">)</span>
  <span class="token punctuation">}</span>
</code></pre> 
<p>在这个方法中所调用的方法isMatch()是定义在Search这个类中的，实际上调用的是this. isMatch()，this表示Search这个类的对象，程序在运行过程中需要将Search对象序列化以后传递到Executor端。<br> 5．解决方案<br> 使类继承scala.Serializable即可。</p> 
<pre><code class="prism language-bash">class Search<span class="token punctuation">(</span><span class="token punctuation">)</span> extends Serializable<span class="token punctuation">{<!-- --></span><span class="token punctuation">..</span>.<span class="token punctuation">}</span>
</code></pre> 
<p>2.5.2 传递一个属性<br> 1．创建Spark主程序</p> 
<pre><code class="prism language-bash">object TransmitTest <span class="token punctuation">{<!-- --></span>

  def main<span class="token punctuation">(</span>args: Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span>: Unit <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>

    //1.初始化配置信息及SparkContext
    val sparkConf: SparkConf <span class="token operator">=</span> new SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span>.setAppName<span class="token punctuation">(</span><span class="token string">"WordCount"</span><span class="token punctuation">)</span>.setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>
    val sc <span class="token operator">=</span> new SparkContext<span class="token punctuation">(</span>sparkConf<span class="token punctuation">)</span>

//2.创建一个RDD
    val rdd: RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>Array<span class="token punctuation">(</span><span class="token string">"hadoop"</span>, <span class="token string">"spark"</span>, <span class="token string">"hive"</span>, <span class="token string">"atguigu"</span><span class="token punctuation">))</span>

//3.创建一个Search对象
    val search <span class="token operator">=</span> new Search<span class="token punctuation">(</span><span class="token punctuation">)</span>

//4.运用第一个过滤函数并打印结果
    val match1: RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> search.getMatche2<span class="token punctuation">(</span>rdd<span class="token punctuation">)</span>
    match1.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>.foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>2．运行程序</p> 
<pre><code class="prism language-bash">Exception <span class="token keyword">in</span> thread <span class="token string">"main"</span> org.apache.spark.SparkException: Task not serializable
    at org.apache.spark.util.ClosureCleaner$.ensureSerializable<span class="token punctuation">(</span>ClosureCleaner.scala:298<span class="token punctuation">)</span>
    at org.apache.spark.util.ClosureCleaner$.org<span class="token variable">$apache</span><span class="token variable">$spark</span><span class="token variable">$util</span><span class="token variable">$ClosureCleaner</span><span class="token variable">$$</span>clean<span class="token punctuation">(</span>ClosureCleaner.scala:288<span class="token punctuation">)</span>
    at org.apache.spark.util.ClosureCleaner$.clean<span class="token punctuation">(</span>ClosureCleaner.scala:108<span class="token punctuation">)</span>
    at org.apache.spark.SparkContext.clean<span class="token punctuation">(</span>SparkContext.scala:2101<span class="token punctuation">)</span>
    at org.apache.spark.rdd.RDD<span class="token variable">$$</span>anonfun<span class="token variable">$filter</span><span class="token variable">$1</span>.apply<span class="token punctuation">(</span>RDD.scala:387<span class="token punctuation">)</span>
    at org.apache.spark.rdd.RDD<span class="token variable">$$</span>anonfun<span class="token variable">$filter</span><span class="token variable">$1</span>.apply<span class="token punctuation">(</span>RDD.scala:386<span class="token punctuation">)</span>
    at org.apache.spark.rdd.RDDOperationScope$.withScope<span class="token punctuation">(</span>RDDOperationScope.scala:151<span class="token punctuation">)</span>
    at org.apache.spark.rdd.RDDOperationScope$.withScope<span class="token punctuation">(</span>RDDOperationScope.scala:112<span class="token punctuation">)</span>
    at org.apache.spark.rdd.RDD.withScope<span class="token punctuation">(</span>RDD.scala:362<span class="token punctuation">)</span>
    at org.apache.spark.rdd.RDD.filter<span class="token punctuation">(</span>RDD.scala:386<span class="token punctuation">)</span>
    at com.atguigu.Search.getMatche1<span class="token punctuation">(</span>SeriTest.scala:39<span class="token punctuation">)</span>
    at com.atguigu.SeriTest$.main<span class="token punctuation">(</span>SeriTest.scala:18<span class="token punctuation">)</span>
    at com.atguigu.SeriTest.main<span class="token punctuation">(</span>SeriTest.scala<span class="token punctuation">)</span>
Caused by: java.io.NotSerializableException: com.atguigu.Search
</code></pre> 
<p>3．问题说明</p> 
<pre><code class="prism language-bash"> //过滤出包含字符串的RDD
  def getMatche2<span class="token punctuation">(</span>rdd: RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span>: RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    rdd.filter<span class="token punctuation">(</span>x <span class="token operator">=</span><span class="token operator">&gt;</span> x.contains<span class="token punctuation">(</span>query<span class="token punctuation">))</span>
  <span class="token punctuation">}</span>
</code></pre> 
<p>在这个方法中所调用的方法query是定义在Search这个类中的字段，实际上调用的是this. query，this表示Search这个类的对象，程序在运行过程中需要将Search对象序列化以后传递到Executor端。<br> 4．解决方案<br> 1）使类继承scala.Serializable即可。</p> 
<pre><code class="prism language-bash">class Search<span class="token punctuation">(</span><span class="token punctuation">)</span> extends Serializable<span class="token punctuation">{<!-- --></span><span class="token punctuation">..</span>.<span class="token punctuation">}</span>
</code></pre> 
<p>2）将类变量query赋值给局部变量<br> 修改getMatche2为</p> 
<pre><code class="prism language-bash">  //过滤出包含字符串的RDD
  def getMatche2<span class="token punctuation">(</span>rdd: RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span>: RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    val query_ <span class="token builtin class-name">:</span> String <span class="token operator">=</span> this.query//将类变量赋值给局部变量
    rdd.filter<span class="token punctuation">(</span>x <span class="token operator">=</span><span class="token operator">&gt;</span> x.contains<span class="token punctuation">(</span>query_<span class="token punctuation">))</span>
  <span class="token punctuation">}</span>
</code></pre> 
<p>2.6 RDD依赖关系<br> 2.6.1 Lineage<br> RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。<img src="https://images2.imgbox.com/95/01/mG6EKRzp_o.png" alt="在这里插入图片描述"><br> （1）读取一个HDFS文件并将其中内容映射成一个个元组<br> scala&gt; val wordAndOne = sc.textFile(“/fruit.tsv”).flatMap(<em>.split(“\t”)).map((</em>,1))<br> wordAndOne: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[22] at map at :24<br> （2）统计每一种key对应的个数<br> scala&gt; val wordAndCount = wordAndOne.reduceByKey(<em>+</em>)<br> wordAndCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[23] at reduceByKey at :26<br> （3）查看“wordAndOne”的Lineage<br> scala&gt; wordAndOne.toDebugString<br> res5: String =<br> (2) MapPartitionsRDD[22] at map at :24 []<br> | MapPartitionsRDD[21] at flatMap at :24 []<br> | /fruit.tsv MapPartitionsRDD[20] at textFile at :24 []<br> | /fruit.tsv HadoopRDD[19] at textFile at :24 []<br> （4）查看“wordAndCount”的Lineage<br> scala&gt; wordAndCount.toDebugString<br> res6: String =<br> (2) ShuffledRDD[23] at reduceByKey at :26 []<br> ±(2) MapPartitionsRDD[22] at map at :24 []<br> | MapPartitionsRDD[21] at flatMap at :24 []<br> | /fruit.tsv MapPartitionsRDD[20] at textFile at :24 []<br> | /fruit.tsv HadoopRDD[19] at textFile at :24 []<br> （5）查看“wordAndOne”的依赖类型<br> scala&gt; wordAndOne.dependencies<br> res7: Seq[org.apache.spark.Dependency[<em>]] = List(org.apache.spark.OneToOneDependency@5d5db92b)<br> （6）查看“wordAndCount”的依赖类型<br> scala&gt; wordAndCount.dependencies<br> res8: Seq[org.apache.spark.Dependency[</em>]] = List(org.apache.spark.ShuffleDependency@63f3e6a8)<br> 注意：RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。<br> 2.6.2 窄依赖 <br> 窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用,<strong>窄依赖我们形象的比喻为独生子女</strong><img src="https://images2.imgbox.com/f0/7c/4NZzB4aB_o.png" alt="在这里插入图片描述"><br> 2.6.3 宽依赖<br> 宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition，会引起shuffle,总结：<strong>宽依赖我们形象的比喻为超生</strong><img src="https://images2.imgbox.com/24/4a/i1yGKXaq_o.png" alt="在这里插入图片描述"><br> 2.6.4 DAG<br> DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。<img src="https://images2.imgbox.com/f7/4a/1AIDzx6t_o.png" alt="在这里插入图片描述"><br> 2.6.5 任务划分（面试重点）<br> RDD任务切分中间分为：Application、Job、Stage和Task<br> 1）Application：初始化一个SparkContext即生成一个Application<br> 2）Job：一个Action算子就会生成一个Job<br> 3）Stage：根据RDD之间的依赖关系的不同将Job划分成不同的Stage，遇到一个宽依赖则划分一个Stage。<img src="https://images2.imgbox.com/7b/7b/xtpLglvd_o.png" alt="在这里插入图片描述"><br> 4）Task：Stage是一个TaskSet，将Stage划分的结果发送到不同的Executor执行即为一个Task。<br> 注意：Application-&gt;Job-&gt;Stage-&gt; Task每一层都是1对n的关系。<br> 2.7 RDD缓存<br> RDD通过persist方法或cache方法可以将前面的计算结果缓存，默认情况下 persist() 会把数据以序列化的形式缓存在 JVM 的堆空间中。<br> 但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。<br> <img src="https://images2.imgbox.com/d3/81/jhcJJ6Iz_o.png" alt="在这里插入图片描述"><br> 通过查看源码发现cache最终也是调用了persist方法，默认的存储级别都是仅在内存存储一份，Spark的存储级别还有好多种，存储级别在object StorageLevel中定义的。<br> <img src="https://images2.imgbox.com/25/91/dOYOQGcd_o.png" alt="在这里插入图片描述"><br> 在存储级别的末尾加上“<em>2”来把持久化数据存为两份<img src="https://images2.imgbox.com/02/c5/nX4gLkVE_o.png" alt="在这里插入图片描述"><br> 缓存有可能丢失，或者存储存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。<br> （1）创建一个RDD<br> scala&gt; val rdd = sc.makeRDD(Array(“atguigu”))<br> rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[19] at makeRDD at :25<br> （2）将RDD转换为携带当前时间戳不做缓存<br> scala&gt; val nocache = rdd.map(</em>.toString+System.currentTimeMillis)<br> nocache: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[20] at map at :27<br> （3）多次打印结果<br> scala&gt; nocache.collect<br> res0: Array[String] = Array(atguigu1538978275359)</p> 
<p>scala&gt; nocache.collect<br> res1: Array[String] = Array(atguigu1538978282416)</p> 
<p>scala&gt; nocache.collect<br> res2: Array[String] = Array(atguigu1538978283199)<br> （4）将RDD转换为携带当前时间戳并做缓存<br> scala&gt; val cache = rdd.map(_.toString+System.currentTimeMillis).cache<br> cache: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[21] at map at :27<br> （5）多次打印做了缓存的结果<br> scala&gt; cache.collect<br> res3: Array[String] = Array(atguigu1538978435705)</p> 
<p>scala&gt; cache.collect<br> res4: Array[String] = Array(atguigu1538978435705)</p> 
<p>scala&gt; cache.collect<br> res5: Array[String] = Array(atguigu1538978435705)<br> 2.8 RDD CheckPoint<br> Spark中对于数据的保存除了持久化操作之外，还提供了一种检查点的机制，检查点（本质是通过将RDD写入Disk做检查点）是为了通过lineage做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的RDD开始重做Lineage，就会减少开销。检查点通过将数据写入到HDFS文件系统实现了RDD的检查点功能。<br> 为当前RDD设置检查点。该函数将会创建一个二进制的文件，并存储到checkpoint目录中，该目录是用SparkContext.setCheckpointDir()设置的。在checkpoint的过程中，该RDD的所有依赖于父RDD中的信息将全部被移除。对RDD进行checkpoint操作并不会马上被执行，必须执行Action操作才能触发。<br> 案例实操：<br> （1）设置检查点<br> scala&gt; sc.setCheckpointDir(“hdfs://hadoop102:9000/checkpoint”)<br> （2）创建一个RDD<br> scala&gt; val rdd = sc.parallelize(Array(“atguigu”))<br> rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[14] at parallelize at :24<br> （3）将RDD转换为携带当前时间戳并做checkpoint<br> scala&gt; val ch = rdd.map(_+System.currentTimeMillis)<br> ch: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[16] at map at :26</p> 
<p>scala&gt; ch.checkpoint<br> （4）多次打印结果<br> scala&gt; ch.collect<br> res55: Array[String] = Array(atguigu1538981860336)</p> 
<p>scala&gt; ch.collect<br> res56: Array[String] = Array(atguigu1538981860504)</p> 
<p>scala&gt; ch.collect<br> res57: Array[String] = Array(atguigu1538981860504)</p> 
<p>scala&gt; ch.collect<br> res58: Array[String] = Array(atguigu1538981860504)<br> 第3章 键值对RDD数据分区器<br> Spark目前支持Hash分区和Range分区，用户也可以自定义分区，Hash分区为当前的默认分区，Spark中分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle过程属于哪个分区和Reduce的个数<br> 注意：<br> (1)只有Key-Value类型的RDD才有分区器的，非Key-Value类型的RDD分区器的值是None<br> (2)每个RDD的分区ID范围：0~numPartitions-1，决定这个值是属于那个分区的。<br> 3.1 获取RDD分区<br> 可以通过使用RDD的partitioner 属性来获取 RDD 的分区方式。它会返回一个 scala.Option 对象， 通过get方法获取其中的值。相关源码如下：<br> def getPartition(key: Any): Int = key match {<!-- --><br> case null =&gt; 0<br> case _ =&gt; Utils.nonNegativeMod(key.hashCode, numPartitions)<br> }<br> def nonNegativeMod(x: Int, mod: Int): Int = {<!-- --><br> val rawMod = x % mod<br> rawMod + (if (rawMod &lt; 0) mod else 0)<br> }<br> （1）创建一个pairRDD<br> scala&gt; val pairs = sc.parallelize(List((1,1),(2,2),(3,3)))<br> pairs: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[3] at parallelize at :24<br> （2）查看RDD的分区器<br> scala&gt; pairs.partitioner<br> res1: Option[org.apache.spark.Partitioner] = None<br> （3）导入HashPartitioner类<br> scala&gt; import org.apache.spark.HashPartitioner<br> import org.apache.spark.HashPartitioner<br> （4）使用HashPartitioner对RDD进行重新分区<br> scala&gt; val partitioned = pairs.partitionBy(new HashPartitioner(2))<br> partitioned: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[4] at partitionBy at :27<br> （5）查看重新分区后RDD的分区器<br> scala&gt; partitioned.partitioner<br> res2: Option[org.apache.spark.Partitioner] = Some(org.apache.spark.HashPartitioner@2)<br> <strong>3.2 Hash分区</strong><br> HashPartitioner分区的原理：对于给定的key，计算其hashCode，并除以分区的个数取余，如果余数小于0，则用余数+分区的个数（否则加0），最后返回的值就是这个key所属的分区ID。<br> 使用Hash分区的实操</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> nopar.partitioner
res20: Option<span class="token punctuation">[</span>org.apache.spark.Partitioner<span class="token punctuation">]</span> <span class="token operator">=</span> None
</code></pre> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val nopar <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>List<span class="token variable"><span class="token punctuation">((</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">))</span></span>,8<span class="token punctuation">)</span>
nopar: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span>nopar.mapPartitionsWithIndex<span class="token variable"><span class="token punctuation">((</span>index<span class="token punctuation">,</span>iter<span class="token punctuation">)</span><span class="token operator">=</span><span class="token operator">&gt;</span>{ Iterator<span class="token punctuation">(</span>index.toString<span class="token operator">+</span>" <span class="token operator">:</span> "<span class="token operator">+</span>iter.mkString<span class="token punctuation">(</span>"<span class="token operator">|</span>"<span class="token punctuation">))</span></span> <span class="token punctuation">}</span><span class="token punctuation">)</span>.collect
res0: Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span><span class="token string">"0 : "</span>, <span class="token number">1</span> <span class="token builtin class-name">:</span> <span class="token punctuation">(</span><span class="token number">1,3</span><span class="token punctuation">)</span>, <span class="token number">2</span> <span class="token builtin class-name">:</span> <span class="token punctuation">(</span><span class="token number">1,2</span><span class="token punctuation">)</span>, <span class="token number">3</span> <span class="token builtin class-name">:</span> <span class="token punctuation">(</span><span class="token number">2,4</span><span class="token punctuation">)</span>, <span class="token string">"4 : "</span>, <span class="token number">5</span> <span class="token builtin class-name">:</span> <span class="token punctuation">(</span><span class="token number">2,3</span><span class="token punctuation">)</span>, <span class="token number">6</span> <span class="token builtin class-name">:</span> <span class="token punctuation">(</span><span class="token number">3,6</span><span class="token punctuation">)</span>, <span class="token number">7</span> <span class="token builtin class-name">:</span> <span class="token punctuation">(</span><span class="token number">3,8</span><span class="token punctuation">))</span> 
</code></pre> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val hashpar <span class="token operator">=</span> nopar.partitionBy<span class="token punctuation">(</span>new org.apache.spark.HashPartitioner<span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">))</span>
hashpar: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> ShuffledRDD<span class="token punctuation">[</span><span class="token number">12</span><span class="token punctuation">]</span> at partitionBy at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:26
</code></pre> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> hashpar.count
res18: Long <span class="token operator">=</span> <span class="token number">6</span>
</code></pre> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> hashpar.partitioner
res21: Option<span class="token punctuation">[</span>org.apache.spark.Partitioner<span class="token punctuation">]</span> <span class="token operator">=</span> Some<span class="token punctuation">(</span>org.apache.spark.HashPartitioner@7<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> hashpar.mapPartitions<span class="token punctuation">(</span>iter <span class="token operator">=</span><span class="token operator">&gt;</span> Iterator<span class="token punctuation">(</span>iter.length<span class="token punctuation">))</span>.collect<span class="token punctuation">(</span><span class="token punctuation">)</span>
res19: Array<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span><span class="token number">0</span>, <span class="token number">3</span>, <span class="token number">1</span>, <span class="token number">2</span>, <span class="token number">0</span>, <span class="token number">0</span>, <span class="token number">0</span><span class="token punctuation">)</span>
</code></pre> 
<p>3.3 Ranger分区<br> HashPartitioner分区弊端：可能导致每个分区中数据量的不均匀，极端情况下会导致某些分区拥有RDD的全部数据。<br> RangePartitioner作用：将一定范围内的数映射到某一个分区内，尽量保证每个分区中数据量的均匀，而且分区与分区之间是有序的，一个分区中的元素肯定都是比另一个分区内的元素小或者大，但是分区内的元素是不能保证顺序的。简单的说就是将一定范围内的数映射到某一个分区内。实现过程为：<br> 第一步：先重整个RDD中抽取出样本数据，将样本数据排序，计算出每个分区的最大key值，形成一个Array[KEY]类型的数组变量rangeBounds；<br> 第二步：判断key在rangeBounds中所处的范围，给出该key值在下一个RDD中的分区id下标；该分区器要求RDD中的KEY类型必须是可以排序的<br> 3.4 自定义分区<br> 要实现自定义的分区器，你需要继承 org.apache.spark.Partitioner 类并实现下面三个方法。<br> （1）numPartitions: Int:返回创建出来的分区数。<br> （2）getPartition(key: Any): Int:返回给定键的分区编号(0到numPartitions-1)。<br> （3）equals():Java 判断相等性的标准方法。这个方法的实现非常重要，Spark 需要用这个方法来检查你的分区器对象是否和其他分区器实例相同，这样 Spark 才可以判断两个 RDD 的分区方式是否相同。<br> 需求：将相同后缀的数据写入相同的文件，通过将相同后缀的数据分区到相同的分区并保存输出来实现。<br> （1）创建一个pairRDD</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val data <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>Array<span class="token variable"><span class="token punctuation">((</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">))</span></span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-bash">data: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）定义一个自定义分区类</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> :paste
// Entering <span class="token function">paste</span> mode <span class="token punctuation">(</span>ctrl-D to finish<span class="token punctuation">)</span>
class CustomerPartitioner<span class="token punctuation">(</span>numParts:Int<span class="token punctuation">)</span> extends org.apache.spark.Partitioner<span class="token punctuation">{<!-- --></span>

  //覆盖分区数
  override def numPartitions: Int <span class="token operator">=</span> numParts

  //覆盖分区号获取函数
  override def getPartition<span class="token punctuation">(</span>key: Any<span class="token punctuation">)</span>: Int <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    val ckey: String <span class="token operator">=</span> key.toString
    ckey.substring<span class="token punctuation">(</span>ckey.length-1<span class="token punctuation">)</span>.toInt%numParts
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

// Exiting <span class="token function">paste</span> mode, now interpreting.

defined class CustomerPartitioner
</code></pre> 
<p>（3）将RDD使用自定义的分区类进行重新分区</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val par <span class="token operator">=</span> data.partitionBy<span class="token punctuation">(</span>new CustomerPartitioner<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">))</span>
par: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> ShuffledRDD<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> at partitionBy at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:27
</code></pre> 
<p>（4）查看重新分区后的数据分布</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> par.mapPartitionsWithIndex<span class="token variable"><span class="token punctuation">((</span>index<span class="token punctuation">,</span>items<span class="token punctuation">)</span><span class="token operator">=</span><span class="token operator">&gt;</span>items.map<span class="token punctuation">((</span>index<span class="token punctuation">,</span>_<span class="token punctuation">))</span></span><span class="token punctuation">)</span>.collect
res3: Array<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, <span class="token punctuation">(</span>Int, Int<span class="token punctuation">))</span><span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token variable"><span class="token punctuation">((</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">))</span></span>, <span class="token punctuation">(</span><span class="token number">0</span>,<span class="token punctuation">(</span><span class="token number">4,4</span><span class="token punctuation">))</span>, <span class="token punctuation">(</span><span class="token number">0</span>,<span class="token punctuation">(</span><span class="token number">6,6</span><span class="token punctuation">))</span>, <span class="token punctuation">(</span><span class="token number">1</span>,<span class="token punctuation">(</span><span class="token number">1,1</span><span class="token punctuation">))</span>, <span class="token punctuation">(</span><span class="token number">1</span>,<span class="token punctuation">(</span><span class="token number">3,3</span><span class="token punctuation">))</span>, <span class="token punctuation">(</span><span class="token number">1</span>,<span class="token punctuation">(</span><span class="token number">5,5</span><span class="token punctuation">))</span><span class="token punctuation">)</span>
</code></pre> 
<p>使用自定义的 Partitioner 是很容易的:只要把它传给 partitionBy() 方法即可。Spark 中有许多依赖于数据混洗的方法，比如 join() 和 groupByKey()，它们也可以接收一个可选的 Partitioner 对象来控制输出数据的分区方式。<br> 第4章 数据读取与保存<br> Spark的数据读取及数据保存可以从两个维度来作区分：文件格式以及文件系统。<br> 文件格式分为：Text文件、Json文件、Csv文件、Sequence文件以及Object文件；<br> 文件系统分为：本地文件系统、HDFS、HBASE以及数据库。<br> 4.1文件类数据读取与保存<br> 4.1.1 Text文件<br> 1）数据读取:textFile(String)</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val hdfsFile <span class="token operator">=</span> sc.textFile<span class="token punctuation">(</span><span class="token string">"hdfs://hadoop102:9000/fruit.txt"</span><span class="token punctuation">)</span>
hdfsFile: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> hdfs://hadoop102:9000/fruit.txt MapPartitionsRDD<span class="token punctuation">[</span><span class="token number">21</span><span class="token punctuation">]</span> at textFile at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>2）数据保存: saveAsTextFile(String)</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> hdfsFile.saveAsTextFile<span class="token punctuation">(</span><span class="token string">"/fruitOut"</span><span class="token punctuation">)</span>
</code></pre> 
<p>4.1.2 Json文件<br> 如果JSON文件中每一行就是一个JSON记录，那么可以通过将JSON文件当做文本文件来读取，然后利用相关的JSON库对每一条数据进行JSON解析。<br> 注意：使用RDD读取JSON文件处理很复杂，同时SparkSQL集成了很好的处理JSON文件的方式，所以应用中多是采用SparkSQL处理JSON文件。<br> （1）导入解析json所需的包</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> <span class="token function">import</span> scala.util.parsing.json.JSON
</code></pre> 
<p>（2）上传json文件到HDFS</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>atguigu@hadoop102 spark<span class="token punctuation">]</span>$ hadoop fs <span class="token parameter variable">-put</span> ./examples/src/main/resources/people.json /
</code></pre> 
<p>（3）读取文件</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val json <span class="token operator">=</span> sc.textFile<span class="token punctuation">(</span><span class="token string">"/people.json"</span><span class="token punctuation">)</span>
json: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> /people.json MapPartitionsRDD<span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span> at textFile at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（4）解析json数据</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val result  <span class="token operator">=</span> json.map<span class="token punctuation">(</span>JSON.parseFull<span class="token punctuation">)</span>
result: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Option<span class="token punctuation">[</span>Any<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> MapPartitionsRDD<span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span> at map at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:27
</code></pre> 
<p>（5）打印</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> result.collect
res11: Array<span class="token punctuation">[</span>Option<span class="token punctuation">[</span>Any<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span>Some<span class="token punctuation">(</span>Map<span class="token punctuation">(</span>name -<span class="token operator">&gt;</span> Michael<span class="token punctuation">))</span>, Some<span class="token punctuation">(</span>Map<span class="token punctuation">(</span>name -<span class="token operator">&gt;</span> Andy, age -<span class="token operator">&gt;</span> <span class="token number">30.0</span><span class="token punctuation">))</span>, Some<span class="token punctuation">(</span>Map<span class="token punctuation">(</span>name -<span class="token operator">&gt;</span> Justin, age -<span class="token operator">&gt;</span> <span class="token number">19.0</span><span class="token punctuation">))</span><span class="token punctuation">)</span>
</code></pre> 
<p>4.1.3 Sequence文件<br> SequenceFile文件是Hadoop用来存储二进制形式的key-value对而设计的一种平面文件(Flat File)。Spark 有专门用来读取 SequenceFile 的接口。在 SparkContext 中，可以调用 sequenceFile<a href="path" rel="nofollow"> keyClass, valueClass</a>。<br> 注意：SequenceFile文件只针对PairRDD<br> （1）创建一个RDD</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>Array<span class="token variable"><span class="token punctuation">((</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">))</span></span><span class="token punctuation">)</span>
rdd: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">13</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）将RDD保存为Sequence文件</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd.saveAsSequenceFile<span class="token punctuation">(</span><span class="token string">"file:///opt/module/spark/seqFile"</span><span class="token punctuation">)</span>
</code></pre> 
<p>（3）查看该文件</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>atguigu@hadoop102 seqFile<span class="token punctuation">]</span>$ <span class="token builtin class-name">pwd</span>
/opt/module/spark/seqFile
</code></pre> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>atguigu@hadoop102 seqFile<span class="token punctuation">]</span>$ ll
总用量 <span class="token number">8</span>
-rw-r--r-- <span class="token number">1</span> atguigu atguigu <span class="token number">108</span> <span class="token number">10</span>月  <span class="token number">9</span> <span class="token number">10</span>:29 part-00000
-rw-r--r-- <span class="token number">1</span> atguigu atguigu <span class="token number">124</span> <span class="token number">10</span>月  <span class="token number">9</span> <span class="token number">10</span>:29 part-00001
-rw-r--r-- <span class="token number">1</span> atguigu atguigu   <span class="token number">0</span> <span class="token number">10</span>月  <span class="token number">9</span> <span class="token number">10</span>:29 _SUCCESS

<span class="token punctuation">[</span>atguigu@hadoop102 seqFile<span class="token punctuation">]</span>$ <span class="token function">cat</span> part-00000
SEQ org.apache.hadoop.io.IntWritable org.apache.hadoop.io.IntWritableط
</code></pre> 
<p>（4）读取Sequence文件</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val <span class="token function">seq</span> <span class="token operator">=</span> sc.sequenceFile<span class="token punctuation">[</span>Int,Int<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"file:///opt/module/spark/seqFile"</span><span class="token punctuation">)</span>
seq: org.apache.spark.rdd.RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> MapPartitionsRDD<span class="token punctuation">[</span><span class="token number">18</span><span class="token punctuation">]</span> at sequenceFile at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（5）打印读取后的Sequence文件</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> seq.collect
res14: Array<span class="token punctuation">[</span><span class="token punctuation">(</span>Int, Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token variable"><span class="token punctuation">((</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">))</span></span>
</code></pre> 
<p>4.1.4 对象文件<br> 对象文件是将对象序列化后保存的文件，采用Java的序列化机制。可以通过objectFile<a href="path" rel="nofollow">k,v</a> 函数接收一个路径，读取对象文件，返回对应的 RDD，也可以通过调用saveAsObjectFile() 实现对对象文件的输出。因为是序列化所以要指定类型。<br> （1）创建一个RDD</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val rdd <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>Array<span class="token punctuation">(</span><span class="token number">1,2</span>,3,4<span class="token punctuation">))</span>
rdd: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> ParallelCollectionRDD<span class="token punctuation">[</span><span class="token number">19</span><span class="token punctuation">]</span> at parallelize at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（2）将RDD保存为Object文件</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> rdd.saveAsObjectFile<span class="token punctuation">(</span><span class="token string">"file:///opt/module/spark/objectFile"</span><span class="token punctuation">)</span>
</code></pre> 
<p>（3）查看该文件</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>atguigu@hadoop102 objectFile<span class="token punctuation">]</span>$ <span class="token builtin class-name">pwd</span>
/opt/module/spark/objectFile

<span class="token punctuation">[</span>atguigu@hadoop102 objectFile<span class="token punctuation">]</span>$ ll
总用量 <span class="token number">8</span>
-rw-r--r-- <span class="token number">1</span> atguigu atguigu <span class="token number">142</span> <span class="token number">10</span>月  <span class="token number">9</span> <span class="token number">10</span>:37 part-00000
-rw-r--r-- <span class="token number">1</span> atguigu atguigu <span class="token number">142</span> <span class="token number">10</span>月  <span class="token number">9</span> <span class="token number">10</span>:37 part-00001
-rw-r--r-- <span class="token number">1</span> atguigu atguigu   <span class="token number">0</span> <span class="token number">10</span>月  <span class="token number">9</span> <span class="token number">10</span>:37 _SUCCESS
</code></pre> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>atguigu@hadoop102 objectFile<span class="token punctuation">]</span>$ <span class="token function">cat</span> part-00000 
SEQ<span class="token operator">!</span>org.apache.hadoop.io.NullWritable"org.apache.hadoop.io.BytesWritableW@`l
</code></pre> 
<p>（4）读取Object文件</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val objFile <span class="token operator">=</span> sc.objectFile<span class="token punctuation">[</span>Int<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"file:///opt/module/spark/objectFile"</span><span class="token punctuation">)</span>
objFile: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> MapPartitionsRDD<span class="token punctuation">[</span><span class="token number">31</span><span class="token punctuation">]</span> at objectFile at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:24
</code></pre> 
<p>（5）打印读取后的Sequence文件</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> objFile.collect
res19: Array<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">2</span>, <span class="token number">3</span>, <span class="token number">4</span><span class="token punctuation">)</span>
</code></pre> 
<p>4.2文件系统类数据读取与保存<br> 4.2.1 HDFS<br> Spark的整个生态系统与Hadoop是完全兼容的,所以对于Hadoop所支持的文件类型或者数据库类型,Spark也同样支持.另外,由于Hadoop的API有新旧两个版本,所以Spark为了能够兼容Hadoop所有的版本,也提供了两套创建操作接口.对于外部存储创建操作而言,hadoopRDD和newHadoopRDD是最为抽象的两个函数接口,主要包含以下四个参数.<br> 1）输入格式(InputFormat): 制定数据输入的类型,如TextInputFormat等,新旧两个版本所引用的版本分别是org.apache.hadoop.mapred.InputFormat和org.apache.hadoop.mapreduce.InputFormat(NewInputFormat)<br> 2）键类型: 指定[K,V]键值对中K的类型<br> 3）值类型: 指定[K,V]键值对中V的类型<br> 4）分区值: 指定由外部存储生成的RDD的partition数量的最小值,如果没有指定,系统会使用默认值defaultMinSplits<br> 注意:其他创建操作的API接口都是为了方便最终的Spark程序开发者而设置的,是这两个接口的高效实现版本.例如,对于textFile而言,只有path这个指定文件路径的参数,其他参数在系统内部指定了默认值。<br> 1.在Hadoop中以压缩形式存储的数据,不需要指定解压方式就能够进行读取,因为Hadoop本身有一个解压器会根据压缩文件的后缀推断解压算法进行解压.<br> 2.如果用Spark从Hadoop中读取某种类型的数据不知道怎么读取的时候,上网查找一个使用map-reduce的时候是怎么读取这种这种数据的,然后再将对应的读取方式改写成上面的hadoopRDD和newAPIHadoopRDD两个类就行了<br> 4.2.2 MySQL数据库连接<br> 支持通过Java JDBC访问关系型数据库。需要通过JdbcRDD进行，示例如下:<br> （1）添加依赖</p> 
<pre><code class="prism language-bash"><span class="token operator">&lt;</span>dependency<span class="token operator">&gt;</span>
    <span class="token operator">&lt;</span>groupId<span class="token operator">&gt;</span>mysql<span class="token operator">&lt;</span>/groupId<span class="token operator">&gt;</span>
    <span class="token operator">&lt;</span>artifactId<span class="token operator">&gt;</span>mysql-connector-java<span class="token operator">&lt;</span>/artifactId<span class="token operator">&gt;</span>
    <span class="token operator">&lt;</span>version<span class="token operator">&gt;</span><span class="token number">5.1</span>.2<span class="token operator"><span class="token file-descriptor important">7</span>&lt;</span>/version<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>/dependency<span class="token operator">&gt;</span>
</code></pre> 
<p>（2）Mysql读取：</p> 
<pre><code class="prism language-bash">package com.atguigu

<span class="token function">import</span> java.sql.DriverManager

<span class="token function">import</span> org.apache.spark.rdd.JdbcRDD
<span class="token function">import</span> org.apache.spark.<span class="token punctuation">{<!-- --></span>SparkConf, SparkContext<span class="token punctuation">}</span>

object MysqlRDD <span class="token punctuation">{<!-- --></span>

 def main<span class="token punctuation">(</span>args: Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span>: Unit <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>

   //1.创建spark配置信息
   val sparkConf: SparkConf <span class="token operator">=</span> new SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span>.setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>.setAppName<span class="token punctuation">(</span><span class="token string">"JdbcRDD"</span><span class="token punctuation">)</span>

   //2.创建SparkContext
   val sc <span class="token operator">=</span> new SparkContext<span class="token punctuation">(</span>sparkConf
</code></pre> 
<p>)</p> 
<p>//3.定义连接mysql的参数</p> 
<pre><code class="prism language-bash">   val driver <span class="token operator">=</span> <span class="token string">"com.mysql.jdbc.Driver"</span>
   val url <span class="token operator">=</span> <span class="token string">"jdbc:mysql://hadoop102:3306/rdd"</span>
   val userName <span class="token operator">=</span> <span class="token string">"root"</span>
   val passWd <span class="token operator">=</span> <span class="token string">"000000"</span>

   //创建JdbcRDD
   val rdd <span class="token operator">=</span> new JdbcRDD<span class="token punctuation">(</span>sc, <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">{<!-- --></span>
     Class.forName<span class="token punctuation">(</span>driver<span class="token punctuation">)</span>
     DriverManager.getConnection<span class="token punctuation">(</span>url, userName, passWd<span class="token punctuation">)</span>
   <span class="token punctuation">}</span>,
     <span class="token string">"select * from <span class="token variable"><span class="token variable">`</span>rddtable<span class="token variable">`</span></span> where <span class="token variable"><span class="token variable">`</span><span class="token function">id</span><span class="token variable">`</span></span>&gt;=?;"</span>,
     <span class="token number">1</span>,
     <span class="token number">10</span>,
     <span class="token number">1</span>,
     r <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">(</span>r.getInt<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>, r.getString<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">))</span>
   <span class="token punctuation">)</span>

   //打印最后结果
   println<span class="token punctuation">(</span>rdd.count<span class="token punctuation">(</span><span class="token punctuation">))</span>
   rdd.foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>

   sc.stop<span class="token punctuation">(</span><span class="token punctuation">)</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>Mysql写入：</p> 
<pre><code class="prism language-bash">def main<span class="token punctuation">(</span>args: Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
  val sparkConf <span class="token operator">=</span> new SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span>.setMaster<span class="token punctuation">(</span><span class="token string">"local[2]"</span><span class="token punctuation">)</span>.setAppName<span class="token punctuation">(</span><span class="token string">"HBaseApp"</span><span class="token punctuation">)</span>
  val sc <span class="token operator">=</span> new SparkContext<span class="token punctuation">(</span>sparkConf<span class="token punctuation">)</span>
  val data <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>List<span class="token punctuation">(</span><span class="token string">"Female"</span>, <span class="token string">"Male"</span>,<span class="token string">"Female"</span><span class="token punctuation">))</span>

  data.foreachPartition<span class="token punctuation">(</span>insertData<span class="token punctuation">)</span>
<span class="token punctuation">}</span>

def insertData<span class="token punctuation">(</span>iterator: Iterator<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span>: Unit <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
Class.forName <span class="token punctuation">(</span><span class="token string">"com.mysql.jdbc.Driver"</span><span class="token punctuation">)</span>.newInstance<span class="token punctuation">(</span><span class="token punctuation">)</span>
  val conn <span class="token operator">=</span> java.sql.DriverManager.getConnection<span class="token punctuation">(</span><span class="token string">"jdbc:mysql://hadoop102:3306/rdd"</span>, <span class="token string">"root"</span>, <span class="token string">"000000"</span><span class="token punctuation">)</span>
  iterator.foreach<span class="token punctuation">(</span>data <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">{<!-- --></span>
    val <span class="token function">ps</span> <span class="token operator">=</span> conn.prepareStatement<span class="token punctuation">(</span><span class="token string">"insert into rddtable(name) values (?)"</span><span class="token punctuation">)</span>
    ps.setString<span class="token punctuation">(</span><span class="token number">1</span>, data<span class="token punctuation">)</span> 
    ps.executeUpdate<span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token punctuation">}</span><span class="token punctuation">)</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>4.2.3 HBase数据库<br> 由于 org.apache.hadoop.hbase.mapreduce.TableInputFormat 类的实现，Spark 可以通过Hadoop输入格式访问HBase。这个输入格式会返回键值对数据，其中键的类型为org. apache.hadoop.hbase.io.ImmutableBytesWritable，而值的类型为org.apache.hadoop.hbase.client.<br> Result。<br> （1）添加依赖</p> 
<pre><code class="prism language-bash"><span class="token operator">&lt;</span>dependency<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>groupId<span class="token operator">&gt;</span>org.apache.hbase<span class="token operator">&lt;</span>/groupId<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>artifactId<span class="token operator">&gt;</span>hbase-server<span class="token operator">&lt;</span>/artifactId<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>version<span class="token operator">&gt;</span><span class="token number">1.3</span>.<span class="token operator"><span class="token file-descriptor important">1</span>&lt;</span>/version<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>/dependency<span class="token operator">&gt;</span>

<span class="token operator">&lt;</span>dependency<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>groupId<span class="token operator">&gt;</span>org.apache.hbase<span class="token operator">&lt;</span>/groupId<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>artifactId<span class="token operator">&gt;</span>hbase-client<span class="token operator">&lt;</span>/artifactId<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>version<span class="token operator">&gt;</span><span class="token number">1.3</span>.<span class="token operator"><span class="token file-descriptor important">1</span>&lt;</span>/version<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>/dependency<span class="token operator">&gt;</span>
</code></pre> 
<p>（2）从HBase读取数据</p> 
<pre><code class="prism language-bash">package com.atguigu

<span class="token function">import</span> org.apache.hadoop.conf.Configuration
<span class="token function">import</span> org.apache.hadoop.hbase.HBaseConfiguration
<span class="token function">import</span> org.apache.hadoop.hbase.client.Result
<span class="token function">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable
<span class="token function">import</span> org.apache.hadoop.hbase.mapreduce.TableInputFormat
<span class="token function">import</span> org.apache.spark.rdd.RDD
<span class="token function">import</span> org.apache.spark.<span class="token punctuation">{<!-- --></span>SparkConf, SparkContext<span class="token punctuation">}</span>
<span class="token function">import</span> org.apache.hadoop.hbase.util.Bytes

object HBaseSpark <span class="token punctuation">{<!-- --></span>

  def main<span class="token punctuation">(</span>args: Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span>: Unit <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>

    //创建spark配置信息
    val sparkConf: SparkConf <span class="token operator">=</span> new SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span>.setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>.setAppName<span class="token punctuation">(</span><span class="token string">"JdbcRDD"</span><span class="token punctuation">)</span>

    //创建SparkContext
    val sc <span class="token operator">=</span> new SparkContext<span class="token punctuation">(</span>sparkConf<span class="token punctuation">)</span>

    //构建HBase配置信息
    val conf: Configuration <span class="token operator">=</span> HBaseConfiguration.create<span class="token punctuation">(</span><span class="token punctuation">)</span>
    conf.set<span class="token punctuation">(</span><span class="token string">"hbase.zookeeper.quorum"</span>, <span class="token string">"hadoop102,hadoop103,hadoop104"</span><span class="token punctuation">)</span>
    conf.set<span class="token punctuation">(</span>TableInputFormat.INPUT_TABLE, <span class="token string">"rddtable"</span><span class="token punctuation">)</span>

    //从HBase读取数据形成RDD
    val hbaseRDD: RDD<span class="token punctuation">[</span><span class="token punctuation">(</span>ImmutableBytesWritable, Result<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> sc.newAPIHadoopRDD<span class="token punctuation">(</span>
      conf,
      classOf<span class="token punctuation">[</span>TableInputFormat<span class="token punctuation">]</span>,
      classOf<span class="token punctuation">[</span>ImmutableBytesWritable<span class="token punctuation">]</span>,
      classOf<span class="token punctuation">[</span>Result<span class="token punctuation">]</span><span class="token punctuation">)</span>

    val count: Long <span class="token operator">=</span> hbaseRDD.count<span class="token punctuation">(</span><span class="token punctuation">)</span>
    println<span class="token punctuation">(</span>count<span class="token punctuation">)</span>

    //对hbaseRDD进行处理
    hbaseRDD.foreach <span class="token punctuation">{<!-- --></span>
      <span class="token keyword">case</span> <span class="token punctuation">(</span>_, result<span class="token punctuation">)</span> <span class="token operator">=</span><span class="token operator">&gt;</span>
        val key: String <span class="token operator">=</span> Bytes.toString<span class="token punctuation">(</span>result.getRow<span class="token punctuation">)</span>
        val name: String <span class="token operator">=</span> Bytes.toString<span class="token punctuation">(</span>result.getValue<span class="token punctuation">(</span>Bytes.toBytes<span class="token punctuation">(</span><span class="token string">"info"</span><span class="token punctuation">)</span>, Bytes.toBytes<span class="token punctuation">(</span><span class="token string">"name"</span><span class="token punctuation">))</span><span class="token punctuation">)</span>
        val color: String <span class="token operator">=</span> Bytes.toString<span class="token punctuation">(</span>result.getValue<span class="token punctuation">(</span>Bytes.toBytes<span class="token punctuation">(</span><span class="token string">"info"</span><span class="token punctuation">)</span>, Bytes.toBytes<span class="token punctuation">(</span><span class="token string">"color"</span><span class="token punctuation">))</span><span class="token punctuation">)</span>
        println<span class="token punctuation">(</span><span class="token string">"RowKey:"</span> + key + <span class="token string">",Name:"</span> + name + <span class="token string">",Color:"</span> + color<span class="token punctuation">)</span>
    <span class="token punctuation">}</span>

    //关闭连接
    sc.stop<span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token punctuation">}</span>

<span class="token punctuation">}</span>
</code></pre> 
<p>3）往HBase写入</p> 
<pre><code class="prism language-bash">def main<span class="token punctuation">(</span>args: Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
//获取Spark配置信息并创建与spark的连接
  val sparkConf <span class="token operator">=</span> new SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span>.setMaster<span class="token punctuation">(</span><span class="token string">"local[*]"</span><span class="token punctuation">)</span>.setAppName<span class="token punctuation">(</span><span class="token string">"HBaseApp"</span><span class="token punctuation">)</span>
  val sc <span class="token operator">=</span> new SparkContext<span class="token punctuation">(</span>sparkConf<span class="token punctuation">)</span>

//创建HBaseConf
  val conf <span class="token operator">=</span> HBaseConfiguration.create<span class="token punctuation">(</span><span class="token punctuation">)</span>
  val jobConf <span class="token operator">=</span> new JobConf<span class="token punctuation">(</span>conf<span class="token punctuation">)</span>
  jobConf.setOutputFormat<span class="token punctuation">(</span>classOf<span class="token punctuation">[</span>TableOutputFormat<span class="token punctuation">]</span><span class="token punctuation">)</span>
  jobConf.set<span class="token punctuation">(</span>TableOutputFormat.OUTPUT_TABLE, <span class="token string">"fruit_spark"</span><span class="token punctuation">)</span>

//构建Hbase表描述器
  val fruitTable <span class="token operator">=</span> TableName.valueOf<span class="token punctuation">(</span><span class="token string">"fruit_spark"</span><span class="token punctuation">)</span>
  val tableDescr <span class="token operator">=</span> new HTableDescriptor<span class="token punctuation">(</span>fruitTable<span class="token punctuation">)</span>
  tableDescr.addFamily<span class="token punctuation">(</span>new HColumnDescriptor<span class="token punctuation">(</span><span class="token string">"info"</span>.getBytes<span class="token punctuation">))</span>

//创建Hbase表
  val admin <span class="token operator">=</span> new HBaseAdmin<span class="token punctuation">(</span>conf<span class="token punctuation">)</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>admin.tableExists<span class="token punctuation">(</span>fruitTable<span class="token punctuation">))</span> <span class="token punctuation">{<!-- --></span>
    admin.disableTable<span class="token punctuation">(</span>fruitTable<span class="token punctuation">)</span>
    admin.deleteTable<span class="token punctuation">(</span>fruitTable<span class="token punctuation">)</span>
  <span class="token punctuation">}</span>
  admin.createTable<span class="token punctuation">(</span>tableDescr<span class="token punctuation">)</span>

//定义往Hbase插入数据的方法
  def convert<span class="token punctuation">(</span>triple: <span class="token punctuation">(</span>Int, String, Int<span class="token punctuation">))</span> <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    val put <span class="token operator">=</span> new Put<span class="token punctuation">(</span>Bytes.toBytes<span class="token punctuation">(</span>triple._1<span class="token punctuation">))</span>
    put.addImmutable<span class="token punctuation">(</span>Bytes.toBytes<span class="token punctuation">(</span><span class="token string">"info"</span><span class="token punctuation">)</span>, Bytes.toBytes<span class="token punctuation">(</span><span class="token string">"name"</span><span class="token punctuation">)</span>, Bytes.toBytes<span class="token punctuation">(</span>triple._2<span class="token punctuation">))</span>
    put.addImmutable<span class="token punctuation">(</span>Bytes.toBytes<span class="token punctuation">(</span><span class="token string">"info"</span><span class="token punctuation">)</span>, Bytes.toBytes<span class="token punctuation">(</span><span class="token string">"price"</span><span class="token punctuation">)</span>, Bytes.toBytes<span class="token punctuation">(</span>triple._3<span class="token punctuation">))</span>
    <span class="token punctuation">(</span>new ImmutableBytesWritable, put<span class="token punctuation">)</span>
  <span class="token punctuation">}</span>

//创建一个RDD
  val initialRDD <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>List<span class="token variable"><span class="token punctuation">((</span><span class="token number">1</span><span class="token punctuation">,</span>"apple"<span class="token punctuation">,</span><span class="token number">11</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span>"banana"<span class="token punctuation">,</span><span class="token number">12</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span>"pear"<span class="token punctuation">,</span><span class="token number">13</span><span class="token punctuation">))</span></span><span class="token punctuation">)</span>

//将RDD内容写到HBase
  val localData <span class="token operator">=</span> initialRDD.map<span class="token punctuation">(</span>convert<span class="token punctuation">)</span>

  localData.saveAsHadoopDataset<span class="token punctuation">(</span>jobConf<span class="token punctuation">)</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>第5章 RDD编程进阶<br> 5.1 累加器<br> 累加器用来对信息进行聚合，通常在向 Spark传递函数时，比如使用 map() 函数或者用 filter() 传条件时，可以使用驱动器程序中定义的变量，但是集群中运行的每个任务都会得到这些变量的一份新的副本，更新这些副本的值也不会影响驱动器中的对应变量。如果我们想实现所有分片处理时更新共享变量的功能，那么累加器可以实现我们想要的效果。<br> 5.1.1 系统累加器<br> 针对一个输入的日志文件，如果我们想计算文件中所有空行的数量，我们可以编写以下程序：</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val notice <span class="token operator">=</span> sc.textFile<span class="token punctuation">(</span><span class="token string">"./NOTICE"</span><span class="token punctuation">)</span>
notice: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> ./NOTICE MapPartitionsRDD<span class="token punctuation">[</span><span class="token number">40</span><span class="token punctuation">]</span> at textFile at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:32
</code></pre> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val blanklines <span class="token operator">=</span> sc.accumulator<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
warning: there were two deprecation warnings<span class="token punctuation">;</span> re-run with <span class="token parameter variable">-deprecation</span> <span class="token keyword">for</span> details
blanklines: org.apache.spark.Accumulator<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
</code></pre> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val tmp <span class="token operator">=</span> notice.flatMap<span class="token punctuation">(</span>line <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">{<!-- --></span>
     <span class="token operator">|</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>line <span class="token operator">==</span> <span class="token string">""</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
     <span class="token operator">|</span>       blanklines <span class="token operator">+=</span> <span class="token number">1</span>
     <span class="token operator">|</span>    <span class="token punctuation">}</span>
     <span class="token operator">|</span>    line.split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span>
     <span class="token operator">|</span> <span class="token punctuation">}</span><span class="token punctuation">)</span>
tmp: org.apache.spark.rdd.RDD<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> MapPartitionsRDD<span class="token punctuation">[</span><span class="token number">41</span><span class="token punctuation">]</span> at flatMap at <span class="token operator">&lt;</span>console<span class="token operator">&gt;</span>:36
</code></pre> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> tmp.count<span class="token punctuation">(</span><span class="token punctuation">)</span>
res31: Long <span class="token operator">=</span> <span class="token number">3213</span>
</code></pre> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> blanklines.value
res32: Int <span class="token operator">=</span> <span class="token number">171</span>
</code></pre> 
<h6><a id="_1804"></a>累加器的用法如下所示：</h6> 
<p>通过在驱动器中调用SparkContext.accumulator(initialValue)方法，创建出存有初始值的累加器。返回值为 org.apache.spark.Accumulator[T] 对象，其中 T 是初始值 initialValue 的类型。Spark闭包里的执行器代码可以使用累加器的 += 方法(在Java中是 add)增加累加器的值。 驱动器程序可以调用累加器的value属性(在Java中使用value()或setValue())来访问累加器的值。</p> 
<p>注意：工作节点上的任务不能访问累加器的值。从这些任务的角度来看，累加器是一个只写变量。</p> 
<p>对于要在行动操作中使用的累加器，Spark只会把每个任务对各累加器的修改应用一次。因此，如果想要一个无论在失败还是重复计算时都绝对可靠的累加器，我们必须把它放在 foreach() 这样的行动操作中。转化操作中累加器可能会发生不止一次更新</p> 
<p><strong>5.1.2 自定义累加器</strong><br> 自定义累加器类型的功能在1.X版本中就已经提供了，但是使用起来比较麻烦，在2.0版本后，累加器的易用性有了较大的改进，而且官方还提供了一个新的抽象类：AccumulatorV2来提供更加友好的自定义类型累加器的实现方式。实现自定义类型累加器需要继承AccumulatorV2并至少覆写下例中出现的方法，下面这个累加器可以用于在程序运行过程中收集一些文本类信息，最终以Set[String]的形式返回。1</p> 
<pre><code class="prism language-bash">package com.atguigu.spark

<span class="token function">import</span> org.apache.spark.util.AccumulatorV2
<span class="token function">import</span> org.apache.spark.<span class="token punctuation">{<!-- --></span>SparkConf, SparkContext<span class="token punctuation">}</span>
<span class="token function">import</span> scala.collection.JavaConversions._

class LogAccumulator extends org.apache.spark.util.AccumulatorV2<span class="token punctuation">[</span>String, java.util.Set<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token punctuation">{<!-- --></span>
  private val _logArray: java.util.Set<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> new java.util.HashSet<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

  override def isZero: Boolean <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    _logArray.isEmpty
  <span class="token punctuation">}</span>

  override def reset<span class="token punctuation">(</span><span class="token punctuation">)</span>: Unit <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    _logArray.clear<span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token punctuation">}</span>

  override def add<span class="token punctuation">(</span>v: String<span class="token punctuation">)</span>: Unit <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    _logArray.add<span class="token punctuation">(</span>v<span class="token punctuation">)</span>
  <span class="token punctuation">}</span>

  override def merge<span class="token punctuation">(</span>other: org.apache.spark.util.AccumulatorV2<span class="token punctuation">[</span>String, java.util.Set<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>: Unit <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    other match <span class="token punctuation">{<!-- --></span>
      <span class="token keyword">case</span> o: LogAccumulator <span class="token operator">=</span><span class="token operator">&gt;</span> _logArray.addAll<span class="token punctuation">(</span>o.value<span class="token punctuation">)</span>
    <span class="token punctuation">}</span>

  <span class="token punctuation">}</span>

  override def value: java.util.Set<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    java.util.Collections.unmodifiableSet<span class="token punctuation">(</span>_logArray<span class="token punctuation">)</span>
  <span class="token punctuation">}</span>

  override def copy<span class="token punctuation">(</span><span class="token punctuation">)</span>:org.apache.spark.util.AccumulatorV2<span class="token punctuation">[</span>String, java.util.Set<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    val newAcc <span class="token operator">=</span> new LogAccumulator<span class="token punctuation">(</span><span class="token punctuation">)</span>
    _logArray.synchronized<span class="token punctuation">{<!-- --></span>
      newAcc._logArray.addAll<span class="token punctuation">(</span>_logArray<span class="token punctuation">)</span>
    <span class="token punctuation">}</span>
    newAcc
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

// 过滤掉带字母的
object LogAccumulator <span class="token punctuation">{<!-- --></span>
  def main<span class="token punctuation">(</span>args: Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    val <span class="token assign-left variable">conf</span><span class="token operator">=</span>new SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span>.setAppName<span class="token punctuation">(</span><span class="token string">"LogAccumulator"</span><span class="token punctuation">)</span>
    val <span class="token assign-left variable">sc</span><span class="token operator">=</span>new SparkContext<span class="token punctuation">(</span>conf<span class="token punctuation">)</span>

   val accum <span class="token operator">=</span> new LogAccumulator
    sc.register<span class="token punctuation">(</span>accum, <span class="token string">"logAccum"</span><span class="token punctuation">)</span>
    val <span class="token function">sum</span> <span class="token operator">=</span> sc.parallelize<span class="token punctuation">(</span>Array<span class="token punctuation">(</span><span class="token string">"1"</span>, <span class="token string">"2a"</span>, <span class="token string">"3"</span>, <span class="token string">"4b"</span>, <span class="token string">"5"</span>, <span class="token string">"6"</span>, <span class="token string">"7cd"</span>, <span class="token string">"8"</span>, <span class="token string">"9"</span><span class="token punctuation">)</span>, <span class="token number">2</span><span class="token punctuation">)</span>.filter<span class="token punctuation">(</span>line <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">{<!-- --></span>
      val pattern <span class="token operator">=</span> <span class="token string">""</span>"^-?<span class="token punctuation">(</span><span class="token punctuation">\</span>d+<span class="token punctuation">)</span><span class="token string">""</span>"
      val flag <span class="token operator">=</span> line.matches<span class="token punctuation">(</span>pattern<span class="token punctuation">)</span>
      <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>flag<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        accum.add<span class="token punctuation">(</span>line<span class="token punctuation">)</span>
      <span class="token punctuation">}</span>
      flag
    <span class="token punctuation">}</span><span class="token punctuation">)</span>.map<span class="token punctuation">(</span>_.toInt<span class="token punctuation">)</span>.reduce<span class="token punctuation">(</span>_ + _<span class="token punctuation">)</span>

   println<span class="token punctuation">(</span><span class="token string">"sum: "</span> + <span class="token function">sum</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span>v <span class="token operator">&lt;</span>- accum.value<span class="token punctuation">)</span> print<span class="token punctuation">(</span>v + <span class="token string">""</span><span class="token punctuation">)</span>
    println<span class="token punctuation">(</span><span class="token punctuation">)</span>
    sc.stop<span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>5.2 广播变量（调优策略）<br> 广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个Spark操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，甚至是机器学习算法中的一个很大的特征向量，广播变量用起来都很顺手。 在多个并行操作中使用同一个变量，但是 Spark会为每个任务分别发送。</p> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> val broadcastVar <span class="token operator">=</span> sc.broadcast<span class="token punctuation">(</span>Array<span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">2</span>, <span class="token number">3</span><span class="token punctuation">))</span>
broadcastVar: org.apache.spark.broadcast.Broadcast<span class="token punctuation">[</span>Array<span class="token punctuation">[</span>Int<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> Broadcast<span class="token punctuation">(</span><span class="token number">35</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-bash">scala<span class="token operator">&gt;</span> broadcastVar.value
res33: Array<span class="token punctuation">[</span>Int<span class="token punctuation">]</span> <span class="token operator">=</span> Array<span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">2</span>, <span class="token number">3</span><span class="token punctuation">)</span>
</code></pre> 
<p>使用广播变量的过程如下：<br> (1) 通过对一个类型 T 的对象调用 SparkContext.broadcast 创建出一个 Broadcast[T] 对象。 任何可序列化的类型都可以这么实现。<br> (2) 通过 value 属性访问该对象的值(在 Java 中为 value() 方法)。<br> (3) 变量只会被发到各个节点一次，应作为只读值处理(修改这个值不会影响到别的节点)。</p> 
<p>第6章 扩展<br> <strong>6.1 RDD相关概念关系</strong><br> <img src="https://images2.imgbox.com/5d/f2/ufs55c9q_o.png" alt="在这里插入图片描述"><br> 输入可能以多个文件的形式存储在HDFS上，每个File都包含了很多块，称为Block。当Spark读取这些文件作为输入时，会根据具体数据格式对应的InputFormat进行解析，一般是将若干个Block合并成一个输入分片，称为InputSplit，注意InputSplit不能跨越文件。随后将为这些输入分片生成具体的Task。InputSplit与Task是一一对应的关系。随后这些具体的Task每个都会被分配到集群上的某个节点的某个Executor去执行。</p> 
<p>1)每个节点可以起一个或多个Executor。</p> 
<p>2)每个Executor由若干core组成，每个Executor的每个core一次只能执行一个Task。</p> 
<p>3)每个Task执行的结果就是生成了目标RDD的一个partiton。<br> 注意: 这里的core是虚拟的core而不是机器的物理CPU核，可以理解为就是Executor的一个工作线程。而 Task被执行的并发度 = Executor数目 * 每个Executor核数。至于partition的数目：<br> 1)对于数据读入阶段，例如sc.textFile，输入文件被划分为多少InputSplit就会需要多少初始Task。</p> 
<p>2)在Map阶段partition数目保持不变。</p> 
<p>3)在Reduce阶段，RDD的聚合会触发shuffle操作，聚合后的RDD的partition数目跟具体操作有关，例如repartition操作会聚合成指定分区数，还有一些算子是可配置的。</p> 
<p>RDD在计算的时候，每个分区都会起一个task，所以rdd的分区数目决定了总的的task数目。申请的计算节点（Executor）数目和每个计算节点核数，决定了你同一时刻可以并行执行的task。</p> 
<p>比如的RDD有100个分区，那么计算的时候就会生成100个task，你的资源配置为10个计算节点，每个两2个核，同一时刻可以并行的task数目为20，计算这个RDD就需要5个轮次。如果计算资源不变，你有101个task的话，就需要6个轮次，在最后一轮中，只有一个task在执行，其余核都在空转。如果资源不变，你的RDD只有2个分区，那么同一时刻只有2个task运行，其余18个核空转，造成资源浪费。这就是在spark调优中，增大RDD分区数目，增大任务并行度的做法。</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/12cba96dd804863e786d8270c6089739/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Speedify使用步骤</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7f3860ffdd3ff1bc0664ddad2cd2a726/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">第二讲：网线的制作方法及步骤</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>