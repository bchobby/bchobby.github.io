<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>sknearl-7处理文本数据 - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="sknearl-7处理文本数据" />
<meta property="og:description" content="本章代码大部分没跑，只供学习
第四节特征工程里提到，有连续特征和离散特征，对于文本数据，文本特征可以看作第三种特征
1 用字符串表示的数据类型 2 例子 电影评论情感分析 给定一个影评（输入），输出影评是正面还是负面
sklearn无法处理文本数据，需要将文本数据转换为数值表示，然后再用机器学习算法处理
3 将文本数据表示为词袋 词袋 即统计每个单词出现的频率
词袋构造步骤
1 划分原始字符串：将原始字符串用空格或标点负号分隔，获取单词拼写
2 构建词表，可进行编号
3 统计单词频率
3.1 词袋应用于玩具数据集 通过sklearn.feature_extraction.text.CountVectorizer构造词袋。构造完了可访问.vocabulary_访问词表，然后调用transform获取词袋，看下词表和词袋
def test_workds_bag(self): bards_words = [&#39;the fool doth think he is wise,&#39;, &#39;but the wise man knows himself to be a fool&#39;] vect = CountVectorizer().fit(bards_words) print(f&#39;vocabulary type: {type(vect.vocabulary_)}, vocabulary: {vect.vocabulary_}&#39;) bag_words_trans = vect.transform(bards_words) print(f&#39;words bag transform type:{type(bag_words_trans)},\n&#39; f&#39;words bag transfrom shape:{bag_words_trans.shape},\n&#39; f&#39;words bag transform repr: {repr(bag_words_trans)}\n&#39; f&#39;words bag transform:\n {bag_words_trans." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/7488d080ca8ec309776772f86105cfcd/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-23T17:33:04+08:00" />
<meta property="article:modified_time" content="2023-10-23T17:33:04+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">sknearl-7处理文本数据</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>本章代码大部分没跑，只供学习</p> 
<p>第四节特征工程里提到，有连续特征和离散特征，对于文本数据，文本特征可以看作第三种特征</p> 
<h2>1 用字符串表示的数据类型</h2> 
<h2>2 例子 电影评论情感分析</h2> 
<p>给定一个影评（输入），输出影评是正面还是负面</p> 
<p>sklearn无法处理文本数据，需要将文本数据转换为数值表示，然后再用机器学习算法处理</p> 
<h2>3 将文本数据表示为词袋</h2> 
<p>词袋 即统计每个单词出现的频率</p> 
<p>词袋构造步骤</p> 
<p>1 划分原始字符串：将原始字符串用空格或标点负号分隔，获取单词拼写</p> 
<p>2 构建词表，可进行编号</p> 
<p>3 统计单词频率</p> 
<p><img alt="" height="498" src="https://images2.imgbox.com/20/38/IJ5OCvt9_o.png" width="770"></p> 
<h3 style="background-color:transparent;">3.1 词袋应用于玩具数据集</h3> 
<p>通过sklearn.feature_extraction.text.CountVectorizer构造词袋。构造完了可访问.vocabulary_访问词表，然后调用transform获取词袋，看下词表和词袋</p> 
<pre><code class="language-python">    def test_workds_bag(self):
        bards_words = ['the fool doth think he is wise,', 'but the wise man knows himself to be a fool']
        vect = CountVectorizer().fit(bards_words)
        print(f'vocabulary type: {type(vect.vocabulary_)}, vocabulary: {vect.vocabulary_}')
        bag_words_trans = vect.transform(bards_words)
        print(f'words bag transform type:{type(bag_words_trans)},\n'
              f'words bag transfrom shape:{bag_words_trans.shape},\n'
              f'words bag transform repr: {repr(bag_words_trans)}\n'
              f'words bag transform:\n {bag_words_trans.toarray()}')</code></pre> 
<p><img alt="" height="236" src="https://images2.imgbox.com/0d/9e/oV0ocKah_o.png" width="989"></p> 
<p>注意，vocabulary只是单词排序，字典的值不是单词出现次数，只是在句子里的下标，注意区分词表和词袋的概念。</p> 
<p>词袋用稀疏矩阵表示（sparse matrix）</p> 
<p>CountVectorier默认使用的正则是"\b\w\w+\b"，含义是提起至少两个字符以上的字母数字且被单词边界分开。所以不会提取长度为1的作为单词，所以上述句子提取的词袋也没提取到a</p> 
<h3>3.2 词袋应用于电影数据集</h3> 
<p>先构造词袋，然后用LogisticRegression交叉验证，然后网格搜索最优的C</p> 
<pre><code class="language-python">    def test_movies_bag(self):
        movie_train, movie_test = load_files('train_path'), load_files('test_path')
        text_tr, ytr, text_te, yte = movie_train.data, movie_train.target, movie_test.data, movie_test.target
        text_tr, test_te = [doc.replace(b"&lt;br /&gt;", b"") for doc in text_tr], [doc.replace(b"&lt;br /&gt;", b"") for doc in text_te]
        vect = CountVectorizer().fit(text_tr)
        xtr = vect.transform(text_tr)
        print(f'vect transform features :{vect.get_feature_names()}')  # shape: (25000, n) feature num is n, sort by alphabet

        # cross validation
        print(f'mean logistic regression cross score: {cross_val_score(LogisticRegression(), xtr, ytr, cv=5)}')

        # grid search
        params_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}
        grid = GridSearchCV(LogisticRegression(), params_grid, cv=5).fit(xtr, ytr)
        print(f'grid best score: {grid.best_score_:.3f}')
        print(f'grid best params: {grid.best_params_}')  # C:0.1
        print(f'grid test score: {grid.score(text_te, yte)}')</code></pre> 
<p>词袋数据存在稀疏矩阵，对于高维稀疏矩阵，线性模型的LogisticRegression性能最好，<strong>约为88%</strong></p> 
<p>其实仅靠词袋还有很多问题，动词有进行时，过去时，单三等形式，还可能有很多写错的字符，这些需要考虑影响程度。可考虑在词袋基础做单词改进（不识别大小写，即大小写不同的单词会被识别为同一单词）</p> 
<p>考虑CountVectorizer提取单词原理，使用"\b\w\w+\b"，对于doesn't，bilibili.txt这类单词，会拆开识别</p> 
<h4>方案1 仅考虑在两个以上的文档中出现的相同单词</h4> 
<p>通过CountVectorizer的min_df参数实现（仅出现一次的单词可能没什么用，先这么试试） </p> 
<pre><code class="language-python">    def get_movie_data_test(self):
        movie_train, movie_test = load_files('train_path'), load_files('test_path')
        text_tr, ytr, text_te, yte = movie_train.data, movie_train.target, movie_test.data, movie_test.target
        text_tr, test_te = [doc.replace(b"&lt;br /&gt;", b"") for doc in text_tr], [doc.replace(b"&lt;br /&gt;", b"") for doc in
                                                                              text_te]
        return text_tr, test_te, ytr, yte
    def test_movies_bag_5_appear(self):
        xtr, xte, ytr, yte = self.get_movie_data_test()
        vect = CountVectorizer(min_df=5).fit(xtr)
        xtr_trans = vect.transform(xtr)
        grid = GridSearchCV(LogisticRegression(), {'C': [0.001, 0.01, 0.1, 1, 10]}, cv=5).fit(xtr, ytr)
        print(f'logistic regression best score: {grid.score(xte, yte)}')</code></pre> 
<p>结论 精度大概为<strong>89%</strong>，发现处理单词出现频率后，精度没明显的提升，但减少了约三分之二的特征，可提升处理速度</p> 
<h2>4 停用词</h2> 
<p>删除没有意义的词语还有一种方法：删除出现频率过高的词语。有两种方法：1使用特定的语言停用词词表（sklearn.feature_extraction.text.ENGLISH_STOP_WORDS提供了停用词词表） 2指定特定频率，舍弃频率在该频率以上的词语。比如说above， into， well， anyone等词</p> 
<p>可以从数据集里删除停用词。虽然减少不了多少特征，但可能会提升性能，因为停用词出现频率可能高一些</p> 
<h2>5 tf-idf放缩数据</h2> 
<p>tf-idf概念 也叫词频-逆向文档频率（term frequency - inverse document frequency， tf-idf）。给词语赋予权重，对于语料库中经常出现的词语，不会赋予很高权重；在某个文档出现频率次数较高的词被识别为术语，赋予较高的权重。最后通过一个量化指标 tf-idf分数来反映单词权重。sklearn里有两个类实现了tf-idf：TfidfTransformer和TfidfVectorizer，前者接受稀疏矩阵并转换，后者接受文本数据完成词袋特征提取和tfidf变换，计算公式如下</p> 
<p style="text-align:center;"><img alt="" height="65" src="https://images2.imgbox.com/e2/a5/wiux9aLm_o.png" width="266"></p> 
<p>N是文档总数量，Nw是出现某个单词的文档数量，tf是单词在查询文档中出现的次数</p> 
<p>可以看下tfidf得分最高和最低的单词</p> 
<p>tfidf得分较小时，说明单词要么出现频率很低，要么就是在很多文档里都有使用</p> 
<p>tfidf得分较大时，说明词汇较高频率出现在某些文档中。但这类词语有的对影评情感分类并没有显著的作用，比如电影标题</p> 
<p>看下idf得分最低的单词（<strong>出现频率最高，只按频率排序，idf和tfidf不一样</strong>）</p> 
<p>这些单词主要是停用词</p> 
<pre><code class="language-python">    def test_tfidf_show_features(self):
        xtr, xte, ytr, yte = self.get_movie_data_test()
        pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())
        grid = GridSearchCV(pipe, {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}, cv=5).fit(xtr, ytr)
        print(f'best cross score: {grid.best_score_}')

        # show tfidf words
        vectorizer = grid.best_estimator_.named_steps["tfidfvectorizer"]
        xtr_trans = vectorizer.transform(xtr)
        max_val = xtr_trans.max(axis=0).toarray().ravel()
        sorted_by_tfidf = max_val.argsort()
        feature_name = np.array(vectorizer.get_feature_names())
        # show tf-idf score
        print(f'tfidf score lowest 20 ea: {feature_name[:20]}')
        print(f'tfidf score highest 20 ea: {feature_name[-20:]}')
        # show idf score
        print(f' idf score lowest 20 ea: {vectorizer.idf_[:20]}')</code></pre> 
<h2>6 研究模型系数</h2> 
<p>看下训练的logistic模型系数的最大最小值</p> 
<pre><code class="language-python">        mglearn.tools.visualize_coefficients(grid.best_estimator_.named_steps["logisticregression"].coef_,
                                             feature_names=feature_names, n_top_features=40)</code></pre> 
<p><img alt="" height="335" src="https://images2.imgbox.com/dc/23/2yAHJdQY_o.png" width="899"></p> 
<p>看x轴发现最小得分的单词大多是负面情绪的单词，比如worst，waste等，得分高的单词大部分也是正面单词：great， excellent等</p> 
<h2>7 多个单词词袋</h2> 
<p>词袋缺点 舍弃了单词顺序</p> 
<p>词袋解决方案 有一种词袋考虑上下文中单词的计数，即某个单词相邻某几个单词的计数</p> 
<p>二元分词 两个词例，以此类推三元等，词例范围可通过vector类的ngram_range参数传入来指定词例个数。ngram_range是一个元组，包括了词例的最小长度和最大长度。CountVectorizer默认是(1,1)的ngram_range</p> 
<pre><code class="language-python">    def test_word_bag_ngram(self):
        bards_words = ['the fool doth think he is wise,', 'but the wise man knows himself to be a fool']
        vector = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)
        print(f'length of feature: {len(vector.vocabulary_)},\nvector feature names: {vector.get_feature_names_out()}')</code></pre> 
<p><img alt="" height="78" src="https://images2.imgbox.com/9d/ab/zkcz1LM1_o.png" width="785"></p> 
<p>仅查看二元分词</p> 
<pre><code class="language-python">    def test_word_bag_ngram(self):
        bards_words = ['the fool doth think he is wise,', 'but the wise man knows himself to be a fool']
        vector = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)
        print(f'length of feature: {len(vector.vocabulary_)},\nvector feature names: {vector.get_feature_names_out()}')
        # show 2 dimension words
        vector = CountVectorizer(ngram_range=(2, 2)).fit(bards_words)
        print(f'feature len: {len(vector.vocabulary_)},\n2 di vector feature names: {vector.get_feature_names_out()}')</code></pre> 
<p><img alt="" height="125" src="https://images2.imgbox.com/37/41/GCRX2eI3_o.png" width="824"></p> 
<p>优缺点 多元分词可能导致过拟合，也会增加计算量，n元分词计算量是一元分词的n倍</p> 
<p>可以同时使用一元，二元，三元分词</p> 
<pre><code class="language-python">        vector = CountVectorizer(ngram_range=(1, 3)).fit(bards_words)
        print(f'feature len: {len(vector.vocabulary_)},\n vector feature names: {vector.get_feature_names_out()}')</code></pre> 
<p><img alt="" height="238" src="https://images2.imgbox.com/ab/51/JpretIox_o.png" width="753"></p> 
<h3>7.1 对影评数据应用3元词袋</h3> 
<p>对影评数据应用1-3元词袋，然后网格搜索出最佳参数，然后热图可视化（没跑，用的教材的图）</p> 
<p><img alt="" height="483" src="https://images2.imgbox.com/7f/bd/dtLbxkQ9_o.png" width="722"></p> 
<p>二元的精度提升了约一个百分点，发现一元到二元精度提升很多，二元到三元没提升多少，表明三元可能没太大作用，</p> 
<p>看下特征系数，绘制bar图</p> 
<p>发现三元特征的特征系数普遍较低，也验证了三元分词没起太多作用</p> 
<h2>8 高级分词、词干提取、词形还原</h2> 
<p>目的 很多单词有不同分词形式，将分词形式作为单独特征可能会导致过拟合，将词干提取或合并可减少次问题导致的误差</p> 
<p>词干提取（stemming） 删除单词不同分词形式的通用分词后缀，然后合并词干</p> 
<p>词形还原（lemmatization） 将单词不同分词形式按照已有分词字典进行合并还原</p> 
<p>标准化 词干提取和词形还原都叫标准化，即将一个单词还原成标准形式</p> 
<p>先看下词干提取</p> 
<pre><code class="language-python">    def test_word_stem(self):
        en_nlp = spacy.load('en_core_web_sm')
        stemmer = nltk.stem.PorterStemmer()

        def compare_normalization(doc):
            doc_spacy = en_nlp(doc)
            print(f'show word split result: {[token.lemma_ for token in doc_spacy]}')
            print(f'show word stem found result: {[stemmer.stem(token.norm_.lower()) for token in doc_spacy]}')

        test_text = "our meeting today was worse than yesterday, I'm scared of meeting the clients tomorrow"
        compare_normalization(test_text)</code></pre> 
<p><img alt="" height="89" src="https://images2.imgbox.com/be/11/DJQBz0dM_o.png" width="1104"></p> 
<p>was词干提取后变成wa，因为词干提取原理是删分词后缀</p> 
<p>worse变成wors，meeting变成meet</p> 
<p>sklearn里没支持词干提取和词形还原，单CountVectorizer可以使用tokenizer指定分词器将文档转换为词例列表</p> 
<p>看下词形还原</p> 
<pre><code class="language-python">    def test_lemmatization(self):
        regexp = re.compile('(?u)\\b\\w\\w+\\b')
        en_nlp = spacy.load('en_core_web_sm')
        old_tokenizer = en_nlp.tokenizer
        en_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(regexp.findall(string))
        def custom_tokenizer(doc):
            doc_spacy = en_nlp(doc, entity=False, parse=False)
            return [token.lemma_ for token in doc_spacy]

        lemma_vect = CountVectorizer(tokenizer=custom_tokenizer, min_df=5)
        xtr, xte, ytr, yte = self.get_movie_data_test()
        xtr_lemma = lemma_vect.fit_transform(xtr)
        print(f'words lemmatization shape: {xtr_lemma.shape}')</code></pre> 
<p>词形还原可以合并特征，可以看作正则化，因为选的特征变少了。数据集比较小时，词形还原可以有较大的性能提升。</p> 
<h2>9 主题建模与文档归类</h2> 
<p>另一种常用的文本建模方法是<strong>主题建模</strong>，比如每个新闻都涉及一些主题，比如财经，体育，科技等。给一个新闻预测是哪个主题，是主题建模要考虑的问题。一般主题建模指<strong>隐含狄利克雷分布（Latent Dirichlet Allocation，LDA</strong>）的分解方法</p> 
<p>机器学习学习到的主题，和我们日常提到的主题可能不太一样。机器学习可能按词频学到词频较高的词语作为主题，类似于PCA的主成分，没有什么让人直观理解的含义，只是个计算量。</p> 
<p>预处理 应用LDA前应删掉常见的频率很高的非主题词，可以在CountVctorizer构造传入参数min_df=.3，表示删除至少在30%文档出现的词语</p> 
<p>任务 现在设置学习目标是10个主题。主题类似于NMF中的分量，没有内在的顺序，但改变主题数量会改变所有主题（其实LDA和NMF有一定相似性，也可试着用NMF提取主题）。此处用batch学习方法，比online方法稍慢，但结果可能会更好，然后增大max_iter，可以得到更好的模型</p> 
<h3>9.1 模型1</h3> 
<pre><code class="language-python">    def test_topic_modeling(self):
        vect = CountVectorizer(max_features=10000, max_df=.15)
        x = vect.fit_transform('test train')
        lda = LatentDirichletAllocation(n_topics=10, learning_method='batch', max_iter=25, random_state=0)
        topics = lda.fit_transform(x)
        print(f'lda component shape: {lda.components_.shape}')  # (10, 10000)</code></pre> 
<p>查看每个主题最重要的词语</p> 
<p><img alt="" height="451" src="https://images2.imgbox.com/2d/bd/NoDDYHt0_o.png" width="502"></p> 
<p>从词汇重要程度看，topic1可能和战争有关，主题2可能和喜剧有关，主题3可能和电视连续剧有关</p> 
<h3>9.2 模型2</h3> 
<pre><code class="language-python">        lda100 = LatentDirichletAllocation(n_topics=100, learning_method='batch', max_iter=25, random_state=0)
        topics_100 = lda100.fit_transform(x)
        # randomly select several topics
        topics_sample = np.array([11, 21, 31, 41, 51])
        sorting_100 = np.argsort(lda100.components_, axis=1)[:, ::-1]
        feature_names_100 = np.array(vect.get_feature_names_out())
        mglearn.tools.print_topics(topics=topics_sample, feature_names=feature_names_100,
                                   sorting=sorting_100, topic_per_chunk=7, n_words=20)</code></pre> 
<h3>9.3 汇总每个文档主题重要性</h3> 
<p>还有一种量化指标是将所有文档的主题重要性汇总，然后按会总量从大到小可视化，或者按topic可视化</p> 
<h3>9.4 优缺点</h3> 
<p>主题建模是无监督学习，最好有已知标签进行进一步验证。学习结果和random_state挂钩</p> 
<h2>10 小结</h2> 
<p>讨论了CountVectorizer和TfidfVectorizer，是相对简单的方法，其他高级的方法可使用py的包spacy（相对较新，较高效，设计良好），nltk（很完整的库，有些过时），gensim（着重于主题建模的nlp包）</p> 
<p>研究方向</p> 
<p>1 使用连续向量表示。</p> 
<p>2 递归神经网络（RNN）。很适合自动翻译和摘要，</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f40b994dcb52bf69b8a19ff09634c933/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">CSP认证 202212-01 现值计算 满分题解</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7a1a0fc0bd8cb5a36da60eca393c98cf/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">sklearn-5模型评估与改进</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>