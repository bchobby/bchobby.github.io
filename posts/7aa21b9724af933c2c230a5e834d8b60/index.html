<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>7.kafka&#43;ELK连接 - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="7.kafka&#43;ELK连接" />
<meta property="og:description" content="文章目录 kafka&#43;ELK连接部署Kafkakafka操作命令kafka架构深入Filebeat&#43;Kafka&#43;ELK连接 kafka&#43;ELK连接 部署Kafka ###关闭防火墙 systemctl stop firewalld systemctl disable firewalld setenforce 0 vim /etc/selinux/config SELINUX=disabled ###下载安装包 官方下载地址：http://kafka.apache.org/downloads.html cd /opt wget https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.7.1/kafka_2.13-2.7.1.tgz ###安装 Kafka cd /opt/ tar xf kafka_2.13-2.7.1.tgz mv kafka_2.13-2.7.1 /usr/local/kafka ###修改配置文件 ##备份配置文件 cd /usr/local/kafka/config/ cp server.properties{,.bak} vim server.properties ---21行-- broker.id=0 ###21行，broker的全局唯一编号，每个broker不能重复，因此要在其他机器上配置 broker.id=1、broker.id=2 ---31行--- listeners=PLAINTEXT://192.168.242.70:9092 ###31行，指定监听的IP和端口，如果修改每个broker的IP需区分开来，也可保持默认配置不用修改 ---42行--- num.network.threads=3 ###42行，broker 处理网络请求的线程数量，一般情况下不需要去修改 ---45行--- num.io.threads=8 #45行，用来处理磁盘IO的线程数量，数值应该大于硬盘数 ---48行--- socket.send.buffer.bytes=102400 #48行，发送套接字的缓冲区大小 ---51行--- socket.receive.buffer.bytes=102400 #51行，接收套接字的缓冲区大小 ---54行--- socket.request.max.bytes=104857600 #54行，请求套接字的缓冲区大小 ---60行--- log.dirs=/usr/local/kafka/logs #60行，kafka运行日志存放的路径，也是数据存放的路径 ---65行--- num.partitions=1 #65行，topic在当前broker上的默认分区个数，会被topic创建时的指定参数覆盖 ---69行--- num." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/7aa21b9724af933c2c230a5e834d8b60/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-13T14:53:46+08:00" />
<meta property="article:modified_time" content="2023-07-13T14:53:46+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">7.kafka&#43;ELK连接</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#kafkaELK_1" rel="nofollow">kafka+ELK连接</a></li><li><ul><li><a href="#Kafka_3" rel="nofollow">部署Kafka</a></li><li><a href="#kafka_172" rel="nofollow">kafka操作命令</a></li><li><a href="#kafka_230" rel="nofollow">kafka架构深入</a></li><li><a href="#FilebeatKafkaELK_284" rel="nofollow">Filebeat+Kafka+ELK连接</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="kafkaELK_1"></a>kafka+ELK连接</h2> 
<h3><a id="Kafka_3"></a>部署Kafka</h3> 
<pre><code>###关闭防火墙

systemctl stop firewalld
systemctl disable firewalld

setenforce 0

vim /etc/selinux/config

SELINUX=disabled
</code></pre> 
<pre><code>###下载安装包

官方下载地址：http://kafka.apache.org/downloads.html

cd /opt
wget https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.7.1/kafka_2.13-2.7.1.tgz
</code></pre> 
<pre><code>###安装 Kafka

cd /opt/
tar xf kafka_2.13-2.7.1.tgz
mv kafka_2.13-2.7.1 /usr/local/kafka
</code></pre> 
<pre><code>###修改配置文件

##备份配置文件
cd /usr/local/kafka/config/
cp server.properties{,.bak}

vim server.properties

---21行--
broker.id=0    
###21行，broker的全局唯一编号，每个broker不能重复，因此要在其他机器上配置 broker.id=1、broker.id=2

---31行---
listeners=PLAINTEXT://192.168.242.70:9092    
###31行，指定监听的IP和端口，如果修改每个broker的IP需区分开来，也可保持默认配置不用修改

---42行---
num.network.threads=3    
###42行，broker 处理网络请求的线程数量，一般情况下不需要去修改

---45行---
num.io.threads=8            
#45行，用来处理磁盘IO的线程数量，数值应该大于硬盘数

---48行---
socket.send.buffer.bytes=102400       
#48行，发送套接字的缓冲区大小

---51行---
socket.receive.buffer.bytes=102400    
#51行，接收套接字的缓冲区大小

---54行---
socket.request.max.bytes=104857600    
#54行，请求套接字的缓冲区大小

---60行---
log.dirs=/usr/local/kafka/logs        
#60行，kafka运行日志存放的路径，也是数据存放的路径

---65行---
num.partitions=1    
#65行，topic在当前broker上的默认分区个数，会被topic创建时的指定参数覆盖

---69行---
num.recovery.threads.per.data.dir=1    
#69行，用来恢复和清理data下数据的线程数量

---103行---
log.retention.hours=168    
#103行，segment文件（数据文件）保留的最长时间，单位为小时，默认为7天，超时将被删除

---110行---
log.segment.bytes=1073741824    
#110行，一个segment文件最大的大小，默认为 1G，超出将新建一个新的segment文件

---123行---
zookeeper.connect=192.168.242.70:2181,192.168.242.71:2181,192.168.242.72:2181    
###123行，配置连接Zookeeper集群地址
</code></pre> 
<pre><code>###修改环境变量

vim /etc/profile
export KAFKA_HOME=/usr/local/kafka
export PATH=$PATH:$KAFKA_HOME/bin

###加载配置
source /etc/profile


###复制文件到zookeeper服务器哦

scp -r /usr/local/kafka 192.168.242.71:/usr/local
scp -r /usr/local/kafka 192.168.242.72:/usr/local
</code></pre> 
<pre><code class="prism language-shell"><span class="token comment">###部署kafka启动脚本</span>

<span class="token function">vim</span> /etc/init.d/kafka


<span class="token comment">#!/bin/bash</span>
<span class="token comment">#chkconfig:2345 22 88</span>
<span class="token comment">#description:Kafka Service Control Script</span>
<span class="token assign-left variable">KAFKA_HOME</span><span class="token operator">=</span><span class="token string">'/usr/local/kafka'</span>
<span class="token keyword">case</span> <span class="token variable">$1</span> <span class="token keyword">in</span>
start<span class="token punctuation">)</span>
	<span class="token builtin class-name">echo</span> <span class="token string">"---------- Kafka 启动 ------------"</span>
	<span class="token variable">${KAFKA_HOME}</span>/bin/kafka-server-start.sh <span class="token parameter variable">-daemon</span> <span class="token variable">${KAFKA_HOME}</span>/config/server.properties
<span class="token punctuation">;</span><span class="token punctuation">;</span>
stop<span class="token punctuation">)</span>
	<span class="token builtin class-name">echo</span> <span class="token string">"---------- Kafka 停止 ------------"</span>
	<span class="token variable">${KAFKA_HOME}</span>/bin/kafka-server-stop.sh
<span class="token punctuation">;</span><span class="token punctuation">;</span>
restart<span class="token punctuation">)</span>
	<span class="token variable">$0</span> stop
	<span class="token variable">$0</span> start
<span class="token punctuation">;</span><span class="token punctuation">;</span>
status<span class="token punctuation">)</span>
	<span class="token builtin class-name">echo</span> <span class="token string">"---------- Kafka 状态 ------------"</span>
	<span class="token assign-left variable">count</span><span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span><span class="token function">ps</span> <span class="token parameter variable">-ef</span> <span class="token operator">|</span> <span class="token function">grep</span> kafka <span class="token operator">|</span> <span class="token function">egrep</span> <span class="token parameter variable">-cv</span> <span class="token string">"grep|<span class="token variable">$$</span>"</span><span class="token variable">)</span></span>
	<span class="token keyword">if</span> <span class="token punctuation">[</span> <span class="token string">"<span class="token variable">$count</span>"</span> <span class="token parameter variable">-eq</span> <span class="token number">0</span> <span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token keyword">then</span>
        <span class="token builtin class-name">echo</span> <span class="token string">"kafka is not running"</span>
    <span class="token keyword">else</span>
        <span class="token builtin class-name">echo</span> <span class="token string">"kafka is running"</span>
    <span class="token keyword">fi</span>
<span class="token punctuation">;</span><span class="token punctuation">;</span>
*<span class="token punctuation">)</span>
    <span class="token builtin class-name">echo</span> <span class="token string">"Usage: <span class="token variable">$0</span> {start|stop|restart|status}"</span>
<span class="token keyword">esac</span>
</code></pre> 
<pre><code>###设置开机自启

cd /etc/init.d/
chmod +x /etc/init.d/kafka
chkconfig --add kafka

###分别启动 Kafka
service kafka start


###另外一种启动方式

cd /usr/local/kafka/bin

./kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties

netstat -lntp | grep 9092
</code></pre> 
<p><img src="https://images2.imgbox.com/63/6e/Guv0DJVD_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="kafka_172"></a>kafka操作命令</h3> 
<pre><code>####创建topic

kafka-topics.sh --create --zookeeper 192.168.242.70:2181,192.168.242.71:2181,192.168.242.72:2181 --replication-factor 2 --partitions 3 --topic test



--zookeeper：定义 zookeeper 集群服务器地址，如果有多个 IP 地址使用逗号分割，一般使用一个 IP 即可
--replication-factor：定义分区副本数，1 代表单副本，建议为 2 
--partitions：定义分区数 
--topic：定义 topic 名称
</code></pre> 
<p><img src="https://images2.imgbox.com/2f/61/GBsfFQJ8_o.png" alt="在这里插入图片描述"></p> 
<pre><code>###查看当前服务器中的所有 topic

kafka-topics.sh --list --zookeeper 192.168.242.70:2181,192.168.242.71:2181,192.168.242.72:2181 
</code></pre> 
<pre><code>###查看某个 topic 的详情

kafka-topics.sh  --describe --zookeeper 192.168.242.70:2181,192.168.242.71:2181,192.168.242.72:2181 
</code></pre> 
<p><img src="https://images2.imgbox.com/be/82/9VrQ6MxW_o.png" alt="在这里插入图片描述"></p> 
<pre><code>###发布消息

kafka-console-producer.sh --broker-list 192.168.242.70:9092,192.168.242.71:9092,192.168.242.72:9092  --topic test
</code></pre> 
<pre><code>###消费消息

kafka-console-consumer.sh --bootstrap-server 192.168.242.70:9092,192.168.242.71:9092,192.168.242.72:9092 --topic test --from-beginning

--from-beginning：会把主题中以往所有的数据都读取出来
</code></pre> 
<p><img src="https://images2.imgbox.com/1e/6c/YwlheNkr_o.png" alt="在这里插入图片描述"></p> 
<pre><code>###修改分区数

kafka-topics.sh --zookeeper 192.168.242.70:2181,192.168.242.71:2181,192.168.242.72:2181  --alter --topic test --partitions 6
</code></pre> 
<p><img src="https://images2.imgbox.com/33/60/UmWh9YVd_o.png" alt="在这里插入图片描述"></p> 
<pre><code>###删除 topic

kafka-topics.sh --delete --zookeeper 192.168.242.70:2181,192.168.242.71:2181,192.168.242.72:2181 --topic test
</code></pre> 
<p><img src="https://images2.imgbox.com/4e/f7/mZhVqwMg_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="kafka_230"></a>kafka架构深入</h3> 
<ul><li> <p>Kafka 工作流程及文件存储机制</p> 
  <ul><li>Kafka 中消息是以 topic 进行分类的，生产者生产消息，消费者消费消息，都是面向 topic 的。</li></ul> </li><li> <p>topic 是逻辑上的概念，而 partition 是物理上的概念，每个 partition 对应于一个 log 文件，该 log 文件中存储的就是 producer 生产的数据。</p> </li><li> <p>Producer 生产的数据会被不断追加到该 log 文件末端，且每条数据都有自己的 offset。</p> </li><li> <p>消费者组中的每个消费者，都会实时记录自己消费到了哪个 offset，以便出错恢复时，从上次的位置继续消费。</p> </li><li> <p>由于生产者生产的消息会不断追加到 log 文件末尾，为防止 log 文件过大导致数据定位效率低下，Kafka 采取了分片和索引机制，将每个 partition 分为多个 segment。</p> </li><li> <p>每个 segment 对应两个文件：“.index” 文件和 “.log” 文件。</p> </li><li> <p>这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号。例如，test 这个 topic 有三个分区， 则其对应的文件夹为 test-0、test-1、test-2。</p> </li><li> <p>index 和 log 文件以当前 segment 的第一条消息的 offset 命名。</p> 
  <ul><li>“.index” 文件存储大量的索引信息，</li><li>“.log” 文件存储大量的数据，</li><li>索引文件中的元数据指向对应数据文件中 message 的物理偏移地址。</li></ul> </li><li> <p>数据可靠性保证</p> 
  <ul><li>为保证 producer 发送的数据，能可靠的发送到指定的 topic，topic 的每个 partition 收到 producer 发送的数据后， 都需要向 producer 发送 ack（acknowledgement 确认收到），如果 producer 收到 ack，就会进行下一轮的发送，否则重新发送数据。</li></ul> </li><li> <p>数据一致性问题</p> 
  <ul><li>LEO：指的是每个副本最大的 offset；</li><li>HW：指的是消费者能见到的最大的 offset，所有副本中最小的 LEO。</li></ul> </li><li> <p>follower 故障</p> 
  <ul><li>follower 发生故障后会被临时踢出 ISR（Leader 维护的一个和 Leader 保持同步的 Follower 集合），待该 follower 恢复后，follower 会读取本地磁盘记录的上次的 HW，并将 log 文件高于 HW 的部分截取掉，从 HW 开始向 leader 进行同步。等该 follower 的 LEO 大于等于该 Partition 的 HW，即 follower 追上 leader 之后，就可以重新加入 ISR 了。</li></ul> </li><li> <p>leader 故障</p> 
  <ul><li>leader 发生故障之后，会从 ISR 中选出一个新的 leader， 之后，为保证多个副本之间的数据一致性，其余的 follower 会先将各自的 log 文件高于 HW 的部分截掉，然后从新的 leader 同步数据。</li></ul> </li><li> <p>注：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。</p> </li><li> <p>ack 应答机制</p> 
  <ul><li>对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等 ISR 中的 follower 全部接收成功。</li><li>所以 Kafka 为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡选择。</li></ul> </li><li> <p>当 producer 向 leader 发送数据时，可以通过 request.required.acks 参数来设置数据可靠性的级别：</p> 
  <ul><li>0：这意味着producer无需等待来自broker的确认而继续发送下一批消息。这种情况下数据传输效率最高，但是数据可靠性确是最低的。当broker故障时有可能丢失数据。</li><li>1（默认配置）：这意味着producer在ISR中的leader已成功收到的数据并得到确认后发送下一条message。如果在follower同步成功之前leader故障，那么将会丢失数据。</li><li>-1（或者是all）：producer需要等待ISR中的所有follower都确认接收到数据后才算一次发送完成，可靠性最高。但是如果在 follower 同步完成后，broker 发送ack 之前，leader 发生故障，那么会造成数据重复。</li></ul> </li><li> <p>三种机制性能依次递减，数据可靠性依次递增。</p> </li></ul> 
<p>注：在 0.11 版本以前的Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。在 0.11 及以后版本的 Kafka，引入了一项重大特性：幂等性。所谓的幂等性就是指 Producer 不论向 Server 发送多少次重复数据， Server 端都只会持久化一条。</p> 
<p><mark>+ kafka会通过ack机制保证数据的可靠性</mark></p> 
<ul><li>ack配置参数有</li><li>0（效果类似异步复制）：不等待follower同步完成就让生产者发送下一条消息</li><li>1（效果类似半同步复制）：至少等待一个follower同步完成才让生产者发送下一条消息</li><li>-1（效果类似全同步复制）：等待所有follower同步完成才让生产者发送下一条消息</li></ul> 
<h3><a id="FilebeatKafkaELK_284"></a>Filebeat+Kafka+ELK连接</h3> 
<p><img src="https://images2.imgbox.com/df/a8/UefRj9F2_o.png" alt="在这里插入图片描述"></p> 
<pre><code>###部署 Zookeeper+Kafka 集群

###部署 Filebeat，修改配置文件 

cd /usr/local/filebeat

vim filebeat.yml


filebeat.prospectors:
- type: log
  enabled: true
  paths:
    - /var/log/httpd/access_log
  tags: ["access"]


- type: log
  enabled: true
  paths:
    - /var/log/httpd/error_log
  tags: ["error"]
  
......


#添加输出到 Kafka 的配置

output.kafka:
  enabled: true
  hosts: ["192.168.242.70:9092","192.168.242.71:9092","192.168.242.72:9092"]    #指定 Kafka 集群配置
  topic: "httpd"    #指定 Kafka 的 topic
  
</code></pre> 
<p><img src="https://images2.imgbox.com/c6/06/pp21aijv_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/57/fa/kn1T4p4d_o.png" alt="在这里插入图片描述"></p> 
<pre><code>###启动 filebeat

./filebeat -e -c filebeat.yml
</code></pre> 
<p><img src="https://images2.imgbox.com/72/db/U3SEhrYX_o.png" alt="在这里插入图片描述"></p> 
<pre><code>###部署 ELK，在 Logstash 组件所在节点上新建一个 Logstash 配置文件

cd /etc/logstash/conf.d/

vim kafka.conf

input {
    kafka {
        bootstrap_servers =&gt; "192.168.242.70:9092,192.168.242.71:9092,192.168.242.72:9092"  
    #kafka集群地址
    
        topics  =&gt; "httpd"     #拉取的kafka的指定topic
        type =&gt; "httpd_kafka"  #指定 type 字段
        codec =&gt; "json"        #解析json格式的日志数据
        auto_offset_reset =&gt; "latest"  #拉取最近数据，earliest为从头开始拉取
        decorate_events =&gt; true   #传递给elasticsearch的数据额外增加kafka的属性数据
    }
}

output {
  if "access" in [tags] {
    elasticsearch {
      hosts =&gt; ["192.168.242.66:9200"]
      index =&gt; "httpd_access-%{+YYYY.MM.dd}"
    }
  }
  
  if "error" in [tags] {
    elasticsearch {
      hosts =&gt; ["192.168.242.66:9200"]
      index =&gt; "httpd_error-%{+YYYY.MM.dd}"
    }
  }
  
  stdout { codec =&gt; rubydebug }
}

</code></pre> 
<pre><code>####启动 logstash

logstash -f kafka.conf
</code></pre> 
<p><img src="https://images2.imgbox.com/db/77/RrV5Tosq_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/04/56/OuVv3cXJ_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b2608f3b89d0fe8291716682498ee6e8/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">C# SolidWorks 二次开发 -从零开始创建一个插件(1)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8ba70bce5c68f48e94ce1c8e2b789408/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">DataWay 分页查询</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>