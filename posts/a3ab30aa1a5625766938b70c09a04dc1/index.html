<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>“RDD、DataFrame、DataSet的概念、区别联系、相互转换操作” - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="“RDD、DataFrame、DataSet的概念、区别联系、相互转换操作”" />
<meta property="og:description" content="提示：文章内容只供参考！
目录
一、RDD的概念
二、DataFrame 是什么
三、DataSet 是什么
四、创建 DataFrame
五、RDD 转换为 DataFrame
六、DataFrame 转换为 RDD
七、创建 DataSet
八、RDD 转换为 DataSet
九、DataSet 转换为 RDD
十、DataFrame 转换为 DataSet
十一、DataSet 转换为 DataFrame
十二、RDD、DataFrame、DataSet 三者的关系
三者的共性
三者的区别
一、RDD的概念 RDD 是Spark的核心抽象，即 弹性分布式数据集（residenta distributed dataset）。代表一个不可变，可分区，里面元素可并行计算的集合。其具有数据流模型的特点：自动容错，位置感知性调度和可伸缩性。
在Spark中，对数据的所有操作不外乎创建RDD、转化已有RDD以及调用 RDD操作进行求值。
二、DataFrame 是什么 在 Spark 中，DataFrame 是一种以 RDD 为基础的分布式数据集，类似于传统数据库中 的二维表格。DataFrame 与 RDD 的主要区别在于，前者带有 schema 元信息，即 DataFrame 所表示的二维表数据集的每一列都带有名称和类型。这使得 Spark SQL 得以洞察更多的结构信息，从而对藏于 DataFrame 背后的数据源以及作用于 DataFrame 之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观 RDD，由于无从得知所存数据元素的具体内部结构，Spark Core 只能在 stage 层面进行简单、通用的流水线优化。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/a3ab30aa1a5625766938b70c09a04dc1/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-12-09T10:24:21+08:00" />
<meta property="article:modified_time" content="2022-12-09T10:24:21+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">“RDD、DataFrame、DataSet的概念、区别联系、相互转换操作”</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <blockquote> 
 <p>提示：文章内容只供参考！</p> 
</blockquote> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81RDD%E7%9A%84%E6%A6%82%E5%BF%B5-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81RDD%E7%9A%84%E6%A6%82%E5%BF%B5" rel="nofollow">一、RDD的概念</a></p> 
<p id="%E4%BA%8C%E3%80%81DataFrame%20%E6%98%AF%E4%BB%80%E4%B9%88-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81DataFrame%20%E6%98%AF%E4%BB%80%E4%B9%88" rel="nofollow">二、DataFrame 是什么</a></p> 
<p id="%E4%B8%89%E3%80%81DataSet%20%E6%98%AF%E4%BB%80%E4%B9%88-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81DataSet%20%E6%98%AF%E4%BB%80%E4%B9%88" rel="nofollow">三、DataSet 是什么</a></p> 
<p id="%E5%9B%9B%E3%80%81%E5%88%9B%E5%BB%BA%20DataFrame-toc" style="margin-left:0px;"><a href="#%E5%9B%9B%E3%80%81%E5%88%9B%E5%BB%BA%20DataFrame" rel="nofollow">四、创建 DataFrame</a></p> 
<p id="%E4%BA%94%E3%80%81RDD%20%E8%BD%AC%E6%8D%A2%E4%B8%BA%20DataFrame-toc" style="margin-left:0px;"><a href="#%E4%BA%94%E3%80%81RDD%20%E8%BD%AC%E6%8D%A2%E4%B8%BA%20DataFrame" rel="nofollow">五、RDD 转换为 DataFrame</a></p> 
<p id="%C2%A0%E5%85%AD%E3%80%81DataFrame%20%E8%BD%AC%E6%8D%A2%E4%B8%BA%20RDD-toc" style="margin-left:0px;"><a href="#%C2%A0%E5%85%AD%E3%80%81DataFrame%20%E8%BD%AC%E6%8D%A2%E4%B8%BA%20RDD" rel="nofollow"> 六、DataFrame 转换为 RDD</a></p> 
<p id="%E4%B8%83%E3%80%81%E5%88%9B%E5%BB%BA%20DataSet-toc" style="margin-left:0px;"><a href="#%E4%B8%83%E3%80%81%E5%88%9B%E5%BB%BA%20DataSet" rel="nofollow">七、创建 DataSet</a></p> 
<p id="%E5%85%AB%E3%80%81RDD%20%E8%BD%AC%E6%8D%A2%E4%B8%BA%20DataSet-toc" style="margin-left:0px;"><a href="#%E5%85%AB%E3%80%81RDD%20%E8%BD%AC%E6%8D%A2%E4%B8%BA%20DataSet" rel="nofollow">八、RDD 转换为 DataSet</a></p> 
<p id="%C2%A0%E4%B9%9D%E3%80%81DataSet%20%E8%BD%AC%E6%8D%A2%E4%B8%BA%20RDD-toc" style="margin-left:0px;"><a href="#%C2%A0%E4%B9%9D%E3%80%81DataSet%20%E8%BD%AC%E6%8D%A2%E4%B8%BA%20RDD" rel="nofollow"> 九、DataSet 转换为 RDD</a></p> 
<p id="%C2%A0%E5%8D%81%E3%80%81DataFrame%20%E8%BD%AC%E6%8D%A2%E4%B8%BA%20DataSet-toc" style="margin-left:0px;"><a href="#%C2%A0%E5%8D%81%E3%80%81DataFrame%20%E8%BD%AC%E6%8D%A2%E4%B8%BA%20DataSet" rel="nofollow"> 十、DataFrame 转换为 DataSet</a></p> 
<p id="%C2%A0%E5%8D%81%E4%B8%80%E3%80%81DataSet%20%E8%BD%AC%E6%8D%A2%E4%B8%BA%20DataFrame-toc" style="margin-left:0px;"><a href="#%C2%A0%E5%8D%81%E4%B8%80%E3%80%81DataSet%20%E8%BD%AC%E6%8D%A2%E4%B8%BA%20DataFrame" rel="nofollow"> 十一、DataSet 转换为 DataFrame</a></p> 
<p id="%C2%A0%E5%8D%81%E4%BA%8C%E3%80%81RDD%E3%80%81DataFrame%E3%80%81DataSet%20%E4%B8%89%E8%80%85%E7%9A%84%E5%85%B3%E7%B3%BB-toc" style="margin-left:0px;"><a href="#%C2%A0%E5%8D%81%E4%BA%8C%E3%80%81RDD%E3%80%81DataFrame%E3%80%81DataSet%20%E4%B8%89%E8%80%85%E7%9A%84%E5%85%B3%E7%B3%BB" rel="nofollow"> 十二、RDD、DataFrame、DataSet 三者的关系</a></p> 
<p id="%E4%B8%89%E8%80%85%E7%9A%84%E5%85%B1%E6%80%A7-toc" style="margin-left:80px;"><a href="#%E4%B8%89%E8%80%85%E7%9A%84%E5%85%B1%E6%80%A7" rel="nofollow">三者的共性</a></p> 
<p id="%E4%B8%89%E8%80%85%E7%9A%84%E5%8C%BA%E5%88%AB-toc" style="margin-left:80px;"><a href="#%E4%B8%89%E8%80%85%E7%9A%84%E5%8C%BA%E5%88%AB" rel="nofollow">三者的区别</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="%E4%B8%80%E3%80%81RDD%E7%9A%84%E6%A6%82%E5%BF%B5"><a id="pandas_16"></a>一、RDD的概念</h2> 
<p><strong><code>RDD</code></strong> 是<code>Spark</code>的核心抽象，即 <strong>弹性分布式数据集</strong>（<code>residenta distributed dataset</code>）。代表一个不可变，可分区，里面元素可并行计算的集合。其具有数据流模型的特点：自动容错，位置感知性调度和可伸缩性。<br> 在<code>Spark</code>中，对数据的所有操作不外乎创建<code>RDD</code>、转化已有<code>RDD</code>以及调用 <code>RDD</code>操作进行求值。</p> 
<h2 id="%E4%BA%8C%E3%80%81DataFrame%20%E6%98%AF%E4%BB%80%E4%B9%88"><a id="_19"></a>二、DataFrame 是什么</h2> 
<h3><a id="1_20"></a></h3> 
<p style="margin-left:.0001pt;text-align:left;">在 Spark 中，DataFrame 是一种以 RDD 为基础的分布式数据集，类似于传统数据库中 的二维表格。DataFrame 与 RDD 的主要区别在于，前者带有 schema 元信息，即 DataFrame 所表示的二维表数据集的每一列都带有名称和类型。这使得 Spark SQL 得以洞察更多的结构信息，从而对藏于 DataFrame 背后的数据源以及作用于 DataFrame 之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观 RDD，由于无从得知所存数据元素的具体内部结构，Spark Core 只能在 stage 层面进行简单、通用的流水线优化。</p> 
<p style="margin-left:.0001pt;text-align:left;">同时，与 Hive 类似，DataFrame 也支持嵌套数据类型（struct、array 和 map）。从 API 易用性的角度上看，DataFrame API 提供的是一套高层的关系操作，比函数式的 RDD API 要 更加友好，门槛更低。</p> 
<p style="margin-left:.0001pt;text-align:left;"><img alt="" height="239" src="https://images2.imgbox.com/88/d1/dB2NWYA9_o.png" width="492"></p> 
<p>左侧的 RDD[Person]虽然以 Person 为类型参数，但 Spark 框架本身不了解 Person 类的内部结构。而右侧的 DataFrame 却提供了详细的结构信息，使得 Spark SQL 可以清楚地知道 该数据集中包含哪些列，每列的名称和类型各是什么。 DataFrame 是为数据提供了 Schema 的视图。可以把它当做数据库中的一张表来对待 DataFrame 也是懒执行的，但性能上比 RDD 要高，主要原因：优化的执行计划，即查询计 划通过 Spark catalyst optimiser 进行优化。 </p> 
<h2 id="%E4%B8%89%E3%80%81DataSet%20%E6%98%AF%E4%BB%80%E4%B9%88">三、DataSet 是什么</h2> 
<p style="margin-left:.0001pt;text-align:left;">DataSet 是分布式数据集合。DataSet 是 Spark 1.6 中添加的一个新抽象，是 DataFrame 的一个扩展。它提供了 RDD 的优势（强类型，使用强大的 lambda 函数的能力）以及 Spark SQL 优化执行引擎的优点。DataSet 也可以使用功能性的转换（操作 map，flatMap，filter 等等）</p> 
<p style="margin-left:.0001pt;text-align:left;">➢ DataSet 是 DataFrame API 的一个扩展，是 SparkSQL 最新的数据抽象</p> 
<p style="margin-left:.0001pt;text-align:left;">➢ 用户友好的 API 风格，既具有类型安全检查也具有 DataFrame 的查询优化特性；</p> 
<p style="margin-left:.0001pt;text-align:left;">➢ 用样例类来对 DataSet 中定义数据的结构信息，样例类中每个属性的名称直接映射到 DataSet 中的字段名称；</p> 
<p style="margin-left:.0001pt;text-align:left;">➢ DataSet 是强类型的。比如可以有 DataSet[Car]，DataSet[Person]。</p> 
<p style="margin-left:.0001pt;text-align:left;">➢ DataFrame 是 DataSet 的特列，DataFrame=DataSet[Row] ，所以可以通过 as 方法将 DataFrame 转换为 DataSet。Row 是一个类型，跟 Car、Person 这些的类型一样，所有的表结构信息都用 Row 来表示。获取数据时需要指定顺序</p> 
<h2 id="%E5%9B%9B%E3%80%81%E5%88%9B%E5%BB%BA%20DataFrame" style="margin-left:.0001pt;text-align:left;">四、<strong><strong><strong>创建 DataFrame</strong></strong></strong></h2> 
<p style="margin-left:.0001pt;text-align:justify;">在 Spark SQL 中 SparkSession 是创建 DataFrame 和执行 SQL 的入口，创建 DataFrame</p> 
<p style="margin-left:.0001pt;text-align:justify;">有三种方式：通过 Spark 的数据源进行创建；从一个存在的 RDD 进行转换；还可以从 Hive</p> 
<p style="margin-left:.0001pt;text-align:justify;">Table 进行查询返回。</p> 
<p style="margin-left:18pt;">从 Spark 数据源进行创建</p> 
<p style="margin-left:.0001pt;text-align:justify;">Spark-SQL支持的数据类型：</p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="39" src="https://images2.imgbox.com/34/a9/oJF1NmJu_o.png" width="693"></p> 
<p style="margin-left:18pt;">➢ 在 spark 的 bin/data 目录中创建 user.json 文件</p> 
<p style="margin-left:.0001pt;text-align:justify;">{"username":"zhangsan","age":20}</p> 
<p style="margin-left:18pt;">{"username":"lisi","age":17}</p> 
<p style="margin-left:18pt;">➢ 读取 json 文件创建 DataFrame</p> 
<p style="margin-left:18pt;"><strong><strong>val df = spark.read.json("data/user.json")</strong></strong></p> 
<p style="margin-left:18pt;"><img alt="" height="77" src="https://images2.imgbox.com/e6/84/hftK14hA_o.png" width="693"></p> 
<p style="margin-left:18pt;"><span style="background-color:#ffff00;">注意：如果从内存中获取数据，spark 可以知道数据类型具体是什么。如果是数字，默认作为 Int 处理；但是从文件中读取的数字，不能确定是什么类型，所以用 bigint 接收，可以和 Long 类型转换，但是和 Int 不能进行转换</span><span style="background-color:#ffff00;">。</span></p> 
<p style="margin-left:18pt;">展示数据：</p> 
<p style="margin-left:18pt;"><strong><strong>d</strong></strong><strong><strong>f.show</strong></strong></p> 
<p style="margin-left:18pt;"><img alt="" height="146" src="https://images2.imgbox.com/5d/f6/IOztOnrr_o.png" width="379"></p> 
<h2 id="%E4%BA%94%E3%80%81RDD%20%E8%BD%AC%E6%8D%A2%E4%B8%BA%20DataFrame">五、<strong><strong><strong>RDD 转换为 DataFrame</strong></strong></strong></h2> 
<p style="margin-left:.0001pt;text-align:justify;">在 IDEA 中开发程序时，如果需要 RDD 与 DF 或者 DS 之间互相操作，那么需要引入 import spark.implicits._ 这里的 spark 不是 Scala 中的包名，而是创建的 sparkSession 对象的变量名称，所以必 须先创建 SparkSession 对象再导入。这里的 spark 对象不能使用 var 声明，因为 Scala 只支持 val 修饰的对象的引入。</p> 
<p style="margin-left:.0001pt;text-align:justify;">spark-shell 中无需导入，自动完成此操作。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><strong>val idRDD = sc.textFile("data/id.txt")</strong></strong></p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="28" src="https://images2.imgbox.com/b6/66/4uRpB0Cq_o.png" width="693"></p> 
<p> <strong><strong>idRDD.toDF("id").show</strong></strong></p> 
<p><img alt="" height="189" src="https://images2.imgbox.com/60/1f/bdd9hFxN_o.png" width="323"></p> 
<p style="margin-left:.0001pt;text-align:justify;">实际开发中，一般通过样例类将 RDD 转换为 DataFrame</p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><strong>case class User(name:String, age:Int)</strong></strong></p> 
<p><img alt="" height="58" src="https://images2.imgbox.com/16/f7/9FEIP4Eo_o.png" width="469"></p> 
<p> <strong><strong>sc.makeRDD(List(("zhangsan",30), ("lisi",40))).map(t=&gt;User(t._1, t._2)).toDF.show</strong></strong></p> 
<p><img alt="" height="161" src="https://images2.imgbox.com/55/66/drs9oqHB_o.png" width="693"></p> 
<h2 id="%C2%A0%E5%85%AD%E3%80%81DataFrame%20%E8%BD%AC%E6%8D%A2%E4%B8%BA%20RDD"> 六、<strong><strong><strong>DataFrame 转换为 RDD</strong></strong></strong></h2> 
<p style="margin-left:.0001pt;text-align:justify;">DataFrame 其实就是对 RDD 的封装，所以可以直接获取内部的 RDD</p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><strong>val df = sc.makeRDD(List(("zhangsan",30), ("lisi",40))).map(t=&gt;User(t._1, t._2)).toDF</strong></strong></p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="37" src="https://images2.imgbox.com/52/92/L3dt0cLN_o.png" width="693"></p> 
<p> <strong><strong>val rdd = df.rdd</strong></strong></p> 
<p><img alt="" height="37" src="https://images2.imgbox.com/37/b1/wzQ43XIH_o.png" width="693"></p> 
<p><strong><strong>val array = rdd.collect</strong></strong></p> 
<p><img alt="" height="47" src="https://images2.imgbox.com/c4/80/Ubv6BjH5_o.png" width="693"></p> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="color:#ff0000;">注意：此时得到的 RDD 存储类型为 Row</span></p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><strong>array(0)</strong></strong></p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="52" src="https://images2.imgbox.com/18/15/Sxupoq8u_o.png" width="494"></p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><strong>array(0)(0)</strong></strong></p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="53" src="https://images2.imgbox.com/80/ee/ZZedhjAq_o.png" width="241"></p> 
<p><strong><strong>array(0).getAs[String]("name")</strong></strong> </p> 
<p><img alt="" height="58" src="https://images2.imgbox.com/36/e0/glAYnxUl_o.png" width="428"> </p> 
<h2 id="%E4%B8%83%E3%80%81%E5%88%9B%E5%BB%BA%20DataSet">七、<strong><strong><strong>创建 DataSet</strong></strong></strong></h2> 
<p style="margin-left:.0001pt;text-align:justify;">1） 使用样例类序列创建 DataSet</p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><strong>case class Person(name: String, age: Long)</strong></strong></p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="48" src="https://images2.imgbox.com/ad/5a/sK2knxYC_o.png" width="502"></p> 
<p> <strong><strong>val caseClassDS = Seq(Person("zhangsan",2)).toDS()</strong></strong></p> 
<p><img alt="" height="55" src="https://images2.imgbox.com/6e/d2/T5WRFcur_o.png" width="693"></p> 
<p> <strong><strong>caseClassDS.show</strong></strong></p> 
<p><img alt="" height="122" src="https://images2.imgbox.com/c4/d9/zZU1Y6KN_o.png" width="288"></p> 
<p> 2） 使用基本类型的序列创建 DataSet</p> 
<p><strong><strong>val ds = Seq(1,2,3,4,5).toDS</strong></strong></p> 
<p><img alt="" height="122" src="https://images2.imgbox.com/47/c7/smaSMY22_o.png" width="288"></p> 
<p> <strong><strong>ds.show</strong></strong></p> 
<p><img alt="" height="159" src="https://images2.imgbox.com/1d/23/6xiYy2oU_o.png" width="154"></p> 
<p> <strong><strong>注意：在实际使用的时候，很少用到把序列转换成DataSet，更多的是通过RDD来得到DataSet</strong></strong></p> 
<h2 id="%E5%85%AB%E3%80%81RDD%20%E8%BD%AC%E6%8D%A2%E4%B8%BA%20DataSet">八、<strong><strong><strong>RDD 转换为 DataSet</strong></strong></strong></h2> 
<p style="margin-left:.0001pt;text-align:justify;">SparkSQL 能够自动将包含有 case 类的 RDD 转换成 DataSet，case 类定义了 table 的结 构，case 类属性通过反射变成了表的列名。Case 类可以包含诸如 Seq 或者 Array 等复杂的结构。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><strong>case class User(name:String, age:Int)</strong></strong></p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="73" src="https://images2.imgbox.com/83/6f/yxE9Kv0P_o.png" width="449"></p> 
<p> <strong><strong>sc.makeRDD(List(("zhangsan",30), ("lisi",49))).map(t=&gt;User(t._1, t._2)).toDS</strong></strong></p> 
<p><img alt="" height="51" src="https://images2.imgbox.com/3b/aa/TX8IqTdr_o.png" width="693"></p> 
<h2 id="%C2%A0%E4%B9%9D%E3%80%81DataSet%20%E8%BD%AC%E6%8D%A2%E4%B8%BA%20RDD"> 九、<strong><strong><strong>DataSet 转换为 RDD</strong></strong></strong></h2> 
<p style="margin-left:.0001pt;text-align:justify;">DataSet 其实也是对 RDD 的封装，所以可以直接获取内部的 RDD</p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><strong>case class User(name:String, age:Int)</strong></strong></p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="57" src="https://images2.imgbox.com/3b/1d/kk9h5jQA_o.png" width="468"></p> 
<p> <strong><strong>sc.makeRDD(List(("zhangsan",30), ("lisi",49))).map(t=&gt;User(t._1, t._2)).toDS</strong></strong></p> 
<p><img alt="" height="48" src="https://images2.imgbox.com/54/1b/QrwrYpUH_o.png" width="693"></p> 
<p> <strong><strong>val rdd = res3.rdd</strong></strong></p> 
<p><img alt="" height="52" src="https://images2.imgbox.com/04/e1/ba1aJQgR_o.png" width="693"></p> 
<p> <strong><strong>rdd.collect</strong></strong></p> 
<p><img alt="" height="68" src="https://images2.imgbox.com/ed/6f/J4ZVctkR_o.png" width="610"></p> 
<h2 id="%C2%A0%E5%8D%81%E3%80%81DataFrame%20%E8%BD%AC%E6%8D%A2%E4%B8%BA%20DataSet"> 十、<strong><strong><strong>DataFrame 转换为 DataSet</strong></strong></strong></h2> 
<p style="margin-left:.0001pt;text-align:justify;">DataFrame 其实是 DataSet 的特例，所以它们之间是可以互相转换的。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><strong>case class User(name:String, age:Int)</strong></strong></p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="63" src="https://images2.imgbox.com/fa/44/0paiKxQq_o.png" width="461"></p> 
<p> <strong><strong>val df = sc.makeRDD(List(("zhangsan",30), ("lisi",49))).toDF("name","age")</strong></strong></p> 
<p><img alt="" height="82" src="https://images2.imgbox.com/ee/e3/33IZ4Ah9_o.png" width="621"></p> 
<p> <strong><strong>val ds = df.as[User]</strong></strong></p> 
<p><img alt="" height="44" src="https://images2.imgbox.com/5b/94/rs0fyJUl_o.png" width="653"></p> 
<h2 id="%C2%A0%E5%8D%81%E4%B8%80%E3%80%81DataSet%20%E8%BD%AC%E6%8D%A2%E4%B8%BA%20DataFrame"> 十一、<strong><strong><strong>DataSet 转换为 DataFrame</strong></strong></strong></h2> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><strong>val ds = df.as[User]</strong></strong></p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="60" src="https://images2.imgbox.com/ad/19/2agzqXsj_o.png" width="668"></p> 
<p> <strong><strong>val df = ds.toDF</strong></strong></p> 
<p><img alt="" height="62" src="https://images2.imgbox.com/30/9d/VY9ngXUz_o.png" width="623"></p> 
<h2 id="%C2%A0%E5%8D%81%E4%BA%8C%E3%80%81RDD%E3%80%81DataFrame%E3%80%81DataSet%20%E4%B8%89%E8%80%85%E7%9A%84%E5%85%B3%E7%B3%BB"> 十二、<strong><strong><strong>RDD、DataFrame、DataSet 三者的关系</strong></strong></strong></h2> 
<p style="margin-left:.0001pt;text-align:justify;">在 SparkSQL 中 Spark 为我们提供了两个新的抽象，分别是 DataFrame 和 DataSet。他们 和 RDD 有什么区别呢？</p> 
<p style="margin-left:.0001pt;text-align:justify;">首先从版本的产生上来看：</p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p style="margin-left:.0001pt;text-align:justify;">➢ Spark1.0 =&gt; RDD</p> 
<p style="margin-left:.0001pt;text-align:justify;">➢ Spark1.3 =&gt; DataFrame</p> 
<p style="margin-left:.0001pt;text-align:justify;">➢ Spark1.6 =&gt; Dataset</p> 
<p style="margin-left:.0001pt;text-align:justify;">如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不</p> 
<p style="margin-left:.0001pt;text-align:justify;">同是的他们的执行效率和执行方式。在后期的 Spark 版本中，DataSet 有可能会逐步取代 RDD和 DataFrame 成为唯一的 API 接口。</p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<h4 id="%E4%B8%89%E8%80%85%E7%9A%84%E5%85%B1%E6%80%A7" style="text-align:justify;"><strong><strong><strong>三者的共性</strong></strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">➢ RDD、DataFrame、DataSet 全都是 spark 平台下的分布式弹性数据集，为处理超大型数</p> 
<p style="margin-left:.0001pt;text-align:justify;">据提供便利;</p> 
<p style="margin-left:.0001pt;text-align:justify;">➢ 三者都有惰性机制，在进行创建、转换，如 map 方法时，不会立即执行，只有在遇到</p> 
<p style="margin-left:.0001pt;text-align:justify;">Action 如 foreach 时，三者才会开始遍历运算;</p> 
<p style="margin-left:.0001pt;text-align:justify;">➢ 三者有许多共同的函数，如 filter，排序等;</p> 
<p style="margin-left:.0001pt;text-align:justify;">➢ 在对 DataFrame 和 Dataset 进行操作许多操作都需要这个包:</p> 
<p style="margin-left:.0001pt;text-align:justify;">import spark.implicits._（在创建好 SparkSession 对象后尽量直接导入）</p> 
<p style="margin-left:.0001pt;text-align:justify;">➢ 三者都会根据 Spark 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会</p> 
<p style="margin-left:.0001pt;text-align:justify;">内存溢出</p> 
<p style="margin-left:.0001pt;text-align:justify;">➢ 三者都有 partition 的概念</p> 
<p style="margin-left:.0001pt;text-align:justify;">➢ DataFrame 和 DataSet 均可使用模式匹配获取各个字段的值和类型</p> 
<h4 id="%E4%B8%89%E8%80%85%E7%9A%84%E5%8C%BA%E5%88%AB" style="text-align:justify;"><strong><strong><strong>三者的区别</strong></strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">1) RDD</p> 
<p style="margin-left:.0001pt;text-align:justify;">➢ RDD 一般和 spark mllib 同时使用</p> 
<p style="margin-left:.0001pt;text-align:justify;">➢ RDD 不支持 sparksql 操作</p> 
<p style="margin-left:.0001pt;text-align:justify;">2) DataFrame</p> 
<p style="margin-left:.0001pt;text-align:justify;">➢ 与 RDD 和 Dataset 不同，DataFrame 每一行的类型固定为 Row，每一列的值没法直</p> 
<p style="margin-left:.0001pt;text-align:justify;">接访问，只有通过解析才能获取各个字段的值</p> 
<p style="margin-left:.0001pt;text-align:justify;">➢ DataFrame 与 DataSet 一般不与 spark mllib 同时使用</p> 
<p style="margin-left:.0001pt;text-align:justify;">➢ DataFrame 与 DataSet 均支持 SparkSQL 的操作，比如 select，groupby 之类，还能</p> 
<p style="margin-left:.0001pt;text-align:justify;">注册临时表/视窗，进行 sql 语句操作</p> 
<p style="margin-left:.0001pt;text-align:justify;">➢ DataFrame 与 DataSet 支持一些特别方便的保存方式，比如保存成 csv，可以带上表</p> 
<p style="margin-left:.0001pt;text-align:justify;">头，这样每一列的字段名一目了然</p> 
<p style="margin-left:.0001pt;text-align:justify;">3) DataSet</p> 
<p style="margin-left:.0001pt;text-align:justify;">➢ Dataset 和 DataFrame 拥有完全相同的成员函数，区别只是每一行的数据类型不同。</p> 
<p style="margin-left:.0001pt;text-align:justify;">DataFrame 其实就是 DataSet 的一个特例 type DataFrame = Dataset[Row]</p> 
<p style="margin-left:.0001pt;text-align:justify;">➢ DataFrame 也可以叫 Dataset[Row],每一行的类型是 Row，不解析，每一行究竟有哪</p> 
<p style="margin-left:.0001pt;text-align:justify;">些字段，各个字段又是什么类型都无从得知，只能用上面提到的 getAS 方法或者共性里提到的模式匹配拿出特定字段。而 Dataset 中，每一行是什么类型是不一定的，在自定义了 case class 之后可以很自由的获得每一行的信息。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="393" src="https://images2.imgbox.com/65/a7/KA1g8Cwe_o.png" width="693"></p> 
<p> </p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p style="margin-left:.0001pt;text-align:justify;">三者可以通过上图的方式进行相互转换。</p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p style="margin-left:.0001pt;text-align:justify;"></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1a036289bc825b621dbf191887b15ce8/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Python 中__new__详解及使用介绍</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1a4b3e62a228b3bd271380a723fbb6af/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【已解决】redis启动错误： Warning: no config file specified, using the default config. In order to specify a</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>