<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>R-CNN(目标检测算法)介绍 - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="R-CNN(目标检测算法)介绍" />
<meta property="og:description" content="一、什么是目标检测 1.1 目标检测概念 目标检测 (Object Detection) 是计算机视觉领域中的一项任务，旨在从图像或视频中准确地定位和识别多个感兴趣的目标物体。目标检测不仅需要确定目标的类别，还要找到目标在图像中的位置.由于各类目标不同的外观，颜色，大小以及在成像时光照，遮挡等具有挑战性的问题，目标检测一直处于不断的优化和研究中.
1.2 目标检测步骤: 数据收集和标注：首先，需要收集包含目标对象的图像或视频数据集。这些数据集应该覆盖所需检测的目标类别，并且需要进行手动标注，即为每个目标在图像中标记边界框及对应的类别标签。
候选区域生成：通过使用候选区域生成算法（如选择性搜索、边缘框架、基于深度学习的区域建议网络等），从输入图像中生成一组可能包含目标的候选区域。这些候选区域是用来进一步分析和检测目标的区域。
特征提取：对于每个候选区域，使用卷积神经网络（CNN）或其他特征提取方法来提取区域的特征表示。这些特征通常是在预训练的网络上提取得到的，可以捕捉到图像中的局部和全局信息。
目标分类与回归：基于提取的特征，使用分类器（如支持向量机(SVM)、softmax分类器）进行目标分类，确定候选区域中的目标类别。同时，还进行边界框回归，以更准确地定位目标在候选区域中的位置和尺寸。
非极大值抑制：由于一个目标可能会在多个候选区域中被检测到，需要进行非极大值抑制。该过程通过选择具有最高置信度的目标检测结果，并消除重叠的检测结果来排除冗余。
后处理和可视化：最后，对于每个被保留的目标检测结果，可以进行后处理操作，如去除低置信度的检测结果、进一步分析目标属性等。同时，还可以将检测结果以边界框或标签的形式可视化，使其易于理解和解释。
边界框是什么?
边界框（bounding box）是在目标检测和物体识别中经常使用的一种表示方法，用于定位目标在图像中的位置和范围。边界框是一个矩形框，通常由左上角和右下角两个点的坐标表示。
边界框提供了目标在图像中的大致位置信息，并且可以通过其坐标来确定目标的尺寸和形状。在目标检测任务中，边界框通常与目标类别关联，用于表示检测到的目标及其对应的类别。
边界框可以简洁地描述目标的位置和区域，使得我们可以准确地标记、跟踪和定位目标。它是许多目标检测算法的核心组成部分，例如RCNN系列、YOLO、SSD等算法。
边界框通常用一组数值来表示，其中最常见的是四个浮点数或整数，分别表示左上角和右下角点的(x, y)坐标。有时候还会包括目标的类别标签和置信度得分等附加信息。
通过边界框，我们可以方便地进行目标的定位、裁剪、划分和测量等操作，为目标检测和物体识别任务提供了重要的信息基础。
1.3 目标检测算法分类 1.传统方法与深度学习方法:
传统方法：传统的目标检测方法主要基于手工设计的特征和机器学习算法，如Haar特征、HOG（Histogram of Oriented Gradients）和SIFT（Scale-Invariant Feature Transform）等。这些方法通常在目标检测性能方面存在一定的限制。深度学习方法：近年来，随着深度学习的发展，基于深度神经网络的目标检测方法取得了显著的进展。这些方法通过端到端的训练，自动学习图像特征表示和目标分类器，例如RCNN系列、YOLO系列、SSD、RetinaNet和EfficientDet等。 2.两阶段方法与单阶段方法:
两阶段方法(Tow Stage)：两阶段方法将目标检测任务分为候选区域生成和目标分类定位两个阶段。首先，通过选择性搜索、区域建议网络（RPN）等方法生成候选区域，然后对每个候选区域进行分类和边界框回归。典型的两阶段方法有RCNN系列和Faster R-CNN等。单阶段方法(One Stage)：单阶段方法直接在图像上密集地预测目标的类别和边界框，通常通过将图像分割为网格单元，并为每个单元生成预测来实现。常见的单阶段方法包括YOLO系列和SSD等。 3.基于特征金字塔(Feature Pyramid)的方法:
特征金字塔是指在不同尺度上提取特征并融合它们以检测不同大小的目标。基于特征金字塔的方法可以更好地处理多尺度目标。例如，FPN（Feature Pyramid Network）和RetinaNet就是基于特征金字塔的目标检测算法。 4.单目标检测与多目标检测：
单目标检测：单目标检测算法旨在检测和定位图像中的单个目标实例。多目标检测：多目标检测算法能够同时检测和定位图像中的多个目标实例，例如目标检测中的行人、车辆、动物等. 二、什么是R-CNN 2.1 R-CNN概念 RCNN（Region-based Convolutional Neural Networks）是一种经典的目标检测算法，最初由Girshick等人在2014年提出。它通过将图像分割成多个候选区域，并对每个候选区域进行分类和边界框回归来实现目标检测。
2.2 R-CNN结构 整体步骤:
候选区域生成：在图像中提取候选区域，这些候选区域通常是通过选择性搜索（Selective Search）等算法生成的。选择性搜索将图像分割为多个不同尺度和形状的区域，并根据颜色、纹理、大小等特征进行合并，以产生可能包含目标的候选区域。
特征提取：对每个候选区域应用卷积神经网络（CNN）来提取特征表示。通常使用预训练的CNN模型，如AlexNet、VGGNet或ResNet等，在候选区域上进行前向传播，以获得固定长度的特征向量。
目标分类:提取的特征向量作为输入，经过一个支持向量机（Support Vector Machine，SVM）分类器进行目标类别的分类。在训练阶段，SVM使用正负样本对模型进行训练，学习区分目标和非目标的能力。
非极大值抑制：由于选择性搜索算法可能会生成重叠的候选区域，为了消除冗余的检测结果，R-CNN使用非极大值抑制（Non-Maximum Suppression，NMS）来筛选最终的检测边界框。NMS通过比较检测框之间的重叠程度和得分，并保留最具有代表性的框。
边框位置修正 Boundingbox Regression
2.2.1 候选区域生成-----选择性搜索（Selective Search)算法: 选择性搜索（Selective Search）是一种用于生成候选区域的图像分割算法，常用于目标检测任务中。它通过分割和合并策略生成多个大小、形状和纹理不同的候选区域，以涵盖可能包含目标对象的各种可能性。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/aec719b5cd399228c355bc2247627e80/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-22T22:42:30+08:00" />
<meta property="article:modified_time" content="2023-10-22T22:42:30+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">R-CNN(目标检测算法)介绍</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2><strong>一、什么是目标检测</strong></h2> 
<h3>1.1 目标检测概念</h3> 
<p>目标检测 (Object Detection) 是计算机视觉领域中的一项任务，旨在从图像或视频中准确地定位和识别多个感兴趣的目标物体。目标检测不仅需要确定目标的类别，还要找到目标在图像中的位置.由于各类目标不同的外观，颜色，大小以及在成像时光照，遮挡等具有挑战性的问题，目标检测一直处于不断的优化和研究中.</p> 
<p>                                <img alt="" height="505" src="https://images2.imgbox.com/a1/b4/RbbBJ51Z_o.png" width="397"></p> 
<h3>1.2 目标检测步骤:</h3> 
<ol><li> <p>数据收集和标注：首先，需要收集包含目标对象的图像或视频数据集。这些数据集应该覆盖所需检测的目标类别，并且需要进行手动标注，即为每个目标在图像中标记边界框及对应的类别标签。</p> </li><li> <p>候选区域生成：通过使用候选区域生成算法（如选择性搜索、边缘框架、基于深度学习的区域建议网络等），从输入图像中生成一组可能包含目标的候选区域。这些候选区域是用来进一步分析和检测目标的区域。</p> </li><li> <p>特征提取：对于每个候选区域，使用卷积神经网络（CNN）或其他特征提取方法来提取区域的特征表示。这些特征通常是在预训练的网络上提取得到的，可以捕捉到图像中的局部和全局信息。</p> </li><li> <p>目标分类与回归：基于提取的特征，使用分类器（如支持向量机(SVM)、softmax分类器）进行目标分类，确定候选区域中的目标类别。同时，还进行边界框回归，以更准确地定位目标在候选区域中的位置和尺寸。</p> </li><li> <p>非极大值抑制：由于一个目标可能会在多个候选区域中被检测到，需要进行非极大值抑制。该过程通过选择具有最高置信度的目标检测结果，并消除重叠的检测结果来排除冗余。</p> </li><li> <p>后处理和可视化：最后，对于每个被保留的目标检测结果，可以进行后处理操作，如去除低置信度的检测结果、进一步分析目标属性等。同时，还可以将检测结果以边界框或标签的形式可视化，使其易于理解和解释。</p> </li></ol> 
<p>边界框是什么?</p> 
<p>  边界框（bounding box）是在目标检测和物体识别中经常使用的一种表示方法，用于定位目标在图像中的位置和范围。边界框是一个矩形框，通常由左上角和右下角两个点的坐标表示。</p> 
<p>  边界框提供了目标在图像中的大致位置信息，并且可以通过其坐标来确定目标的尺寸和形状。在目标检测任务中，边界框通常与目标类别关联，用于表示检测到的目标及其对应的类别。</p> 
<p>边界框可以简洁地描述目标的位置和区域，使得我们可以准确地标记、跟踪和定位目标。它是许多目标检测算法的核心组成部分，例如RCNN系列、YOLO、SSD等算法。</p> 
<p>边界框通常用一组数值来表示，其中最常见的是四个浮点数或整数，分别表示左上角和右下角点的(x, y)坐标。有时候还会包括目标的类别标签和置信度得分等附加信息。</p> 
<p>通过边界框，我们可以方便地进行目标的定位、裁剪、划分和测量等操作，为目标检测和物体识别任务提供了重要的信息基础。</p> 
<h3>1.3 目标检测算法分类</h3> 
<p>1.传统方法与深度学习方法:</p> 
<ul><li>传统方法：传统的目标检测方法主要基于手工设计的特征和机器学习算法，如Haar特征、HOG（Histogram of Oriented Gradients）和SIFT（Scale-Invariant Feature Transform）等。这些方法通常在目标检测性能方面存在一定的限制。</li><li>深度学习方法：近年来，随着深度学习的发展，基于深度神经网络的目标检测方法取得了显著的进展。这些方法通过端到端的训练，自动学习图像特征表示和目标分类器，例如RCNN系列、YOLO系列、SSD、RetinaNet和EfficientDet等。</li></ul> 
<p>2.两阶段方法与单阶段方法:</p> 
<ul><li>两阶段方法(Tow Stage)：两阶段方法将目标检测任务分为候选区域生成和目标分类定位两个阶段。首先，通过选择性搜索、区域建议网络（RPN）等方法生成候选区域，然后对每个候选区域进行分类和边界框回归。典型的两阶段方法有RCNN系列和Faster R-CNN等。</li><li>单阶段方法(One Stage)：单阶段方法直接在图像上密集地预测目标的类别和边界框，通常通过将图像分割为网格单元，并为每个单元生成预测来实现。常见的单阶段方法包括YOLO系列和SSD等。</li></ul> 
<p>3.基于特征金字塔(<span style="color:#4d4d4d;font-size:16px;">Feature Pyramid)的方法:</span></p> 
<ul><li>特征金字塔是指在不同尺度上提取特征并融合它们以检测不同大小的目标。基于特征金字塔的方法可以更好地处理多尺度目标。例如，FPN（Feature Pyramid Network）和RetinaNet就是基于特征金字塔的目标检测算法。</li></ul> 
<p>4.单目标检测与多目标检测：</p> 
<ul><li>单目标检测：单目标检测算法旨在检测和定位图像中的单个目标实例。</li><li>多目标检测：多目标检测算法能够同时检测和定位图像中的多个目标实例，例如目标检测中的行人、车辆、动物等.</li></ul> 
<h2><strong>二、什么是R-CNN</strong></h2> 
<h3>2.1 R-CNN概念</h3> 
<p>RCNN（Region-based Convolutional Neural Networks）是一种经典的目标检测算法，最初由Girshick等人在2014年提出。它通过将图像分割成多个候选区域，并对每个候选区域进行分类和边界框回归来实现目标检测。</p> 
<h3>2.2 R-CNN结构</h3> 
<p><img alt="" height="219" src="https://images2.imgbox.com/d5/c4/5hjzjkxK_o.png" width="640"></p> 
<p></p> 
<p>整体步骤:</p> 
<ol><li> <p>候选区域生成：在图像中提取候选区域，这些候选区域通常是通过选择性搜索（Selective Search）等算法生成的。选择性搜索将图像分割为多个不同尺度和形状的区域，并根据颜色、纹理、大小等特征进行合并，以产生可能包含目标的候选区域。</p> </li><li> <p>特征提取：对每个候选区域应用卷积神经网络（CNN）来提取特征表示。通常使用预训练的CNN模型，如AlexNet、VGGNet或ResNet等，在候选区域上进行前向传播，以获得固定长度的特征向量。</p> </li><li> <p>目标分类:提取的特征向量作为输入，经过一个支持向量机（Support Vector Machine，SVM）分类器进行目标类别的分类。在训练阶段，SVM使用正负样本对模型进行训练，学习区分目标和非目标的能力。</p> </li><li> <p>非极大值抑制：由于选择性搜索算法可能会生成重叠的候选区域，为了消除冗余的检测结果，R-CNN使用非极大值抑制（Non-Maximum Suppression，NMS）来筛选最终的检测边界框。NMS通过比较检测框之间的重叠程度和得分，并保留最具有代表性的框。</p> </li><li> <p>边框位置修正 Boundingbox Regression</p> </li></ol> 
<p></p> 
<h4>2.2.1 候选区域生成-----选择性搜索（Selective Search)算法:</h4> 
<p>选择性搜索（Selective Search）是一种用于生成候选区域的图像分割算法，常用于目标检测任务中。它通过分割和合并策略生成多个大小、形状和纹理不同的候选区域，以涵盖可能包含目标对象的各种可能性。</p> 
<p><img alt="" height="326" src="https://images2.imgbox.com/59/36/fqQZGAD4_o.png" width="860"></p> 
<p>以下是Selective Search算法的详细介绍：</p> 
<ol><li> <p>分割策略：选择性搜索从原始图像开始，首先将图像分割成许多小区域。这些小区域可以基于像素颜色、纹理、亮度等特征进行分割。具体常用的分割方法有基于图像像素相似性的连通区域分割或均值漂移分割等。</p> </li><li> <p>合并策略：在分割步骤之后，选择性搜索使用合并策略来逐渐合并相似的区域。这一步骤能够根据相似性度量（如颜色直方图、纹理特征等）来衡量两个区域之间的相似程度，并将相似性最高的区域合并为一个更大的区域。这个过程迭代进行，直到只剩下一个整体区域。</p> </li><li> <p>候选区域生成：在合并完成后，选择性搜索得到了多个不同尺寸、形状和纹理的候选区域。这些候选区域可以覆盖可能包含目标对象的各种可能性，从而提供了多样化的选择。</p> </li></ol> 
<p>选择性搜索算法的优点在于它能够生成多样性且具有不同尺度的候选区域，从而能够更全面地涵盖目标对象的各种变化。它能够适应不同图像场景和目标特征的变化，并且对于较复杂的图像具有较好的效果。</p> 
<p>值得注意的是，选择性搜索是一种计算密集型算法，因为它需要对图像进行大量的分割和合并操作。为了提高效率，可以采用一些加速策略，如针对不同尺度的图像金字塔、减小分割区域数量等。</p> 
<p>总结来说，选择性搜索是一种常用的目标检测中的区域提取算法，通过分割和合并策略生成多样性的候选区域。这些候选区域能够涵盖可能包含目标对象的各种变化，为后续的目标分类和定位提供了多样性的选择。</p> 
<h4>2.2.2 特征提取</h4> 
<p>1.网络架构选择</p> 
<p>两个可选方案：第一选择经典的Alexnet；第二选择VGG16。经过测试Alexnet精度为58.5%，VGG16精度为66%。VGG这个模型的特点是选择比较小的卷积核、选择较小的跨步，这个网络的精度高，不过计算量是Alexnet的7倍。后面为了简单起见，我们就直接选Alexnet.</p> 
<p><img alt="" height="188" src="https://images2.imgbox.com/5f/0e/520NtSO6_o.png" width="655">Alexnet特征提取部分包含了5个卷积层、2个全连接层，在Alexnet中pool5层神经元个数为9216、 fc6、fc7的神经元个数都是4096，通过这个网络训练完毕后，最后提取特征每个输入候选框图片都能得到一个4096维的特征向量.</p> 
<p>2.有监督预训练阶段(训练网络参数)</p> 
<p><img alt="" height="152" src="https://images2.imgbox.com/04/f1/vqA8qKiG_o.png" width="646"></p> 
<p>在实际测试的时候，模型需要通过CNN提取出中的特征，用于后面的分类与回归。所以，如何训练好CNN成为重中之重。</p> 
<p>由于物体标签训练数据少，如果要直接采用随机初始化CNN参数的方法,目前的训练数据量是不够的. 基于此，采用有监督的预训练，使用一个大的数据集（ImageNet )来训练AlexNet，得到一个1000分类的预训练（Pre-trained）模型。网络优化求解时采用随机梯度下降法（SGD），学习率大小为0.001.</p> 
<p>3.fine-tuning阶段</p> 
<p><img alt="" height="156" src="https://images2.imgbox.com/e3/11/SzBWgGmL_o.png" width="616"></p> 
<p>接着采用 selective search 搜索出来的候选框 （PASCAL VOC 数据库中的图片） 继续对上面预训练的CNN模型进行fine-tuning训练。</p> 
<p>将原来预训练模型最后的1000-way的全连接层（分类层）换成21-way的分类层（20类物体+背景），然后计算每个region proposal和ground truth 的IoU，对于IoU&gt;0.5的region proposal被视为正样本，否则为负样本（即背景）。IOU（Intersection over Union），也称为Jaccard系数，是一种用于评估目标检测和图像分割任务性能的常用指标。它衡量预测边界框或分割掩码与真实边界框或分割掩码之间的重叠程度。</p> 
<p>另外，由于对于一张图片的多有候选区域来说，负样本是远远大于正样本数，所以需要将正样本进行上采样来保证样本分布均衡。在每次迭代的过程中，选择层次采样，每个mini-batch中采样两张图像，从中随机选取32个正样本和96个负样本组成一个mini-batch（128，正负比：1：3）。我们使用0.001的学习率和SGD来进行训练。</p> 
<p>4.提取并保存RP的特征向量<br> 提取特征的CNN网络经过了预训练和微调后不再训练，就固定不变了，只单纯的作为一个提特征的工具了。虽然文中训练了CNN网络对region proposal进行分类，但是实际中，这个CNN的作用只是提取每个region proposal的feature。<br> 所以，我们输入VOC训练数据集，SS搜索出2000个RP后输入进CNN进行前向传播，然后保存AlexNet的FC7层4096维的features，以供后续的SVM分类使用。</p> 
<h4>2.2.3 SVM分类</h4> 
<p>这是一个二分类问题，假设我们要检测车辆。我们知道只有当bounding box把整量车都包含在内，那才叫正样本；如果bounding box 没有包含到车辆，那么我们就可以把它当做负样本。但问题是当我们的检测窗口只有部分包含物体，那该怎么定义正负样本呢？作者测试了IOU阈值各种方案数值0,0.1,0.2,0.3,0.4,0.5。最后通过训练发现，如果选择IOU阈值为0.3效果最好（选择为0精度下降了4个百分点，选择0.5精度下降了5个百分点）,即当重叠度小于0.3的时候，我们就把它标注为<strong>负样本</strong>。一旦CNN f7层特征被提取出来，那么我们将为每个物体类训练一个svm分类器。当我们用CNN提取2000个候选框，可以得到20004096这样的特征向量矩阵，然后我们只需要把这样的一个矩阵与svm权值矩阵4096N点乘(N为分类类别数目，因为我们训练的N个svm，每个svm包含了4096个权值w)，就可以得到结果了。<br>  </p> 
<p><img alt="" height="265" src="https://images2.imgbox.com/eb/21/10GhrWyI_o.png" width="733"></p> 
<p>得到的特征输入到SVM进行分类看看这个feature vector所对应的region proposal是需要的物体还是无关的实物(background) 。 排序，canny边界检测之后就得到了我们需要的bounding-box。</p> 
<p>总结一下：整个系统分为三个部分：1.产生不依赖与特定类别的region proposals，这些region proposals定义了一个整个检测器可以获得的候选目标2.一个大的卷积神经网络，对每个region产生一个固定长度的特征向量3.一系列特定类别的线性SVM分类器。</p> 
<h4>2.2.4 非极大值抑制法（NMS）</h4> 
<p>目的: 筛选候选区域，目标是一个物体只保留一个最优的框，来抑制那些冗余的候选框,一句话就是去掉多余的region proposals.</p> 
<p><img alt="" height="234" src="https://images2.imgbox.com/7e/7e/0CRiPd2L_o.png" width="608"></p> 
<p>selective research会产生2k个region proposals。经过svm打分后，一个物体可能就有多个框。而我们只需要一个最优的（分最好的框），经过NMS抑制冗余的框，得到下图。</p> 
<p><img alt="" height="277" src="https://images2.imgbox.com/c0/dc/Fyi8l4CL_o.png" width="720"></p> 
<h4>2.2.5边框位置修正 Boundingbox Regression</h4> 
<p>去掉了冗余的box还不够，因为这些box都是算法生成的，与ground truth(物体真实位置)肯定存在出入，我们希望这些box能尽可能接近ground truth。例如下图</p> 
<p><img alt="" height="631" src="https://images2.imgbox.com/ae/88/hok4faYx_o.png" width="720"></p> 
<p></p> 
<p>红色框是ground truth，绿色框是生成的region proposal，这里看到我们需要修正绿色框，让它更接近红色框。于是使用了一组BBox Repression（每个分别对应一个类别）来做这个事情，输入原本的框，预测出一个新的框来接近ground truth。因为一个框可以用四个值来表示（x,y,w,h）分别是框的中心点横坐标，中心点纵坐标，宽，高。</p> 
<h2>三、R-CNN的优点和缺点</h2> 
<p>优点:</p> 
<ol><li> <p>准确性较高：R-CNN通过对候选区域进行CNN特征提取和分类，能够捕捉目标的丰富语义信息，从而获得较高的检测准确性。</p> </li><li> <p>强大的特征表示：通过在预训练的深度卷积神经网络上进行特征提取，RCNN可以学习到通用的、具有判别性的特征表示，从而在不同任务和数据集上表现良好。</p> </li><li> <p>对小目标检测效果好：由于使用候选区域生成方法，RCNN在处理小目标时相对有效，可以更好地定位和检测小尺寸的目标。</p> </li><li> <p>可以实现多类别检测：RCNN可以应用于多类别的目标检测任务，并通过预测每个候选区域的概率来进行多类别分类。</p> </li></ol> 
<p>缺点：</p> 
<ol><li> <p>计算资源消耗大：RCNN需要逐个处理候选区域，导致计算资源消耗较大。这使得RCNN在速度方面相对较慢，难以满足实时性要求。</p> </li><li> <p>复杂的训练流程：RCNN的训练过程相对复杂，包括候选区域生成、特征提取和分类等多个阶段，需要消耗大量的时间和计算资源。</p> </li><li> <p>不支持端到端训练：由于RCNN的训练流程中包含多个独立的步骤，无法进行端到端的联合训练。这可能导致训练过程较为繁琐，并且难以优化整体性能。</p> </li><li> <p>候选区域生成不精确：RCNN使用选择性搜索等方法生成候选区域，但这些方法在目标定位上可能存在一定的误差，导致一些目标被漏检或错误地检测。</p> </li></ol> 
<p>总的来说，虽然R-CNN在准确性和特征表示方面表现出色，但对计算资源的需求较高，并且训练过程相对复杂。后续发展的一系列改进算法（如Fast R-CNN、Faster R-CNN等）也针对这些缺点进行了改进，提升了速度和性能。</p> 
<p>相关R-CNN论文:</p> 
<p>Multi_level_Region_based_Convolutional_Neural_Network_for_Image_Emotion_Classification</p> 
<p>参考:<a href="https://blog.csdn.net/Tomxiaodai/article/details/81749845?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169797726016800213089237%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=169797726016800213089237&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-6-81749845-null-null.142%5Ev96%5Epc_search_result_base6&amp;utm_term=rcnn%E7%BB%93%E6%9E%84&amp;spm=1018.2226.3001.4187" title="R-CNN网络结构讲解_rcnn网络结构-CSDN博客">R-CNN网络结构讲解_rcnn网络结构-CSDN博客</a><a href="https://blog.csdn.net/qq_46092061/article/details/119698640?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169797726016800213089237%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=169797726016800213089237&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-119698640-null-null.142%5Ev96%5Epc_search_result_base6&amp;utm_term=rcnn%E7%BB%93%E6%9E%84&amp;spm=1018.2226.3001.4187" title="【精选】【目标检测算法】R-CNN（详解）_rcnn-CSDN博客">【精选】【目标检测算法】R-CNN（详解）_rcnn-CSDN博客</a><a href="https://zhuanlan.zhihu.com/p/23006190" rel="nofollow" title="RCNN- 将CNN引入目标检测的开山之作 - 知乎 (zhihu.com)">RCNN- 将CNN引入目标检测的开山之作 - 知乎 (zhihu.com)</a>                                                                <a href="https://zhuanlan.zhihu.com/p/411241526" rel="nofollow" title="RCNN学习总结 - 知乎 (zhihu.com)">RCNN学习总结 - 知乎 (zhihu.com)</a>                                                                                                      <a href="https://blog.csdn.net/m0_37970224/article/details/86022390?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=rcnn%E4%B8%ADsvm%E5%88%86%E7%B1%BB&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-86022390.142%5Ev96%5Epc_search_result_base6&amp;spm=1018.2226.3001.4187" title="R-CNN算法学习（步骤三：SVM分类）_cnn特征给svm分为哪些类别-CSDN博客">R-CNN算法学习（步骤三：SVM分类）_cnn特征给svm分为哪些类别-CSDN博客</a>                               <a href="https://blog.csdn.net/weixin_43702653/article/details/123973629?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169797748916777224456504%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=169797748916777224456504&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-123973629-null-null.142%5Ev96%5Epc_search_result_base6&amp;utm_term=rcnn%E8%AF%A6%E8%A7%A3&amp;spm=1018.2226.3001.4187" title="R-CNN史上最全讲解_rcnn_江南綿雨的博客-CSDN博客">R-CNN史上最全讲解_rcnn_江南綿雨的博客-CSDN博客</a></p> 
<p>  ​​</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c7957f0d2e362a973aba56d12e02ced7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">c语言中的-100和~100</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/fff8903b0ced295b1544e365c06df54c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">算法刷题总结 (七) 双指针</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>