<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>学习笔记之——基于深度学习的分类网络 - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="学习笔记之——基于深度学习的分类网络" />
<meta property="og:description" content="之前博文介绍了基于深度学习的常用的检测网络《学习笔记之——基于深度学习的目标检测算法》，本博文为常用的CNN分类卷积网络介绍，本博文的主要内容来自于R&amp;C团队的成员的调研报告以及本人的理解~如有不当之处，还请各位看客赐教哈~好，下面进入正题。
目录
引言
分类网络
LeNet
网络结构
测试结果
总结
AlexNet
网络结构
测试结果
网络特点
总结
ZFNet
网络结构
网络特点
训练与测试结果
总结
VGGNet
网络结构
网格特点
训练与测试结果
总结
GoogLeNet
Inception-v1
Inception-v2
Inception-v3
总结
ResNet
网络结构 Residual Block
训练与测试结果
引言 卷积神经网络（CNN）又称卷积网络，通常用做处理图像序列。经典的CNN层包括三个层次，卷积层、激活函数和池化层。通常在输出之前有几个完全连接的层来集成数据并完成分类。本博文将介绍几种经典的分类卷积网络：LeNet-5、AlexNet、ZFNet、VGGNet、GoogLeNet、ResNet。
分类网络 LeNet LeNet-5由Yann LeCun等人于1989年提出，它是一种用于手写体字符识别的非常高效的卷积神经网络，推动了深度学习领域的发展。LeNet5通过巧妙的设计，利用卷积、参数共享、池化等操作提取特征，避免了大量的计算成本，最后再使用全连接神经网络进行分类识别，这个网络是卷积神经网络架构的起点，后续许多网络都以此为范本进行优化。
LeNet-5共有7层，不包含输入，每层都包含可训练参数；每个层有多个特征映射（feature map），每个feature map通过一种卷积滤波器（filter）提取输入的一种特征，最终经过全连接层和Softmax函数完成分类。
网络结构 INPUT层-输入层：首先是数据 INPUT 层，输入图像（0~9的手写体数字）的尺寸统一归一化为32*32，使背景级别（白色）对应-0.1，前景（黑色）对应1.175。这使得输入平均值约为0，而方差约为1，目的是加速学习。32*32的图像大小比MNIST数据集的图片要大一些，这么做的原因是希望潜在的明显特征如笔画断点或角能够出现在最高层特征检测子感受野（receptive field）的中心。因此在训练整个网络之前，需要对28*28的图像加上paddings（即周围填充0）。
C1层-卷积层
输入图片：32*32
卷积核大小：5*5
卷积核种类：6
输出feature map大小：28*28 *注： 输入维度n×n，卷积核大小f×f，填充p，步长s，则输出维度为：
可训练参数：（5*5&#43;1) * 6（每个滤波器5*5=25个unit参数和一个bias参数，一共6个滤波器）
连接数：（5*5&#43;1）*6*28*28=122304
S2层-池化层（下采样层）
输入：28*28
采样区域：2*2
采样方式：4个输入相加，乘以一个可训练参数，再加上一个可训练偏置。结果通过sigmoid。
*注：sigmoid函数：
采样种类：6
输出featureMap大小：14*14
连接数：（2*2&#43;1）*6*14*14
S2中每个特征图的大小是C1中特征图大小的1/4。
详细说明：第一次卷积之后紧接着就是池化运算，分别对2*2大小的区域进行池化，于是得到了S2，6个14*14的特征图。S2这个pooling层是对C1中的2*2区域内的像素求和并乘以一个权值系数再加上一个偏置，然后将这个结果再做一次映射。Pooling层的主要作用就是减少数据，降低数据维度的同时保留最重要的信息。在数据减少后，可以减少神经网络的维度和计算量，也可以防止参数太多过拟合。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/c79959648402da3a47a93f1da7422cde/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-10-05T11:39:49+08:00" />
<meta property="article:modified_time" content="2018-10-05T11:39:49+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">学习笔记之——基于深度学习的分类网络</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>       之前博文介绍了基于深度学习的常用的检测网络《<a href="https://blog.csdn.net/gwplovekimi/article/details/82915395">学习笔记之——基于深度学习的目标检测算法</a>》，本博文为常用的CNN分类卷积网络介绍，本博文的主要内容来自于R&amp;C团队的成员的调研报告以及本人的理解~如有不当之处，还请各位看客赐教哈~好，下面进入正题。</p> 
<p> </p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="%E5%BC%95%E8%A8%80-toc" style="margin-left:0px;"><a href="#%E5%BC%95%E8%A8%80" rel="nofollow">引言</a></p> 
<p id="%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C-toc" style="margin-left:0px;"><a href="#%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C" rel="nofollow">分类网络</a></p> 
<p id="LeNet-toc" style="margin-left:40px;"><a href="#LeNet" rel="nofollow">LeNet</a></p> 
<p id="%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-toc" style="margin-left:80px;"><a href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84" rel="nofollow">网络结构</a></p> 
<p id="%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C-toc" style="margin-left:80px;"><a href="#%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C" rel="nofollow">测试结果</a></p> 
<p id="%C2%A0%E6%80%BB%E7%BB%93-toc" style="margin-left:80px;"><a href="#%C2%A0%E6%80%BB%E7%BB%93" rel="nofollow"> 总结</a></p> 
<p id="AlexNet-toc" style="margin-left:40px;"><a href="#AlexNet" rel="nofollow">AlexNet</a></p> 
<p style="margin-left:80px;"><a href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84" rel="nofollow">网络结构</a></p> 
<p style="margin-left:80px;"><a href="#%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C" rel="nofollow">测试结果</a></p> 
<p id="%E7%BD%91%E7%BB%9C%E7%89%B9%E7%82%B9-toc" style="margin-left:80px;"><a href="#%E7%BD%91%E7%BB%9C%E7%89%B9%E7%82%B9" rel="nofollow">网络特点</a></p> 
<p id="%E6%80%BB%E7%BB%93-toc" style="margin-left:80px;"><a href="#%E6%80%BB%E7%BB%93" rel="nofollow">总结</a></p> 
<p id="ZFNet-toc" style="margin-left:40px;"><a href="#ZFNet" rel="nofollow">ZFNet</a></p> 
<p style="margin-left:80px;"><a href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84" rel="nofollow">网络结构</a></p> 
<p style="margin-left:80px;"><a href="#%E7%BD%91%E7%BB%9C%E7%89%B9%E7%82%B9" rel="nofollow">网络特点</a></p> 
<p id="%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C-toc" style="margin-left:80px;"><a href="#%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C" rel="nofollow">训练与测试结果</a></p> 
<p style="margin-left:80px;"><a href="#%E6%80%BB%E7%BB%93" rel="nofollow">总结</a></p> 
<p id="VGGNet-toc" style="margin-left:40px;"><a href="#VGGNet" rel="nofollow">VGGNet</a></p> 
<p style="margin-left:80px;"><a href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84" rel="nofollow">网络结构</a></p> 
<p id="%E7%BD%91%E6%A0%BC%E7%89%B9%E7%82%B9-toc" style="margin-left:80px;"><a href="#%E7%BD%91%E6%A0%BC%E7%89%B9%E7%82%B9" rel="nofollow">网格特点</a></p> 
<p style="margin-left:80px;"><a href="#%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C" rel="nofollow">训练与测试结果</a></p> 
<p style="margin-left:80px;"><a href="#%E6%80%BB%E7%BB%93" rel="nofollow">总结</a></p> 
<p id="GoogLeNet-toc" style="margin-left:40px;"><a href="#GoogLeNet" rel="nofollow">GoogLeNet</a></p> 
<p id="Inception-v1-toc" style="margin-left:80px;"><a href="#Inception-v1" rel="nofollow">Inception-v1</a></p> 
<p id="Inception-v2-toc" style="margin-left:80px;"><a href="#Inception-v2" rel="nofollow">Inception-v2</a></p> 
<p id="Inception-v3-toc" style="margin-left:80px;"><a href="#Inception-v3" rel="nofollow">Inception-v3</a></p> 
<p style="margin-left:80px;"><a href="#%E6%80%BB%E7%BB%93" rel="nofollow">总结</a></p> 
<p id="ResNet-toc" style="margin-left:40px;"><a href="#ResNet" rel="nofollow">ResNet</a></p> 
<p id="%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%20Residual%20Block-toc" style="margin-left:80px;"><a href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%20Residual%20Block" rel="nofollow">网络结构 Residual Block</a></p> 
<p style="margin-left:80px;"><a href="#%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C" rel="nofollow">训练与测试结果</a></p> 
<hr id="hr-toc"> 
<p> </p> 
<h2 id="%E5%BC%95%E8%A8%80">引言</h2> 
<p style="margin-left:0cm;">      卷积神经网络（CNN）又称卷积网络，通常用做处理图像序列。经典的CNN层包括三个层次，卷积层、激活函数和池化层。通常在输出之前有几个完全连接的层来集成数据并完成分类。本博文将介绍几种经典的分类卷积网络：LeNet-5、AlexNet、ZFNet、VGGNet、GoogLeNet、ResNet。</p> 
<p style="margin-left:0cm;"> </p> 
<h2 id="%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C" style="margin-left:0cm;">分类网络</h2> 
<h3 id="LeNet" style="margin-left:0cm;">LeNet</h3> 
<p>       LeNet-5<span style="color:#444444;">由</span>Yann LeCun等人于1989年提出<span style="color:#444444;">，它是一种用于</span><strong><span style="color:#ff0000;">手写体字符识别</span></strong><span style="color:#444444;">的非常高效的卷积神经网络，</span>推动了深度学习领域的发展。LeNet5通过巧妙的设计，利用<strong><span style="color:#ff0000;">卷积</span></strong>、<strong><span style="color:#ff0000;">参数共享</span></strong>、<strong><span style="color:#ff0000;">池化</span></strong>等操作提取特征，避免了大量的计算成本，最后再使用全连接神经网络进行分类识别，这个网络是卷积神经网络架构的起点，后续许多网络都以此为范本进行优化。</p> 
<p style="text-align:center;"><img alt="" class="has" height="183" src="https://images2.imgbox.com/a3/86/gj24DhPT_o.png" width="649"></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">        LeNet-5</span><span style="color:#444444;">共有</span><strong><span style="color:#ff0000;">7</span></strong><span style="color:#444444;">层，不包含输入，每层都包含可训练参数；每个层有多个特征映射（</span><span style="color:#444444;">feature map</span><span style="color:#444444;">），每个</span><span style="color:#444444;">feature map</span><span style="color:#444444;">通过一种卷积滤波器（</span><span style="color:#444444;">filter</span><span style="color:#444444;">）提取输入的一种特征，最终经过全连接层和</span><span style="color:#444444;">Softmax</span><span style="color:#444444;">函数完成分类。</span></p> 
<p style="margin-left:0cm;"> </p> 
<h4 id="%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><strong>网络结构</strong></h4> 
<p style="margin-left:0cm;"><strong><span style="color:#ff0000;">INPUT</span></strong><strong><span style="color:#ff0000;">层</span></strong><strong><span style="color:#ff0000;">-</span></strong><strong><span style="color:#ff0000;">输入层：</span></strong><span style="color:#444444;">首先是数据 INPUT </span><span style="color:#444444;">层，输入图像（</span><span style="color:#444444;">0~9</span><span style="color:#444444;">的手写体数字）的尺寸统一归一化为</span><span style="color:#444444;">32*32</span><span style="color:#444444;">，使背景级别（白色）对应</span><span style="color:#444444;">-0.1</span><span style="color:#444444;">，前景（黑色）对应</span><span style="color:#444444;">1.175</span><span style="color:#444444;">。这使得输入平均值约为</span><span style="color:#444444;">0</span><span style="color:#444444;">，而方差约为</span><span style="color:#444444;">1</span><span style="color:#444444;">，目的是</span><strong><span style="color:#ff0000;">加速学习</span></strong><span style="color:#444444;">。</span><span style="color:#444444;">32*32</span><span style="color:#444444;">的图像大小比</span><span style="color:#444444;">MNIST</span><span style="color:#444444;">数据集的图片要大一些，这么做的原因是</span><strong><span style="color:#ff0000;">希望潜在的明显特征如笔画断点或角能够出现在最高层特征检测子感受野（</span></strong><strong><span style="color:#ff0000;">receptive field</span></strong><strong><span style="color:#ff0000;">）的中心</span></strong><span style="color:#444444;">。因此在训练整个网络之前，需要对</span><span style="color:#444444;">28*28</span><span style="color:#444444;">的图像加上</span><span style="color:#444444;">paddings</span><span style="color:#444444;">（即周围填充</span><span style="color:#444444;">0</span><span style="color:#444444;">）。</span></p> 
<p style="text-align:center;"><img alt="" class="has" height="354" src="https://images2.imgbox.com/ea/97/BhyEdJER_o.png" width="715"></p> 
<p style="margin-left:0cm;"><strong><span style="color:#ff0000;">C1</span></strong><strong><span style="color:#ff0000;">层</span></strong><strong><span style="color:#ff0000;">-</span></strong><strong><span style="color:#ff0000;">卷积层</span></strong></p> 
<p style="text-indent:50px;"><span style="color:#444444;">输入图片：</span><span style="color:#444444;">32*32</span></p> 
<p style="text-indent:50px;"><span style="color:#444444;">卷积核大小：</span><span style="color:#444444;">5*5</span></p> 
<p style="text-indent:50px;"><span style="color:#444444;">卷积核种类：</span><span style="color:#444444;">6</span></p> 
<p style="text-indent:50px;"><span style="color:#444444;">输出</span><span style="color:#444444;">feature map</span><span style="color:#444444;">大小：</span><span style="color:#444444;">28*28 </span></p> 
<p style="margin-left:0cm;"><em><span style="color:#e36c0a;">*</span></em><em><span style="color:#e36c0a;">注：</span></em><em> </em><em><span style="color:#e36c0a;">输入维度</span></em><em><span style="color:#e36c0a;">n</span></em><em><span style="color:#e36c0a;">×</span></em><em><span style="color:#e36c0a;">n</span></em><em><span style="color:#e36c0a;">，卷积核大小</span></em><em><span style="color:#e36c0a;">f</span></em><em><span style="color:#e36c0a;">×</span></em><em><span style="color:#e36c0a;">f</span></em><em><span style="color:#e36c0a;">，填充</span></em><em><span style="color:#e36c0a;">p</span></em><em><span style="color:#e36c0a;">，步长</span></em><em><span style="color:#e36c0a;">s</span></em><em><span style="color:#e36c0a;">，则输出维度为：</span></em></p> 
<p style="text-align:center;"><img alt="" class="has" height="32" src="https://images2.imgbox.com/7b/a8/FsqXdrJw_o.png" width="103"></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">可训练参数：（</span><span style="color:#444444;">5*5+1) * 6</span><span style="color:#444444;">（每个滤波器</span><span style="color:#444444;">5*5=25</span><span style="color:#444444;">个</span><span style="color:#444444;">unit</span><span style="color:#444444;">参数和一个</span><span style="color:#444444;">bias</span><span style="color:#444444;">参数，一共</span><span style="color:#444444;">6</span><span style="color:#444444;">个滤波器）</span></p> 
<p><span style="color:#444444;">连接数：（</span><span style="color:#444444;">5*5+1</span><span style="color:#444444;">）</span><span style="color:#444444;">*6*28*28=122304</span></p> 
<p style="text-align:center;"><img alt="" class="has" height="186" src="https://images2.imgbox.com/ec/f3/qzgsScwI_o.png" width="416"></p> 
<p style="margin-left:0cm;"><strong><span style="color:#ff0000;">S2</span></strong><strong><span style="color:#ff0000;">层</span></strong><strong><span style="color:#ff0000;">-</span></strong><strong><span style="color:#ff0000;">池化层（下采样层）</span></strong></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">输入：</span><span style="color:#444444;">28*28</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">采样区域：</span><span style="color:#444444;">2*2</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">采样方式：</span><span style="color:#444444;">4</span><span style="color:#444444;">个输入相加，乘以一个可训练参数，再加上一个可训练偏置。结果通过</span><strong><span style="color:#ff0000;">sigmoid</span></strong><span style="color:#444444;">。</span></p> 
<p style="text-indent:50px;"><em><span style="color:#e36c0a;">*</span></em><em><span style="color:#e36c0a;">注：</span></em><em><span style="color:#e36c0a;">sigmoid</span></em><em><span style="color:#e36c0a;">函数：</span></em><img alt="" class="has" height="27" src="https://images2.imgbox.com/f2/0f/xS9eFxxF_o.png" width="87"></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">采样种类：</span><span style="color:#444444;">6</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">输出</span><span style="color:#444444;">featureMap</span><span style="color:#444444;">大小：</span><span style="color:#444444;">14*14</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">连接数：（</span><span style="color:#444444;">2*2+1</span><span style="color:#444444;">）</span><span style="color:#444444;">*6*14*14</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">S2</span><span style="color:#444444;">中每个特征图的大小是</span><span style="color:#444444;">C1</span><span style="color:#444444;">中特征图大小的</span><span style="color:#444444;">1/4</span><span style="color:#444444;">。</span></p> 
<p style="text-align:center;"><img alt="" class="has" height="215" src="https://images2.imgbox.com/25/7f/qTL9BikJ_o.png" width="440"></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">       详细说明：第一次卷积之后紧接着就是池化运算，分别对</span><span style="color:#444444;">2*2</span><span style="color:#444444;">大小的区域进行池化，于是得到了</span><span style="color:#444444;">S2</span><span style="color:#444444;">，</span><span style="color:#444444;">6</span><span style="color:#444444;">个</span><span style="color:#444444;">14*14</span><span style="color:#444444;">的特征图。</span><span style="color:#444444;">S2</span><span style="color:#444444;">这个</span><span style="color:#444444;">pooling</span><span style="color:#444444;">层是对</span><span style="color:#444444;">C1</span><span style="color:#444444;">中的</span><span style="color:#444444;">2*2</span><span style="color:#444444;">区域内的像素求和并乘以一个权值系数再加上一个偏置，然后将这个结果再做一次映射。</span><em><span style="color:#e36c0a;">Pooling</span></em><em><span style="color:#e36c0a;">层的主要作用就是减少数据，降低数据维度的同时保留最重要的信息。在数据减少后，可以减少神经网络的维度和计算量，也可以防止参数太多过拟合。</span></em></p> 
<p style="margin-left:0cm;"><strong><span style="color:#ff0000;">C3</span></strong><strong><span style="color:#ff0000;">层</span></strong><strong><span style="color:#ff0000;">-</span></strong><strong><span style="color:#ff0000;">卷积层</span></strong></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">输入：</span><span style="color:#444444;">S2</span><span style="color:#444444;">的</span><span style="color:#444444;">feature map</span><span style="color:#444444;">组合</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">卷积核大小：</span><span style="color:#444444;">5*5</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">卷积核种类：</span><span style="color:#444444;">16</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">输出</span><span style="color:#444444;">featureMap</span><span style="color:#444444;">大小：</span><span style="color:#444444;">10*10 </span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">可训练参数：</span><span style="color:#444444;">6*(3*5*5+1)+6*(4*5*5+1)+3*(4*5*5+1)+1*(6*5*5+1)=1516</span></p> 
<p><span style="color:#444444;">连接数：</span><span style="color:#444444;">10*10*1516=151600</span></p> 
<p style="text-align:center;"><img alt="" class="has" height="169" src="https://images2.imgbox.com/17/7e/oHyC2iwv_o.png" width="414"></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">C3</span><span style="color:#444444;">层是一个卷积层，卷积和和</span><span style="color:#444444;">C1</span><span style="color:#444444;">相同，卷积核大小依然为</span><span style="color:#444444;">5*5</span><span style="color:#444444;">，不同的是</span><span style="color:#444444;">C3</span><span style="color:#444444;">的每个节点与</span><span style="color:#444444;">S2</span><span style="color:#444444;">中的多个图相连。每个图与</span><span style="color:#444444;">S2</span><span style="color:#444444;">层的连接的方式如上表所示。这种不对称的组合连接的方式</span><strong><span style="color:#ff0000;">有利于提取多种组合特征</span></strong><span style="color:#444444;">。</span><span style="color:#444444;">C3</span><span style="color:#444444;">的前</span><span style="color:#444444;">6</span><span style="color:#444444;">个</span><span style="color:#444444;">feature map</span><span style="color:#444444;">（对应上图第一个红框的</span><span style="color:#444444;">6</span><span style="color:#444444;">列）与</span><span style="color:#444444;">S2</span><span style="color:#444444;">层相连的</span><span style="color:#444444;">3</span><span style="color:#444444;">个</span><span style="color:#444444;">feature map</span><span style="color:#444444;">相连接（上图第一个红框，每一列中</span><span style="color:#444444;">X</span><span style="color:#444444;">的数量为</span><em><span style="color:#e36c0a;">3</span></em><span style="color:#444444;">），后面</span><span style="color:#444444;">6</span><span style="color:#444444;">个</span><span style="color:#444444;">feature map</span><span style="color:#444444;">与</span><span style="color:#444444;">S2</span><span style="color:#444444;">层相连的</span><span style="color:#444444;">4</span><span style="color:#444444;">个</span><span style="color:#444444;">feature map</span><span style="color:#444444;">相连接（上图第二个红框，每一列中</span><span style="color:#444444;">X</span><span style="color:#444444;">的数量为</span><em><span style="color:#e36c0a;">4</span></em><span style="color:#444444;">），后面</span><span style="color:#444444;">3</span><span style="color:#444444;">个</span><span style="color:#444444;">feature map</span><span style="color:#444444;">与</span><span style="color:#444444;">S2</span><span style="color:#444444;">层部分不相连的</span><span style="color:#444444;">4</span><span style="color:#444444;">个</span><span style="color:#444444;">feature map</span><span style="color:#444444;">相连接，最后一个与</span><span style="color:#444444;">S2</span><span style="color:#444444;">层的所有</span><span style="color:#444444;">feature map</span><span style="color:#444444;">相连。总共有</span><span style="color:#444444;">6*(</span><em><span style="color:#e36c0a;">3</span></em><span style="color:#444444;">*5*5+1)+6*(</span><em><span style="color:#e36c0a;">4</span></em><span style="color:#444444;">*5*5+1)+3*(</span><em><span style="color:#e36c0a;">4</span></em><span style="color:#444444;">*5*5+1)+1*(</span><em><span style="color:#e36c0a;">6</span></em><span style="color:#444444;">*5*5+1)=1516</span><span style="color:#444444;">个参数。而图像大小为</span><span style="color:#444444;">10*10</span><span style="color:#444444;">，所以共有</span><span style="color:#444444;">151600</span><span style="color:#444444;">个连接。</span></p> 
<p style="margin-left:0cm;">      <span style="color:#444444;">C3</span><span style="color:#444444;">与</span><span style="color:#444444;">S2</span><span style="color:#444444;">中前</span><span style="color:#444444;">3</span><span style="color:#444444;">个图相连的卷积结构如下图所示：</span></p> 
<p style="text-align:center;"><img alt="" class="has" height="259" src="https://images2.imgbox.com/47/ba/rPqbLzL8_o.png" width="429"></p> 
<p style="margin-left:0cm;"><strong><span style="color:#ff0000;">S4</span></strong><strong><span style="color:#ff0000;">层</span></strong><strong><span style="color:#ff0000;">-</span></strong><strong><span style="color:#ff0000;">池化层（下采样层）</span></strong></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">输入：</span><span style="color:#444444;">10*10</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">采样区域：</span><span style="color:#444444;">2*2</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">采样方式：</span><span style="color:#444444;">4</span><span style="color:#444444;">个输入相加，乘以一个可训练参数，再加上一个可训练偏置。结果通过</span><span style="color:#444444;">sigmoid</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">采样种类：</span><span style="color:#444444;">16</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">输出</span><span style="color:#444444;">featureMap</span><span style="color:#444444;">大小：</span><span style="color:#444444;">5*5</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">神经元数量：</span><span style="color:#444444;">5*5*16=400</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">连接数：</span><span style="color:#444444;">16*</span><span style="color:#444444;">（</span><span style="color:#444444;">2*2+1</span><span style="color:#444444;">）</span><span style="color:#444444;">*5*5=2000</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">S4</span><span style="color:#444444;">中每个特征图的大小是</span><span style="color:#444444;">C3</span><span style="color:#444444;">中特征图大小的</span><span style="color:#444444;">1/4</span></p> 
<p style="margin-left:0cm;"><strong><span style="color:#ff0000;">C5</span></strong><strong><span style="color:#ff0000;">层</span></strong><strong><span style="color:#ff0000;">-</span></strong><strong><span style="color:#ff0000;">卷积层</span></strong></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">输入：</span><span style="color:#444444;">S4</span><span style="color:#444444;">层的全部</span><span style="color:#444444;">16</span><span style="color:#444444;">个单元特征</span><span style="color:#444444;">map</span><span style="color:#444444;">（与</span><span style="color:#444444;">S4</span><span style="color:#444444;">全相连）</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">卷积核大小：</span><span style="color:#444444;">5*5</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">卷积核种类：</span><span style="color:#444444;">120</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">输出</span><span style="color:#444444;">featureMap</span><span style="color:#444444;">大小：</span><span style="color:#444444;">1*1</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">可训练参数</span><span style="color:#444444;">/</span><span style="color:#444444;">连接：</span><span style="color:#444444;">120*</span><span style="color:#444444;">（</span><span style="color:#444444;">16*5*5+1</span><span style="color:#444444;">）</span><span style="color:#444444;">=48120</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">详细说明：</span><span style="color:#444444;">C5</span><span style="color:#444444;">层是一个卷积层。由于</span><span style="color:#444444;">S4</span><span style="color:#444444;">层的</span><span style="color:#444444;">16</span><span style="color:#444444;">个图的大小为</span><span style="color:#444444;">5</span><span style="color:#444444;">×</span><span style="color:#444444;">5</span><span style="color:#444444;">，与卷积核的大小相同，所以卷积后形成的图的大小为</span><span style="color:#444444;">1</span><span style="color:#444444;">×</span><span style="color:#444444;">1</span><span style="color:#444444;">。这里形成</span><span style="color:#444444;">120</span><span style="color:#444444;">个卷积结果。每个都与上一层的</span><span style="color:#444444;">16</span><span style="color:#444444;">个图相连。所以共有</span><span style="color:#444444;">(5</span><span style="color:#444444;">×</span><span style="color:#444444;">5</span><span style="color:#444444;">×</span><span style="color:#444444;">16+1)</span><span style="color:#444444;">×</span><span style="color:#444444;">120 = 48120</span><span style="color:#444444;">个参数，同样有</span><span style="color:#444444;">48120</span><span style="color:#444444;">个连接。</span><span style="color:#444444;">C5</span><span style="color:#444444;">层的网络结构如下：</span></p> 
<p style="text-align:center;"><img alt="" class="has" height="315" src="https://images2.imgbox.com/45/31/tzSaSYMm_o.png" width="332"></p> 
<p style="margin-left:0cm;"> </p> 
<p style="margin-left:0cm;"><strong><span style="color:#ff0000;">F6</span></strong><strong><span style="color:#ff0000;">层</span></strong><strong><span style="color:#ff0000;">-</span></strong><strong><span style="color:#ff0000;">全连接层</span></strong></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">输入：</span><span style="color:#444444;">C5</span><span style="color:#444444;">层输出的</span><span style="color:#444444;"> 120</span><span style="color:#444444;">维向量</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">计算方式：计算输入向量和权重向量之间的点积，再加上一个偏置，结果通过</span><span style="color:#444444;">sigmoid</span><span style="color:#444444;">函数输出。</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">可训练参数</span><span style="color:#444444;">:84*(120+1)=10164</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">详细说明：</span><span style="color:#444444;">6</span><span style="color:#444444;">层是全连接层。</span><span style="color:#444444;">F6</span><span style="color:#444444;">层有</span><span style="color:#444444;">84</span><span style="color:#444444;">个节点，对应于一个</span><span style="color:#444444;">7</span><span style="color:#444444;">×</span><span style="color:#444444;">12</span><span style="color:#444444;">的比特图，</span><span style="color:#444444;">-1</span><span style="color:#444444;">表示白色，</span><span style="color:#444444;">1</span><span style="color:#444444;">表示黑色，这样每个符号的比特图的黑白色就对应于一个编码。该层的训练参数和连接数是</span><span style="color:#444444;">(120 + 1)</span><span style="color:#444444;">×</span><span style="color:#444444;">84=10164</span><span style="color:#444444;">。</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">F6</span><span style="color:#444444;">层的连接方式如下：</span></p> 
<p style="text-align:center;"><img alt="" class="has" height="193" src="https://images2.imgbox.com/49/a2/8fQTbxkL_o.png" width="298"></p> 
<p style="margin-left:0cm;"><strong><span style="color:#ff0000;">Output层</span><span style="color:#ff0000;">-</span><span style="color:#ff0000;">全连接层</span></strong></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">Output</span><span style="color:#444444;">层也是全连接层，共有</span><span style="color:#444444;">10</span><span style="color:#444444;">个节点，分别代表数字</span><span style="color:#444444;">0</span><span style="color:#444444;">到</span><span style="color:#444444;">9</span><span style="color:#444444;">，且如果节点</span><span style="color:#444444;">i</span><span style="color:#444444;">的值为</span><span style="color:#444444;">0</span><span style="color:#444444;">，则网络识别的结果是数字</span><span style="color:#444444;">i</span><span style="color:#444444;">。采用的是</span><strong><span style="color:#ff0000;">径向基函数（</span></strong><strong><span style="color:#ff0000;">RBF</span></strong><strong><span style="color:#ff0000;">）</span></strong><span style="color:#444444;">的网络连接方式。假设</span><span style="color:#444444;">x</span><span style="color:#444444;">是上一层的输入，</span><span style="color:#444444;">y</span><span style="color:#444444;">是</span><span style="color:#444444;">RBF</span><span style="color:#444444;">的输出，则</span><span style="color:#444444;">RBF</span><span style="color:#444444;">输出的计算方式是：</span></p> 
<p style="text-align:center;"><img alt="" class="has" height="61" src="https://images2.imgbox.com/d7/53/H9pHJK4N_o.png" width="171"></p> 
<p style="margin-left:0cm;"> </p> 
<p style="margin-left:0cm;"><span style="color:#444444;">上式</span><span style="color:#444444;">w_ij </span><span style="color:#444444;">的值由</span><span style="color:#444444;">i</span><span style="color:#444444;">的比特图编码确定，</span><span style="color:#444444;">i</span><span style="color:#444444;">从</span><span style="color:#444444;">0</span><span style="color:#444444;">到</span><span style="color:#444444;">9</span><span style="color:#444444;">，</span><span style="color:#444444;">j</span><span style="color:#444444;">取值从</span><span style="color:#444444;">0</span><span style="color:#444444;">到</span><span style="color:#444444;">7*12-1</span><span style="color:#444444;">。</span><span style="color:#444444;">RBF</span><span style="color:#444444;">输出的值越接近于</span><span style="color:#444444;">0</span><span style="color:#444444;">，则越接近于</span><span style="color:#444444;">i</span><span style="color:#444444;">，即越接近于</span><span style="color:#444444;">i</span><span style="color:#444444;">的</span><span style="color:#444444;">ASCII</span><span style="color:#444444;">编码图，表示当前网络输入的识别结果是字符</span><span style="color:#444444;">i</span><span style="color:#444444;">。该层有</span><span style="color:#444444;">84</span><span style="color:#444444;">×</span><span style="color:#444444;">10=840</span><span style="color:#444444;">个参数和连接。然后与</span><strong><span style="color:#ff0000;">Softmax</span></strong><span style="color:#444444;">连接，最终输出结果。</span></p> 
<p style="text-align:center;"><img alt="" class="has" height="407" src="https://images2.imgbox.com/8b/fe/S3CzWplm_o.png" width="532"></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">上图是LeNet-5</span><span style="color:#444444;">识别数字</span><span style="color:#444444;">3</span><span style="color:#444444;">的过程。下图是</span><span style="color:#444444;">LeNet-5</span><span style="color:#444444;">识别数字</span><span style="color:#444444;">8</span><span style="color:#444444;">的过程。</span></p> 
<p style="text-align:center;"><img alt="" class="has" height="521" src="https://images2.imgbox.com/9c/84/fuVLac3V_o.png" width="670"></p> 
<p style="margin-left:0cm;"> </p> 
<h4 id="%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C"><strong>测试结果</strong></h4> 
<p style="margin-left:0cm;"><span style="color:#444444;">在</span><span style="color:#444444;">MNIST </span><span style="color:#444444;">中</span><span style="color:#444444;">60000</span><span style="color:#444444;">个训练集上，误差在第</span><span style="color:#444444;">10~12</span><span style="color:#444444;">次迭代时趋近收敛。</span></p> 
<p style="text-align:center;"><img alt="" class="has" height="539" src="https://images2.imgbox.com/06/10/EY8SmXIZ_o.png" width="661"></p> 
<p style="text-align:center;"><img alt="" class="has" height="655" src="https://images2.imgbox.com/25/22/Q83S7To7_o.png" width="657"></p> 
<p style="margin-left:0cm;">        <span style="color:#444444;">随着训练集的扩充，测试误差在减小。在测试集经过人工随机扭曲扩展的情况下，测试误差达到了</span><span style="color:#444444;">0.8%</span><span style="color:#444444;">。</span></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">        LeNet-5</span><span style="color:#444444;">与其他分类算法的</span><strong><span style="color:#ff0000;">分类错误率</span></strong><span style="color:#444444;">结果比较。</span></p> 
<p style="text-align:center;"><img alt="" class="has" height="579" src="https://images2.imgbox.com/ad/37/LdSe2F4F_o.png" width="739"></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">LeNet-5</span><span style="color:#444444;">与其他分类算法的</span><strong><span style="color:#ff0000;">所必需进行的累加乘法操作次数</span></strong><span style="color:#444444;">结果比较。</span></p> 
<p style="text-align:center;"><img alt="" class="has" height="547" src="https://images2.imgbox.com/a5/4e/Bm5ooa1H_o.png" width="641"></p> 
<p style="margin-left:0cm;"><span style="color:#444444;">LeNet-5</span><span style="color:#444444;">与其他分类算法的</span><strong><span style="color:#ff0000;">必需存储变量的空间大小</span></strong><span style="color:#444444;">结果比较。</span></p> 
<p style="text-align:center;"><img alt="" class="has" height="611" src="https://images2.imgbox.com/83/03/Hn6R6gA8_o.png" width="681"></p> 
<h4 id="%C2%A0%E6%80%BB%E7%BB%93" style="margin-left:0cm;"><strong> 总结</strong></h4> 
<p style="margin-left:0cm;"><strong><span style="color:#ff0000;">优点：</span></strong></p> 
<p style="margin-left:0cm;">LeNet-5是一种用于手写体字符识别的非常高效的卷积神经网络。</p> 
<p style="margin-left:0cm;">卷积神经网络能够很好的利用图像的结构信息。</p> 
<p style="margin-left:0cm;">卷积层的参数较少，这也是由卷积层的主要特性即局部连接和共享权重所决定。</p> 
<p style="margin-left:0cm;"><strong><span style="color:#ff0000;">缺点：</span></strong></p> 
<p style="margin-left:0cm;">由于当时缺乏大规模训练数据，计算机的计算能力也跟不上，LeNet-5 对于复杂问题的处理结果并不理想，下图为82个被LeNet-5误判的数字图像，误判原因主要为训练集没有表现以下数字的特征。</p> 
<p style="text-align:center;"><img alt="" class="has" height="615" src="https://images2.imgbox.com/7b/86/VUZcWIvA_o.png" width="642"></p> 
<p style="margin-left:0cm;"><strong><span style="color:#ff0000;">意义：</span></strong>是第一个真正意义上的深度学习网络，也是现在许多卷积神经网络的雏形。在20世纪90年代，LeNet-5被用于欧美许多大银行的自动手写识别系统（读取支票手写数字等），为商业发展提供帮助。</p> 
<p style="margin-left:0cm;"> </p> 
<h3 id="AlexNet">AlexNet</h3> 
<p>      2012年，Alex Krizhevsky、Ilya Sutskever设计出了一个深层的卷积神经网络AlexNet，夺得了<strong><span style="color:#ff0000;">2012</span></strong><strong><span style="color:#ff0000;">年</span></strong><strong><span style="color:#ff0000;">ImageNet LSVRC</span></strong><strong><span style="color:#ff0000;">的冠军</span></strong>，且准确率远超第二名（top5错误率为15.3%，第二名为26.2%），引起了很大的轰动。AlexNet可以说是具有历史意义的一个网络结构，在此之前，深度学习已经沉寂了很长时间，自2012年AlexNet诞生之后，后面的ImageNet冠军都是用卷积神经网络（CNN）来做的，并且层次越来越深，使得CNN成为在图像识别分类的核心算法模型，带来了深度学习的大爆发。</p> 
<p style="text-align:center;"><img alt="" class="has" height="298" src="https://images2.imgbox.com/0f/f6/vKG5Jc5L_o.png" width="775"></p> 
<p> </p> 
<h4><strong>网络结构</strong></h4> 
<p style="margin-left:0cm;">         AlexNet包括了8个参数层（不包括池化层和<strong><span style="color:#ff0000;">局部响应归一化</span></strong><strong><span style="color:#ff0000;">LRN</span></strong>层），5层卷积层和3层全连接层，最后一个全连接层的输出送到一个1000维的Softmax层，产生一个覆盖1000类标签的分布。LRN层出现在第1个及第2个卷积层后，最大池化层（3*3，步长为2）出现在2个LRN层及最后一个卷积层后。<strong><span style="color:#ff0000;">ReLU</span></strong><strong><span style="color:#ff0000;">激活函数</span></strong>应用在这8层每一层后面。</p> 
<p style="text-align:center;"><img alt="" class="has" height="678" src="https://images2.imgbox.com/79/d9/Ce2R0gou_o.png" width="882"></p> 
<p style="margin-left:0cm;">       Local Response Normalization, LRN，局部响应归一化。指在某一层得到了多通道的响应图后，对响应图上某一位置和临近通道的值按照如下公式做归一化：</p> 
<p style="text-align:center;"><img alt="" class="has" height="79" src="https://images2.imgbox.com/27/6c/6oHkzpwo_o.png" width="284"></p> 
<p style="margin-left:0cm;">     其中<img alt="" class="has" height="21" src="https://images2.imgbox.com/50/01/994Olxu0_o.png" width="26">是特征响应feature map图第<img alt="" class="has" height="19" src="https://images2.imgbox.com/75/05/AGIGLIWO_o.png" width="6">个通道上在<img alt="" class="has" height="19" src="https://images2.imgbox.com/2e/da/spVlis3W_o.png" width="37">位置上的值，<em>k</em>、<em>α</em>、<em>β</em>均为超参数（Alex选用的参数为<img alt="" class="has" height="19" src="https://images2.imgbox.com/51/37/aXXyORbx_o.png" width="251">）局部响应归一化模拟的是动物神经中的横向抑制效应。从公式中可看出，如果在该位置，该通道和临近通道的绝对值都比较大的话，归一化之后值会有变得更小的趋势。LRN示意图如下图所示，红色的feature map中特定的位置与其邻近通道上的相同位置的值进行归一化。</p> 
<p style="text-align:center;"><img alt="" class="has" height="367" src="https://images2.imgbox.com/b7/a9/4QiEdTM5_o.png" width="451"></p> 
<p><em><span style="color:#e36c0a;">Alex</span><span style="color:#e36c0a;">等人提出</span><span style="color:#e36c0a;">LRN</span><span style="color:#e36c0a;">时指出其对网络分类指标的提升是有帮助的（</span><span style="color:#e36c0a;">top-1</span><span style="color:#e36c0a;">错误率下降</span><span style="color:#e36c0a;">1.4%</span><span style="color:#e36c0a;">，</span><span style="color:#e36c0a;">top-5</span><span style="color:#e36c0a;">错误率下降</span><span style="color:#e36c0a;">1.2%</span><span style="color:#e36c0a;">），然而随着更深层次的网络被提出，</span><span style="color:#e36c0a;">LRN</span><span style="color:#e36c0a;">被认为并没有什么作用，论文</span><span style="color:#e36c0a;">Very Deep Convolutional Networks for Large-scale Image Recognition</span><span style="color:#e36c0a;">指出在</span><span style="color:#e36c0a;">VGGNet</span><span style="color:#e36c0a;">第</span><span style="color:#e36c0a;">11</span><span style="color:#e36c0a;">层的网络中</span><span style="color:#e36c0a;">LRN</span><span style="color:#e36c0a;">已经起了副作用。</span></em></p> 
<p style="text-align:center;"><img alt="" class="has" height="778" src="https://images2.imgbox.com/b5/0b/DUgYiS5I_o.png" width="508"></p> 
<p>      以上两个分组卷积是因为Alex当时的显卡不够强大，为了减少计算量同时方便并行，所以采用了同时在两块GPU上分组计算的方法。<strong><span style="color:#ff0000;">传统的卷积层中，相邻的池化单元是不重叠的</span></strong>。如果步长小于卷积核大小，那么池化层将重叠。在<strong><span style="color:#ff0000;">AlexNet</span></strong>中<strong><span style="color:#ff0000;">使用了</span></strong><strong><span style="color:#ff0000;">MAXPooling</span></strong>，步长为2，卷积核大小为3。<strong><span style="color:#ff0000;">论文指出，这种重叠的池化层能“稍微”减轻过拟合</span></strong>。</p> 
<p style="text-align:center;"><img alt="" class="has" height="420" src="https://images2.imgbox.com/5f/a6/OZZiaMMk_o.png" width="302"></p> 
<p style="margin-left:0cm;">       在fc6、fc7两层，AlexNet使用了Dropout方法来避免过拟合。具体操作是每个神经元有50%的概率被设置为没有响应，这使得对于每次训练，神经网络的体系结构都会不同，使得网络更加健壮。</p> 
<p style="text-align:center;"><img alt="" class="has" height="567" src="https://images2.imgbox.com/0c/4e/sqkz7Ne2_o.png" width="500"></p> 
<h4><strong>测试结果</strong></h4> 
<p style="text-align:center;"><img alt="" class="has" height="212" src="https://images2.imgbox.com/13/5b/B71HUQJk_o.png" width="738"></p> 
<h4 id="%E7%BD%91%E7%BB%9C%E7%89%B9%E7%82%B9"><strong>网络特点</strong></h4> 
<p style="text-align:center;"><img alt="" class="has" height="267" src="https://images2.imgbox.com/62/c3/IcMAURSw_o.png" width="332"></p> 
<ol><li>使用非线性激活函数：ReLU</li><li>防止过拟合的方法：Dropout，Data augmentation</li><li>大数据训练：百万级ImageNet图像数据</li><li>其他：GPU实现，LRN归一化层的使用</li></ol> 
<p style="margin-left:0cm;"> <em><span style="color:#e36c0a;">For 1</span></em><em><span style="color:#e36c0a;">：</span></em> Sigmoid 是常用的非线性的激活函数，它能够把输入的连续实值“压缩”到0和1之间。特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1。LeNet-5使用的激活函数便是Sigmoid函数。然而，<strong><span style="color:#ff0000;">Sigmoid</span></strong><strong><span style="color:#ff0000;">函数有两个致命的缺点</span></strong>：1）当输入非常大或者非常小的时候，会有饱和现象，这些神经元的梯度是接近于0的。如果初始值很大的话，梯度在反向传播的时候因为需要乘上一个sigmoid 的导数，所以会使得梯度越来越小，这会<strong><span style="color:#ff0000;">导致网络变的很难学习</span></strong>。2）<strong><span style="color:#ff0000;">Sigmoid </span></strong><strong><span style="color:#ff0000;">的</span></strong><strong> </strong><strong><span style="color:#ff0000;">输出不是</span></strong><strong><span style="color:#ff0000;">0</span></strong><strong><span style="color:#ff0000;">均值</span></strong>。这是不可取的，因为这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。产生的一个结果就是：如果数据进入神经元的时候是正的，那么计算出的梯度也会始终都是正的。<strong><span style="color:#ff0000;">Alex</span></strong><strong><span style="color:#ff0000;">用</span></strong><strong><span style="color:#ff0000;">ReLU</span></strong><strong><span style="color:#ff0000;">代替了</span></strong><strong><span style="color:#ff0000;">Sigmoid</span></strong><strong><span style="color:#ff0000;">，发现使用</span></strong><strong><span style="color:#ff0000;"> ReLU </span></strong><strong><span style="color:#ff0000;">得到的</span></strong><strong><span style="color:#ff0000;">SGD</span></strong><strong><span style="color:#ff0000;">的收敛速度会比</span></strong><strong><span style="color:#ff0000;">sigmoid/tanh </span></strong><strong><span style="color:#ff0000;">快很多</span></strong>。如下图，在 CIFAR-10测试集上，实线代表使用ReLU作为激活函数的训练误差曲线，虚线代表使用tanh作为激活函数的训练误差曲线，ReLU收敛速度远快于tanh。主要是因为它是linear，而且 non-saturating（因为ReLU的导数始终是1），相比于sigmoid/tanh，ReLU 只需要一个阈值就可以得到激活值，而不用去算一大堆复杂的运算。</p> 
<p><em><span style="color:#e36c0a;">For 2</span><span style="color:#e36c0a;">：</span> </em>对<strong><span style="color:#ff0000;">原始图片进行扩充以减少过拟合</span></strong>Data augmentation，方法有：从原始图像（256,256）中，随机的裁剪出一些图像（224,224）。水平翻转图像。给图像增加一些随机的光照。</p> 
<h4 id="%E6%80%BB%E7%BB%93"><strong>总结</strong></h4> 
<p style="margin-left:0cm;"><strong><span style="color:#ff0000;">优点：</span></strong>数据增强，采用了Dropout、Data augmentation避免过拟合，采用ReLU整流线性单元代替Sigmoid函数，第一次采用GPU加速，使得训练网络的速度大大提高。在ILSVRC2012的比赛中取得了优异的成绩。</p> 
<p style="margin-left:0cm;">  <strong><span style="color:#ff0000;">意义：</span></strong>它证明了CNN在复杂模型下的有效性，然后利用GPU加速使得训练在可接受的时间范围内得到结果。AlexNet对几年前的神经网络结构影响深远，2013年ILVRC的冠军结构ZFNet、亚军结构VGGNet都是以AlexNet为基础的。从这一年开始，几乎所有参加ILVRC的参赛者都开始使用卷积神经网络，少数没有使用深度卷积神经网络的参赛者都处于垫底位置。AlexNet的提出确实让CNN和GPU的概念变得流行，且推动了监督学习的发展。</p> 
<p> </p> 
<h3 id="ZFNet">ZFNet</h3> 
<p style="margin-left:0cm;">        ZFNet是由纽约大学的Matthew Zeiler和Rob Fergus所设计，该网络<strong><span style="color:#ff0000;">在</span></strong><strong><span style="color:#ff0000;">AlexNet</span></strong><strong><span style="color:#ff0000;">上进行了微小的改进</span></strong>，但这篇文章主要贡献在于在一定程度上解释了卷积神经网络为什么有效，以及如何提高网络的性能。该网络的贡献在于：1）使用了<strong><span style="color:#ff0000;">反卷积网络</span></strong>，可视化了特征图。通过特征图证明了浅层网络学习到了图像的边缘、颜色和纹理特征，高层网络学习到了图像的抽象特征；2）根据<strong><span style="color:#ff0000;">特征可视化</span></strong>，提出AlexNet第一个卷积层卷积核太大，导致提取到的特征模糊；3）通过几组<strong><span style="color:#ff0000;">遮挡实验</span></strong>，对比分析找出了图像的关键部位；4）论证了<strong><span style="color:#ff0000;">更深的网络模型，具有更好的性能</span></strong>。</p> 
<p> </p> 
<h4><strong>网络结构</strong></h4> 
<p style="text-align:center;"><img alt="" class="has" height="174" src="https://images2.imgbox.com/57/fb/hao72J5D_o.png" width="706"></p> 
<p style="margin-left:0cm;">ZFNet大体保留了AlexNet的结构，而通过特征可视化发现了AlexNet的一些不足之处，对其中一些参数进行了调整：由于AlexNet第一层卷积核混杂了大量的高频和低频信息，缺少中频信息，故ZFNet将第1层卷积核的大小由11×11调整为<strong><span style="color:#ff0000;">7</span></strong><strong><span style="color:#ff0000;">×</span></strong><strong><span style="color:#ff0000;">7</span></strong>；<em><span style="color:#e36c0a;">b</span></em><em><span style="color:#e36c0a;">、</span></em><em><span style="color:#e36c0a;">c</span></em><em><span style="color:#e36c0a;">分别为</span></em><em><span style="color:#e36c0a;">AlexNet</span></em><em><span style="color:#e36c0a;">、</span></em><em><span style="color:#e36c0a;">ZFNet</span></em><em><span style="color:#e36c0a;">第</span></em><em><span style="color:#e36c0a;">1</span></em><em><span style="color:#e36c0a;">层所提取的特征。</span></em></p> 
<p style="text-align:center;"><img alt="" class="has" height="358" src="https://images2.imgbox.com/df/c6/DiXLXaP2_o.png" width="224"></p> 
<p style="margin-left:0cm;">      由于AlexNet第2层卷积过程选择4作为步长，产生了混乱无用的特征，故ZFNet将卷积步长由4调整为<strong><span style="color:#ff0000;">2</span></strong>。<em><span style="color:#e36c0a;">d</span></em><em><span style="color:#e36c0a;">、</span></em><em><span style="color:#e36c0a;">e</span></em><em><span style="color:#e36c0a;">分别为</span></em><em><span style="color:#e36c0a;">AlexNet</span></em><em><span style="color:#e36c0a;">、</span></em><em><span style="color:#e36c0a;">ZFNet</span></em><em><span style="color:#e36c0a;">第</span></em><em><span style="color:#e36c0a;">2</span></em><em><span style="color:#e36c0a;">层所提取的特征，后者没有前者中的模糊特征。</span></em></p> 
<p style="text-align:center;"><img alt="" class="has" height="353" src="https://images2.imgbox.com/1d/9f/dL8rvdXn_o.png" width="710"></p> 
<h4><strong>网络特点</strong></h4> 
<p style="margin-left:0cm;">       反卷积网络Deconvnet：用于<strong><span style="color:#ff0000;">了解（可视化）卷积网络中间层的</span></strong><strong><span style="color:#ff0000;">feature map</span></strong>。在ZFNet中，卷积网络的每一层都附加了一个反卷积层，提供了一条由输出feature map到输入图像的反通路。首先，输入图像通过卷积网络，每层都会产生特定feature map，而后将反卷积网络中观测层的其他连接权值全部置零，将卷积网络观测层产生的feature map当作输入，传送给对应的反卷积层，并依次进行i.unpooling，ii.矫正，iii.反卷积。</p> 
<p style="text-align:center;"><img alt="" class="has" height="701" src="https://images2.imgbox.com/4d/f9/xng8rUz3_o.png" width="718"></p> 
<p style="margin-left:0cm;">     unpooling: 在MAXpooling的过程中，用Switches表格记录下每个pooling区域中的最大值，在unpooling过程中，将最大值标注回记录所在位置，<strong><span style="color:#ff0000;">其余位置填充</span></strong><strong><span style="color:#ff0000;">0</span></strong>。</p> 
<p style="margin-left:0cm;">      矫正：在卷积网络中，使用ReLU作为激活函数保证所有输出都为非负数，这个约束对反卷积过程依然成立，因此<strong><span style="color:#ff0000;">将重构信号同样传输入</span></strong><strong><span style="color:#ff0000;">ReLU</span></strong><strong><span style="color:#ff0000;">中</span></strong>。</p> 
<p style="margin-left:0cm;">        反卷积：卷积网络使用学习得到的卷积核与上层输出feature map做卷积得到该层输出的feature map，为了实现逆过程，反卷积网络使用<strong><span style="color:#ff0000;">相同卷积核的转置</span></strong>作为核，与矫正后的feature map进行卷积运算。</p> 
<p style="text-align:center;"><img alt="" class="has" height="325" src="https://images2.imgbox.com/ee/da/lEs4KvT4_o.png" width="692"></p> 
<p style="text-align:center;"><img alt="" class="has" height="283" src="https://images2.imgbox.com/77/86/yXlNdTpT_o.png" width="626"></p> 
<p style="text-align:center;"><img alt="" class="has" height="479" src="https://images2.imgbox.com/9e/c2/UJrcZUIj_o.png" width="638"></p> 
<p style="text-align:center;"><img alt="" class="has" height="87" src="https://images2.imgbox.com/ca/3d/c0bf04am_o.png" width="786"></p> 
<p style="margin-left:0cm;">      特征不变性：如图，5张不同的图片分别被平移、旋转、缩放。在网络的第1层，输入图片任何小的微变都会导致输出特征变化明显，但随着层数的增加，平移和缩放的变化对最终结果影响越小，但旋转产生的变化并不会减小。这说明卷积网络无法对旋转操作产生不变性，除非物体具有很强的对称性。</p> 
<p style="text-align:center;"><img alt="" class="has" height="658" src="https://images2.imgbox.com/0c/7f/sgMQ9F0u_o.png" width="758"></p> 
<p style="margin-left:0cm;"><img alt="" class="has" height="54" src="https://images2.imgbox.com/60/2d/xyWuUj8U_o.png" width="1032"></p> 
<p style="margin-left:0cm;">遮挡实验：</p> 
<p style="text-align:center;"><img alt="" class="has" height="653" src="https://images2.imgbox.com/93/81/4rOmcMNe_o.png" width="809"></p> 
<p style="margin-left:0cm;">ZFNet展示了输入图片被遮挡的情况。Zeiler等人使用一个灰色矩形对输入图像的每个部分进行遮挡，并测试在不同遮挡情况下，分类器的输出结果，当关键区域发生遮挡时，分类器性能急剧下降。上图中的第1行展示了狗狗图片产生的最强特征，当存在遮挡时，对应输入图片的特征产生刺激强度降低（蓝色区域）。使用部分遮挡的扩展训练集可以使网络产生相关性，提升分类准确度。</p> 
<h4 id="%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C"><strong>训练与测试结果</strong></h4> 
<p style="margin-left:0cm;">      使用ImageNet2012所给的130万张图片进行测试，每张RGB图像被预处理为256×256的大小。ZFNet使用小批量数据SGD随机梯度下降，<strong><span style="color:#ff0000;">mini-batch=128</span></strong><strong><span style="color:#ff0000;">，学习率为</span></strong><strong><span style="color:#ff0000;">10-2</span></strong><strong><span style="color:#ff0000;">，动量为</span></strong><strong><span style="color:#ff0000;">0.9</span></strong>。在第6、第7层全连接层中使用Dropout方法（<strong><span style="color:#ff0000;">Dropout</span></strong><strong><span style="color:#ff0000;">比率为</span></strong><strong><span style="color:#ff0000;">50%</span></strong>）</p> 
<p style="text-align:center;"><img alt="" class="has" height="87" src="https://images2.imgbox.com/c2/5d/zs6AiLu4_o.png" width="363"></p> 
<p style="text-align:center;"><img alt="" class="has" height="253" src="https://images2.imgbox.com/5c/11/IR9byftj_o.png" width="493"></p> 
<p style="margin-left:0cm;"> 利用改进后的ZFNet在ImageNet2012训练集上训练并测试，得出的top-5错误率为14.8%，比AlexNet的15.3%<strong><span style="color:#ff0000;">下降了</span></strong><strong><span style="color:#ff0000;">0.5%</span></strong>。</p> 
<p style="text-align:center;"><img alt="" class="has" height="368" src="https://images2.imgbox.com/b0/d5/fnVoUI3Q_o.png" width="623"></p> 
<p style="margin-left:0cm;">    Zeiler等人测试了改变AlexNet模型的结构对最终分类结果所造成的影响。当第6、7层被完全删除后，错误率上升至22.4%，而删除所有的卷积层后，错误率剧烈上升至50.1%，这说明<strong><span style="color:#ff0000;">模型的深度与分类效果密切相关，深度越大，效果越好</span></strong>，而改变全连接层的节点个数对分类性能影响不大。</p> 
<p style="margin-left:0cm;">为了测试模型泛化能力，Zeiler等人还利用ZFNet测试了Caltch-101，Caltch-256和PASCAL VOC2012共3个库，测试方法为不改变1~7层的训练结果，而只对最深层的softmax分类器重新训练。</p> 
<p style="text-align:center;"><img alt="" class="has" height="242" src="https://images2.imgbox.com/5b/cf/d8onTPGH_o.png" width="617"></p> 
<p style="margin-left:0cm;">比较可知，基于ImageNet学到的特征更有效。</p> 
<p style="text-align:center;"><img alt="" class="has" height="217" src="https://images2.imgbox.com/92/8c/mICs26sn_o.png" width="697"></p> 
<p style="text-align:center;"><img alt="" class="has" height="365" src="https://images2.imgbox.com/be/d4/l5awwDv0_o.png" width="632"></p> 
<p style="margin-left:0cm;">PASCAL库中的测试图片有可能一张包含多个物体，而ZFNet模型一张图片只给出一个预测，所以没能超越历史最好记录，约落后了3.2%。</p> 
<p>通过以上实验，说明使用ImageNet2012数据集训练得到的CNN的<strong><span style="color:#ff0000;">特征提取功能</span></strong>就有<strong><span style="color:#ff0000;">通用性</span></strong>。</p> 
<h4><strong>总结</strong></h4> 
<p style="margin-left:0cm;"><strong><span style="color:#ff0000;">优点</span></strong><strong><span style="color:#ff0000;"> / </span></strong><strong><span style="color:#ff0000;">意义：</span></strong>ZFNet是CNN领域<strong><span style="color:#ff0000;">可视化理解</span></strong>的开山之作。在一定程度上解释了卷积神经网络为什么有效，以及如何提高网络的性能。</p> 
<p style="margin-left:0cm;"><strong> <span style="color:#ff0000;">局限：</span></strong>大体继承了AlexNet的结构，没有提出太多对提升分类准确度有实际帮助的方法。</p> 
<p style="margin-left:0cm;"> </p> 
<h3 id="VGGNet">VGGNet</h3> 
<p>        VGGNet是由牛津大学计算机视觉组和Google DeepMind项目的研究员共同研发的卷积神经网络模型，VGGNet探索了卷积神经网络的深度与其性能之间的关系，通过<strong><span style="color:#ff0000;">反复堆叠</span></strong><strong><span style="color:#ff0000;">3*3</span><span style="color:#ff0000;">的小型卷积核和</span><span style="color:#ff0000;">2*2</span><span style="color:#ff0000;">的最大池化层</span></strong>，VGGNet成功地构筑了16~19层深的卷积神经网络。该模型取得了ILSVRC2014比赛分类项目的第2名和定位项目的第1名。</p> 
<p> </p> 
<h4><strong>网络结构</strong></h4> 
<p style="margin-left:0cm;">        VGG由5层卷积层、3层全连接层、softmax输出层构成，层与层之间使用max-pooling（最大化池）分开，所有隐层的激活单元都采用ReLU函数。<span style="color:#0000ff;"><u><a href="http://ethereon.github.io/netscope/#/gist/dc5003de6943ea5a6b8b" rel="nofollow">VGG16</a></u></span></p> 
<p style="margin-left:0cm;">      在训练期间，VGGNet的输入是固定大小的224×224 RGB图像。预处理是从每个像素中减去在训练集上计算的RGB均值。图像通过一堆卷积层，VGGNet使用<strong><span style="color:#ff0000;">感受野很小的滤波器：</span></strong><strong><span style="color:#ff0000;">3</span></strong><strong><span style="color:#ff0000;">×</span></strong><strong><span style="color:#ff0000;">3</span></strong>（这是捕获左/右，上/下，中心概念的最小尺寸）。在其中一种配置中，我们还使用了1×1卷积滤波器，可以看作输入通道的线性变换（后面是非线性）。<strong><span style="color:#ff0000;">卷积步长固定为</span></strong><strong><span style="color:#ff0000;">1</span></strong><strong><span style="color:#ff0000;">个像素</span></strong>；卷积层输入的空间填充要满足卷积之后保留空间分辨率，即3×3卷积层的填充为1个像素。空间池化由五个<strong><span style="color:#ff0000;">最大池化</span></strong>层进行，这些层在一些卷积层之后（不是所有的卷积层之后都是最大池化）。在<strong><span style="color:#ff0000;">2</span></strong><strong><span style="color:#ff0000;">×</span></strong><strong><span style="color:#ff0000;">2</span></strong><strong><span style="color:#ff0000;">像素窗口上进行最大池化，步长为</span></strong><strong><span style="color:#ff0000;">2</span></strong>。各层参数如下图所示。</p> 
<p style="text-align:center;"><img alt="" class="has" height="720" src="https://images2.imgbox.com/b6/cc/TTgeKyti_o.png" width="689"></p> 
<p style="text-align:center;"><img alt="" class="has" height="85" src="https://images2.imgbox.com/17/bb/wsJwfVht_o.png" width="826"></p> 
<p style="margin-left:0cm;"><em><span style="color:#e36c0a;">百万级别</span></em></p> 
<p style="margin-left:0cm;"><strong><span style="color:#ff0000;">VGG16</span></strong>的模型图如下：</p> 
<p style="text-align:center;"><img alt="" class="has" height="747" src="https://images2.imgbox.com/c5/f4/4zYUUR8n_o.png" width="802"></p> 
<p style="margin-left:0cm;">在卷积层（在不同架构中具有不同深度）之后是三个全连接（FC）层：前两个每个都有4096个通道，第三个执行1000维ILSVRC分类，因此包含1000个通道（一个通道对应一个类别）。最后一层是softmax层。所有网络中全连接层的配置是相同的。</p> 
<p style="margin-left:0cm;">所有隐藏层都配备了ReLU。VGGNet网络（除了一个）都不包含局部响应规范化（LRN）因为<strong><span style="color:#ff0000;">这种规范化并不能提高在</span></strong><strong><span style="color:#ff0000;">ILSVRC</span></strong><strong><span style="color:#ff0000;">数据集上的性能</span></strong>，但增加了内存消耗和计算时间。</p> 
<p style="margin-left:0cm;"> </p> 
<h4 id="%E7%BD%91%E6%A0%BC%E7%89%B9%E7%82%B9"><strong>网格特点</strong></h4> 
<p style="margin-left:0cm;">    VGGNet将<strong><span style="color:#ff0000;">小卷积核</span></strong>带入人们的视线，对比AlexNet中第一个卷积层使用的filter大小为11×11，stride为4，C3和C5层中使用的都是5×5的卷积核；而出现在VGGNet大多数的卷积核的大小均为 3×3，stride均为1。将卷积核的大小缩小的好处为1）<strong><span style="color:#ff0000;">减少参数</span></strong>，2）相当于<strong><span style="color:#ff0000;">进行了更多的非线性映射</span></strong>，可以增加网络的拟合/表达能力。</p> 
<p style="margin-left:0cm;"><em><span style="color:#e36c0a;">For 1</span></em><em><span style="color:#e36c0a;">：</span></em><em> </em>假设三层3×3卷积堆叠的输入和输出有C个通道，堆叠卷积层的参数为3(32C2)=27C2个权重；然而，单个7×7卷积层将需要72C2=49C2个参数，即参数多81％。这可以看作是对7×7卷积滤波器进行正则化，迫使它们通过3×3滤波器（在它们之间注入非线性）进行分解。</p> 
<p style="margin-left:0cm;"><em><span style="color:#e36c0a;">For 2</span></em><em><span style="color:#e36c0a;">：</span></em><em> </em>多少个串联的小卷积核就对应着多少次激活的过程，而一个大的卷积核就只有一次激活的过程。引入了更多的非线性变换，也就意味着模型的表达能力会更强，可以去拟合更高维的分布。</p> 
<p style="margin-left:0cm;">    VGG网络的通道数比AlexNet网络的<strong><span style="color:#ff0000;">通道数多</span></strong>，第一层的通道数为64，后面每层都进行了翻倍，最多到512个通道，通道数的增加，使得<strong><span style="color:#ff0000;">更多的信息可以被提取出来</span></strong>。</p> 
<p style="margin-left:0cm;">    VGG结合<strong><span style="color:#ff0000;">1</span></strong><strong><span style="color:#ff0000;">×</span></strong><strong><span style="color:#ff0000;">1</span></strong><strong><span style="color:#ff0000;">卷积层</span></strong>来替换输出前的3个全连接层，以<strong><span style="color:#ff0000;">增加决策函数非线性</span></strong>而不影响卷积层的感受野。这使得测试得到的全卷积网络因为没有全连接的限制，因而<strong><span style="color:#ff0000;">可以接收任意宽或高的输入</span></strong>。VGGNet使用1×1卷积来在相同维度空间上做线性投影（输入和输出通道的数量相同），再由修正函数对结果非线性化。</p> 
<p> </p> 
<h4><strong>训练与测试结果</strong></h4> 
<p>      通过使用具有动量的小批量（mini-batch）梯度下降优化多项式逻辑回归目标函数来进行训练。<strong><span style="color:#ff0000;">批量大小设为</span></strong><strong><span style="color:#ff0000;">256</span></strong>，<strong><span style="color:#ff0000;">动量为</span></strong><strong><span style="color:#ff0000;">0.9</span></strong>。训练通过权重衰减进行L2正则化（weight decay=5×10-4），前两个全连接层使用Dropout方法（<strong><span style="color:#ff0000;">Dropout</span><span style="color:#ff0000;">比率设定为</span><span style="color:#ff0000;">0.5</span></strong>）。<strong><span style="color:#ff0000;">学习率初始设定为</span></strong><strong><span style="color:#ff0000;">10−2</span></strong>，然后当验证集准确率停止改善时，减少10倍。学习率总共降低3次，学习在37万次迭代后停止（74个epochs）。</p> 
<p style="text-align:center;"><img alt="" class="has" height="217" src="https://images2.imgbox.com/fe/d5/t4sybe6l_o.png" width="466"></p> 
<p>     S定义为归一化训练图像的最小边（训练尺度），VGGNet的输入是从S图像中裁剪得到的。VGGNet训练图像大小的方式有两种，一是修正对应单尺度训练的S，VGGNet针对S=256和S=384进行测试（S=384时使用的是S=256时训练的权重，学习率调整为10-3），二是进行多尺度训练，每个训练图像通过从一定范围<img alt="" class="has" height="19" src="https://images2.imgbox.com/9d/52/MQMcUCUF_o.png" width="81">（分别为256、512）随机采样S来单独进行归一化。这种过程也可以看做<strong><span style="color:#ff0000;">通过尺度抖动使数据集增强</span></strong>。</p> 
<p style="text-indent:50px;"><strong><span style="color:#ff0000;">单一尺度评估结果：</span></strong></p> 
<p style="text-align:center;"><img alt="" class="has" height="339" src="https://images2.imgbox.com/ee/1d/B9PsjFyu_o.png" width="657"></p> 
<p style="margin-left:0cm;">通过分析表3结果，得出如下结论。</p> 
<ol><li>使用局部响应归一化local response normalization(A-LRN)并不能改善A网络性能。</li><li>分类误差随着深度增加而降低。</li><li>在训练时采用图像尺度抖动(scale jittering)可以改善图像分类效果。</li></ol> 
<p style="margin-left:0cm;"><strong><span style="color:#ff0000;">多尺度评估结果：</span></strong></p> 
<p style="margin-left:0cm;"><img alt="" class="has" height="397" src="https://images2.imgbox.com/7b/88/hpcOJDJg_o.png" width="1140"></p> 
<p style="margin-left:0cm;">相对于单一尺度评估，多尺度评估提高了分类精度。 </p> 
<p style="margin-left:0cm;"><strong><span style="color:#ff0000;">与其他模型对比结果：</span></strong></p> 
<p style="text-align:center;"><img alt="" class="has" height="431" src="https://images2.imgbox.com/8e/ab/chrc2mgq_o.png" width="762"></p> 
<p style="margin-left:0cm;">      从上表可以看出，深度VGGNet显著优于前一代模型，在ILSVRC-2012和ILSVRC-2013竞赛中取得了最好的结果。VGGNet取得的结果对于分类任务获胜者（GoogLeNet具有6.7％的错误率）也具有竞争力，并且大大优于ILSVRC-2013获胜者Clarifai，其使用外部训练数据取得了11.2％的错误率，没有外部数据则为11.7％。这是非常显著的，考虑到我们最好的结果是仅通过组合两个模型实现的——明显少于大多数ILSVRC的参赛者。<strong><span style="color:#ff0000;">在单网络性能方面，</span></strong><strong><span style="color:#ff0000;">VGG</span></strong><strong><span style="color:#ff0000;">的架构取得了最好结果</span></strong>（7.0％测试误差），超过单个GoogLeNet 0.9％。</p> 
<p style="margin-left:0cm;"> </p> 
<h4><strong>总结</strong></h4> 
<p style="margin-left:0cm;">意义：VGGNet创新地使用3×3小卷积来代替之前模型中的大卷积，在训练和测试时使用了多尺度评估做数据增强。</p> 
<p>优点：VGGNet相比于AlexNet层数更深，参数更多，但是却可以更快的收敛。利用卷积代替全连接，输入可适应各种尺寸的图片。</p> 
<p> </p> 
<h3 id="GoogLeNet">GoogLeNet</h3> 
<p style="margin-left:0cm;">        有了VGG的铺垫，人们开始意识到，为了更好的网络性能，有一条途径就是加深网络的深度和宽度，但是太过于复杂，参数过多的模型就会使得模型在不够复杂的数据上倾向于<strong><span style="color:#ff0000;">过拟合</span></strong>，并且过多的参数意味着需要更多的算力，也就是需要更多的时间和更多的钱。Google公司的Christian Szegedy在2015年提出了GoogLeNet，其核心思想是：将全连接，甚至是卷积中的局部连接，全部替换为<strong><span style="color:#ff0000;">稀疏连接</span></strong>。原因有二：1）生物神经系统中的连接是稀疏的；2）如果一个数据集的概率分布可以由一个很大、很稀疏的深度神经网络表示时，那么通过分析最后一层激活值的相关统计和对输出高度相关的神经元进行聚类，可以<strong><span style="color:#ff0000;">逐层</span></strong>地<strong><span style="color:#ff0000;">构建出一个最优网络结构</span></strong>。也就是说，一个深度稀疏网络可以被逐层简化，并且因为保留了网络的统计性质，其表达能力也没有被明显减弱。</p> 
<p>         但是由于计算机硬件计算稀疏数据的低效性，现在需要提出的是一种，<strong><span style="color:#ff0000;">既能保持网络结构的稀疏性，又能利用密集矩阵计算的高效性的方法</span></strong>。大量研究表明，可以将稀疏矩阵聚类为<strong><span style="color:#ff0000;">较为密集的子矩阵</span></strong>来提高计算性能，基于此，Inception模块应运而生。</p> 
<p> </p> 
<h4 id="Inception-v1"><strong>Inception-v1</strong></h4> 
<p>       首次出现在ILSVRC 2014的比赛中，以较大优势取得了第一名。该Inception Net通常被称为Inception-v1，它最大的特点是<strong><span style="color:#ff0000;">控制了计算量和参数量</span></strong>的同时，获得了<strong><span style="color:#ff0000;">非常好的分类性能</span></strong>——top-5错误率6.67%，只有AlexNet的一半不到。Inception V1有22层深，比AlexNet的8层或者VGGNet的19层还要更深。但其计算量只有15亿次浮点运算，同时只有500万的参数量，仅为AlexNet参数量（6000万）的1/12，却可以达到远胜于AlexNet的准确率 。</p> 
<p style="margin-left:0cm;"><strong>Inception 模块</strong></p> 
<p style="margin-left:0cm;">       这种基本模块使用了3种不同的卷积核，提取到的是3种不同尺度的特征，既有较为宏观的特征又有较为微观的特征，<strong><span style="color:#ff0000;">增加了特征的多样性</span></strong>。池化层目的是保留较为原始的输入信息。在模块的输出端将提取到的各种特征在channel维度上进行拼接，得到多尺度的特征feature map。</p> 
<p style="text-align:center;"><img alt="" class="has" height="276" src="https://images2.imgbox.com/1b/95/i08E7NHe_o.png" width="538"></p> 
<p style="text-align:center;"><img alt="" class="has" height="279" src="https://images2.imgbox.com/fa/27/JRs8fadI_o.png" width="526"></p> 
<p style="margin-left:0cm;">        但是这种naive的版本在进行5×5卷积的时候依然会造成大量的计算成本。为了解决这个问题，引入1×1卷积，它可以跨通道组织信息，提高网络的表达能力，同时可以对输出通道降维以减少计算成本。</p> 
<p style="text-align:center;"><img alt="" class="has" height="300" src="https://images2.imgbox.com/8d/47/eAS8rtZv_o.png" width="505"></p> 
<p style="margin-left:0cm;">       改进后的Inception模块如上图b所示，通过1×1卷积来<strong><span style="color:#ff0000;">降低通道</span></strong><strong><span style="color:#ff0000;">channal</span></strong><strong><span style="color:#ff0000;">数</span></strong>，把信息聚集起来，再进行计算，有效利用计算力同时进行了<strong><span style="color:#ff0000;">非线性化</span></strong>（VGGNet中使用1×1卷积仅是为了非线性化）。4个分支在最后通过一个聚合操作合并（在输出通道数这个维度上聚合）。Inception Module中包含了3种不同尺寸的卷积和1个最大池化，增加了网络对不同尺度的适应性，同时增加了网络的宽度，避免了因网络太深，训练梯度弥散的问题。</p> 
<p style="margin-left:0cm;"> </p> 
<p><strong>GoogLeNet-v1网络结构</strong></p> 
<p style="text-align:center;"><img alt="" class="has" height="1200" src="https://images2.imgbox.com/e8/6e/pNypSmxf_o.png" width="623"></p> 
<p style="margin-left:0cm;">其各层详细参数如下表所示：</p> 
<p style="text-align:center;"><img alt="" class="has" height="713" src="https://images2.imgbox.com/45/03/JJ4SaSlH_o.png" width="781"></p> 
<p><strong>GoogLeNet-v1网络特点</strong></p> 
<p style="margin-left:0cm;">1）网络中有三个softmax，这是为了<strong><span style="color:#ff0000;">减轻在深层网络反向传播时梯度消失</span></strong>的影响，也就是说，整个网络的loss是由三个softmax共同组成的（辅助的两个loss的计算被乘以<strong><span style="color:#ff0000;">0.3</span></strong>再与最后的loss相加得到最终的损失函数），这样在反向传播的时候，即使最后一个softmax传播回来的梯度消失了，还有前两个softmax传播回来的梯度进行辅助。在对网络进行测试的时候，这两个额外的softmax将会被拿掉。这样不仅仅减轻了梯度消失的影响，而且加速了网络的收敛；</p> 
<p style="margin-left:0cm;">2）网络最后<strong><span style="color:#ff0000;">采用了</span></strong><strong><span style="color:#ff0000;">average pooling</span></strong><strong><span style="color:#ff0000;">来代替全连接层</span></strong>，想法来自NIN，事实证明可以将TOP1 accuracy提高0.6%。但是，实际在最后还是加了一个全连接层，主要是为了方便以后大家整合（finetune）；</p> 
<p style="margin-left:0cm;">3）使用了Inception模块，提高了参数的利用效率，使用1×1卷积来减小计算成本。</p> 
<p style="margin-left:0cm;"> </p> 
<p><strong>GoogLeNet-v1训练与测试结果</strong></p> 
<p style="text-align:center;"><img alt="" class="has" height="219" src="https://images2.imgbox.com/af/27/aInw8r4z_o.png" width="459"></p> 
<p style="margin-left:0cm;">    在测试中，Christian Szegedy采用比Krizhevsky等人更积极的裁剪方法，将图像归一化为四个尺度，分别为256，288，320和352，取这些归一化的图像的左，中，右方块。对于每个方块，采用4个角以及中心224×224裁剪图像以及方块尺寸归一化为224×224，以及它们的镜像版本。这使得每张图像会得到4×3×6×2 = 144的裁剪图像。</p> 
<p style="text-align:center;"><img alt="" class="has" height="284" src="https://images2.imgbox.com/9b/17/gw6RXVh3_o.png" width="409"></p> 
<p style="margin-left:0cm;">GoogLeNet在ILSVRC2014比赛中以top-5错误率6.67%获得第一。</p> 
<p style="margin-left:0cm;"> </p> 
<h4 id="Inception-v2"><strong>Inception-v2</strong></h4> 
<p>      GoogLeNet凭借其优秀的表现，得到了很多研究人员的学习和使用，因此GoogLeNet团队又对其进行了进一步地发掘改进，产生了升级版本的GoogLeNet（Inception-v2）。GoogLeNet设计的初衷就是要<strong><span style="color:#ff0000;">又准又快</span></strong>，而如果只是单纯的堆叠网络虽然可以提高准确率，但是会导致计算效率有明显的下降，所以如何在不增加过多计算量的同时提高网络的表达能力就成为了一个问题。Inception V2版本的解决方案就是<strong><span style="color:#ff0000;">修改</span></strong><strong><span style="color:#ff0000;">Inception</span><span style="color:#ff0000;">的内部计算逻辑</span></strong>，提出了比较特殊的“卷积”计算结构。</p> 
<p> </p> 
<p><strong>Incepion-v2卷积分解</strong></p> 
<p style="margin-left:0cm;">       大尺寸的卷积核可以带来更大的感受野，但也意味着会产生更多的参数，比如5×5卷积核的参数有25个，3×3卷积核的参数有9个，前者是后者的25/9=2.78倍。因此，GoogLeNet团队提出可以<strong><span style="color:#ff0000;">用</span></strong><strong><span style="color:#ff0000;">2</span></strong><strong><span style="color:#ff0000;">个连续的</span></strong><strong><span style="color:#ff0000;">3×3</span></strong><strong><span style="color:#ff0000;">卷积层组成的小网络来代替单个的</span></strong><strong><span style="color:#ff0000;">5×5</span></strong><strong><span style="color:#ff0000;">卷积层</span></strong>，即在保持感受野范围的同时又减少了参数量，如下图：</p> 
<p style="text-align:center;"><img alt="" class="has" height="231" src="https://images2.imgbox.com/dd/92/j7RgKpO1_o.png" width="260"></p> 
<p style="margin-left:0cm;">受这种分解的启发，GoogLeNet团队考虑将卷积核大小进一步缩小为n×1，用3个3×1的卷积核代替卷积，如下图所示：</p> 
<p style="text-align:center;"><img alt="" class="has" height="422" src="https://images2.imgbox.com/4b/9e/oDg3xiPz_o.png" width="360"></p> 
<p style="margin-left:0cm;">因此，任意n×n的卷积都可以通过1×n卷积后接n×1卷积来替代。GoogLeNet团队发现<strong><span style="color:#ff0000;">在网络的前期使用这种分解效果并不好</span></strong>，在<strong><span style="color:#ff0000;">中度大小的特征图（</span></strong><strong><span style="color:#ff0000;">feature map</span></strong><strong><span style="color:#ff0000;">）上使用效果才会更好</span></strong>（特征图大小建议在12到20之间）。于是Inception-v1的结构被改进如下：</p> 
<p style="text-align:center;"><img alt="" class="has" height="388" src="https://images2.imgbox.com/6c/35/LRs1GrvP_o.png" width="571"></p> 
<p style="text-align:center;"><img alt="" class="has" height="714" src="https://images2.imgbox.com/fc/6b/AQtX19m5_o.png" width="504"></p> 
<p style="text-align:center;"><img alt="" class="has" height="881" src="https://images2.imgbox.com/2d/df/GTleU0ww_o.png" width="543"></p> 
<p><strong>Incepion-v2降低特征图大小</strong></p> 
<p style="margin-left:0cm;">一般情况下，如果想让图像缩小，可以有如下两种方式：</p> 
<p style="text-align:center;"><img alt="" class="has" height="362" src="https://images2.imgbox.com/8f/73/7Lnr1hAc_o.png" width="685"></p> 
<p style="margin-left:0cm;">但是方法一（左图）先作pooling（池化）会导致特征表示遇到瓶颈（特征缺失），方法二（右图）是正常的缩小，但计算量很大。为了同时保持特征表示且降低计算量，将网络结构改为下图，使用<strong><span style="color:#ff0000;">两个并行化的模块来降低计算量</span></strong>（卷积、池化并行执行，再进行合并）</p> 
<p style="text-align:center;"><img alt="" class="has" height="401" src="https://images2.imgbox.com/55/e2/ibc6jAsV_o.png" width="694"></p> 
<p><strong>Incepion-v2使用Label Smoothing来对网络输出进行正则化</strong></p> 
<p>        Softmax层的输出可以用<img alt="" class="has" height="32" src="https://images2.imgbox.com/9c/cf/J41VY5Kg_o.png" width="131">表示（即训练样本<em>x</em><img alt="" class="has" height="19" src="https://images2.imgbox.com/9b/88/dWzxrlEp_o.png" width="9">与每个标签<img alt="" class="has" height="19" src="https://images2.imgbox.com/b5/f3/weroqXbw_o.png" width="88">匹配的概率），其中<img alt="" class="has" height="19" src="https://images2.imgbox.com/66/c3/Mj8YTmHL_o.png" width="12">表示对数单位或未归一化的对数概率。定义损失函数为<img alt="" class="has" height="22" src="https://images2.imgbox.com/1b/2a/t3TsuLVh_o.png" width="176">，其中<img alt="" class="has" height="19" src="https://images2.imgbox.com/6a/36/CqRu3EKo_o.png" width="32">为样本在标签上的实际分布且归一化后<img alt="" class="has" height="19" src="https://images2.imgbox.com/26/27/1nk9lVSe_o.png" width="97">。假设分类的标签是独热码表示（正确分类<em>q</em>=1<img alt="" class="has" height="19" src="https://images2.imgbox.com/65/99/ptv6KpM3_o.png" width="39">，其他类别<em>q=0</em><img alt="" class="has" height="19" src="https://images2.imgbox.com/8e/58/MENNJN70_o.png" width="39">），则可以反推出整个训练过程收敛时Softmax的正确分类的输入<img alt="" class="has" height="19" src="https://images2.imgbox.com/38/fc/gqRsUtwk_o.png" width="15">是无穷大，这是一种极其理想的情况，如果让所有的输入都产生这种极其理想的输出，就会过拟合、降低模型的适应能力，即模型过于自信。为了克服过拟合，防止最终出来的正确分类<img alt="" class="has" height="19" src="https://images2.imgbox.com/62/9a/cOYeqvHx_o.png" width="61">，考虑用<img alt="" class="has" height="21" src="https://images2.imgbox.com/bb/cc/OVcEenEl_o.png" width="205">来代替标签分布<img alt="" class="has" height="21" src="https://images2.imgbox.com/92/39/T7PzLbYr_o.png" width="92">，<img alt="" class="has" height="21" src="https://images2.imgbox.com/e6/40/Y9dUAL5M_o.png" width="26">表示当且仅当<img alt="" class="has" height="19" src="https://images2.imgbox.com/59/74/dkBKNjYl_o.png" width="39">时，<img alt="" class="has" height="19" src="https://images2.imgbox.com/e3/65/AuYvJzys_o.png" width="39">，否则为0。这种操作被称为标签平滑正则化LSR，它阻止了样本对应实际标签的最大逻辑单元变得比其它逻辑单元更大。在测试中，这样的做法使top-5错误率<strong><span style="color:#ff0000;">下降了</span></strong><strong><span style="color:#ff0000;">0.2%</span></strong>。</p> 
<p><strong>Incepion-v2网络结构</strong></p> 
<p style="margin-left:0cm;">使用Inception V2作改进版的GoogLeNet，网络结构图如下：</p> 
<p style="text-align:center;"><img alt="" class="has" height="603" src="https://images2.imgbox.com/24/2d/Mk1Z1s8O_o.png" width="701"></p> 
<p style="margin-left:0cm;">      上表中的Figure 5指没有进化的Inception，Figure 6是指小卷积版的Inception（用3×3卷积核代替5×5卷积核），Figure 7是指不对称版的Inception（用1×n、n×1卷积核代替n×n卷积核），在Inception-v2中，抛弃了v1中对LRN的处理过程，训练时间有所缩短。</p> 
<p><strong> Incepion-v2训练与测试结果</strong></p> 
<p style="text-align:center;"><img alt="" class="has" height="213" src="https://images2.imgbox.com/cb/b2/AWJKy5YW_o.png" width="505"></p> 
<p style="margin-left:0cm;">利用TensorFlow分布式机器学习系统对网络进行随机梯度训练，使用50个副本，每个副本在NVidia Kepler GPU上运行，批次大小32个，共100个epoch。取得最好结果的模型是使用<strong><span style="color:#ff0000;">RMSProp</span></strong><strong><span style="color:#ff0000;">优化算法</span></strong>进行训练，其中decay=0.9和ε= 1.0。采用的<strong><span style="color:#ff0000;">学习率为</span></strong><strong><span style="color:#ff0000;">0.045</span></strong>，每两个epoch衰减一次。经实验，模型结果与旧的GoogleNet相比有较大提升，如下表所示：</p> 
<p style="text-align:center;"><img alt="" class="has" height="447" src="https://images2.imgbox.com/49/38/QDHMI9Uo_o.png" width="535"></p> 
<h4 id="Inception-v3">Inception-v3</h4> 
<p style="margin-left:0cm;">           "Inception-v3 = Inception-v2 + Factorization + Batch Normalization"</p> 
<p style="margin-left:0cm;">        Inception V3一个最重要的改进是<strong><span style="color:#ff0000;">分解</span></strong>（Factorization），将7×7分解成两个一维的卷积（1×7，7×1），3×3也是一样（1×3，3×1），这样的好处，既可以加速计算，又可以将1个卷积拆成2个卷积，使得网络深度进一步增加，增加了网络的非线性（每增加一层都要进行ReLU）。另外，网络输入从224×224变为了299×299。</p> 
<p style="margin-left:0cm;"><strong>Batch Normalization批归一化</strong></p> 
<p style="margin-left:0cm;">         正如其字面意思，BN就是对每一批数据进行归一化，对于训练中某一个batch的数据<img alt="" class="has" height="19" src="https://images2.imgbox.com/9c/9e/xW00TMgS_o.png" width="89">（可以是输入数据，也可以是网络中间某一层的输出），BN的前三步如下：</p> 
<p style="text-align:center;"><img alt="" class="has" height="52" src="https://images2.imgbox.com/0e/8b/eJs1dNGN_o.png" width="85"></p> 
<p style="text-align:center;"><img alt="" class="has" height="52" src="https://images2.imgbox.com/bd/44/ZjFi9KNI_o.png" width="131"></p> 
<p style="text-align:center;"><img alt="" class="has" height="36" src="https://images2.imgbox.com/aa/f6/9O9uGVhF_o.png" width="90"></p> 
<p style="margin-left:0cm;">到这步为止，就是一个标准的数据均值除方差的归一化过程。最后，完成下一步便完成了BN的过程：</p> 
<p style="text-align:center;"><img alt="" class="has" height="19" src="https://images2.imgbox.com/69/19/eOxyLzn1_o.png" width="86"></p> 
<p style="margin-left:0cm;">其中，<img alt="" class="has" height="19" src="https://images2.imgbox.com/29/be/z8EuIUyC_o.png" width="9">和<img alt="" class="has" height="19" src="https://images2.imgbox.com/3b/e8/VsoU4Gkd_o.png" width="10">是要学习的参数。所以，BN的本质就是利用优化改变一下方差大小和均值的位置，因为需要统计方差和均值，而这两个值是在每个batch的数据上计算的，所以叫做批归一化。在卷积神经网络中，使用批归一化的目的是<strong><span style="color:#ff0000;">使激活函数能够更有效地利用输入信息</span></strong>，确保随着模型的训练，层能够持续学习输入的分布，内部协方差不断变小，从而<strong><span style="color:#ff0000;">加速训练过程</span></strong>。</p> 
<p style="margin-left:0cm;">  <em><span style="color:#e36c0a;">Inception-v3</span></em><em><span style="color:#e36c0a;">是在Inception-v2的基础上增加了分解、批归一化的方法，主体结构与Inception-v2并无太大区别，测试结果的对比图如下：</span></em></p> 
<p style="text-align:center;"><img alt="" class="has" height="438" src="https://images2.imgbox.com/b7/f3/ybhxVZCt_o.png" width="648"></p> 
<p style="text-align:center;"><img alt="" class="has" height="426" src="https://images2.imgbox.com/eb/4a/TicKB9VO_o.png" width="570"></p> 
<p style="margin-left:0cm;">       上图是作者在MMIST上，实现的3层全连接层，每层有100个神经元，激活函数使用的是sigmoid，权重被初始化为很小的高斯分布的随机数。最后一层接linear+softmax层来分类。训练50000 step，mini-batch大小为60。图中展示了分类的准确度，可以看出，BN后的网络准确率更高，收敛速度更快。</p> 
<p style="text-align:center;"><img alt="" class="has" height="241" src="https://images2.imgbox.com/86/70/kpJpoxhU_o.png" width="620"></p> 
<p style="text-indent:0;">上图是各个模型在ILSVRC 2012上测试结果的对比情况：<br> Inception：      Inception v1，初始学习速率为0.0015<br> BN-Baseline：   Inception的激活函数前加BN<br> BN-x5：        在BN-Baseline基础上，将初始学习速率增大5倍：0.0075<br> BN-x30：       在BN-x5基础上，将初始学习速率从Inception的0.0015增大30倍：0.045<br> BN-x5-sigmoid： 在BN-x5基础上，将ReLU激活函数改为sigmoid</p> 
<p>可以看出，使用了BN的网络在达到较高准确率所需的<strong><span style="color:#ff0000;">时间会大大减少</span></strong>。</p> 
<h4>总结</h4> 
<p>意义：跳出了LeNet-5卷积、激活函数、池化传统结构，提出了Inception模块，网络更深。GoogLeNet解决了传统网络参数空间大，容易过拟合，且训练数据集有限；网络结构复杂，计算资源不足，导致难以应用；深层次网络结构容易出现梯度弥散，模型性能下降等问题，为图像识别领域的发展贡献巨大的力量。</p> 
<p> </p> 
<h3 id="ResNet">ResNet</h3> 
<p style="margin-left:0cm;">       ResNet在2015年被Kaiming He提出，在ImageNet比赛classification任务上获得第一名，因为它“简单与实用”并存，之后很多方法都建立在ResNet50或者ResNet101的基础上完成的，检测，分割，识别等领域都纷纷使用ResNet。ResNet所解决的问题是在网络越来越深的过程中，可能出现的<strong><span style="color:#ff0000;">梯度弥散或梯度爆炸</span></strong>，解决方法是提出了<strong><span style="color:#ff0000;">残差块</span></strong>的模型。</p> 
<p style="text-align:center;"><img alt="" class="has" height="468" src="https://images2.imgbox.com/9b/ad/AF7pRSed_o.png" width="845"></p> 
<p style="margin-left:0cm;">理论上讲，深度加深，网络的表现能力会越强，但是过深的网络可能会退化，如下图所示：</p> 
<p style="text-align:center;"><img alt="" class="has" height="252" src="https://images2.imgbox.com/e8/13/rpPpziNB_o.png" width="710"></p> 
<p style="margin-left:0cm;">         随着网络深度的增加，准确率达到饱和（这可能并不奇怪）然后迅速下降。意外的是，这种下降<strong><span style="color:#ff0000;">不是由过拟合引起</span></strong>的，并且在适当的深度模型上添加更多的层会导致更高的训练误差。</p> 
<p style="margin-left:0cm;"> </p> 
<h4 id="%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%20Residual%20Block"><strong>网络结构 Residual Block</strong></h4> 
<p style="margin-left:0cm;">       Kaiming He提出了<strong><span style="color:#ff0000;">残差学习</span></strong>来解决深度网络中的退化问题。假设多个非线性层可以渐近地近似复杂函数，它等价于假设它们可以渐近地<strong><span style="color:#ff0000;">近似残差函数</span></strong>，即<img alt="" class="has" height="19" src="https://images2.imgbox.com/71/a5/Vda6NuXN_o.png" width="62"> （假设输入输出是相同维度）。因此，残差学习就是明确让网络中的隐藏层近似残差函数<img alt="" class="has" height="19" src="https://images2.imgbox.com/9f/46/cVaEFttb_o.png" width="116">，而不是近似<img alt="" class="has" height="19" src="https://images2.imgbox.com/af/86/lC26Da3R_o.png" width="34">。因此原始函数变为<img alt="" class="has" height="19" src="https://images2.imgbox.com/dd/b7/z3hZxUBh_o.png" width="61">。尽管两种形式应该都能渐近地近似要求的函数（如假设），但学习的难易程度是不同的。</p> 
<p style="text-align:center;"><img alt="" class="has" height="388" src="https://images2.imgbox.com/0b/8a/pWmwxlYa_o.png" width="711"></p> 
<p style="margin-left:0cm;"><strong><span style="color:#ff0000;">         残差学习相比直接学习原始特征更容易</span></strong>。当残差为0时，此时堆积层仅仅做了<strong><span style="color:#ff0000;">恒等映射</span></strong>，至少网络性能不会下降。实际上残差不会为0，这也会使得堆积层在输入特征基础上学习到新的特征，从而拥有更好的性能。残差学习的结构上图右所示。这有点类似与电路中的“短路”，所以是一种短路连接（shortcut connection）。简单地说，就是将网络第<em>l</em><img alt="" class="has" height="19" src="https://images2.imgbox.com/33/9e/544j6g4v_o.png" width="6">层的输入直接连接到第<img alt="" class="has" height="19" src="https://images2.imgbox.com/92/4e/R5NHePUN_o.png" width="33">层激活函数之前。</p> 
<p style="text-align:center;"><img alt="" class="has" height="156" src="https://images2.imgbox.com/b9/e6/dqoRlcg2_o.png" width="307"></p> 
<p style="margin-left:0cm;">         ResNet-34以VGG19结构为基础，使用了许多Residual Block来构建网络，当两层间输入和输出特征的维度相同时，输出可以用<img alt="" class="has" height="19" src="https://images2.imgbox.com/5b/7b/bHj6AEZf_o.png" width="115">表示，其中<img alt="" class="has" height="19" src="https://images2.imgbox.com/a8/10/OoLEdHga_o.png" width="28">表示<img alt="" class="has" height="19" src="https://images2.imgbox.com/3f/0e/BsKALhLt_o.png" width="33">层的线性激活输出<img alt="" class="has" height="19" src="https://images2.imgbox.com/0b/d5/bRKJ9oqV_o.png" width="162">）；若两层间输入和输出特征维度不同时，输出用<img alt="" class="has" height="30" src="https://images2.imgbox.com/74/a5/ixE6sEpa_o.png" width="134">表示，其中<img alt="" class="has" height="19" src="https://images2.imgbox.com/30/40/SY94Sv4w_o.png" width="20">是将输入向量转换成输出向量同维的匹配矩阵。</p> 
<p style="text-align:center;"><img alt="" class="has" height="1200" src="https://images2.imgbox.com/16/70/WAEpBU6c_o.png" width="285"></p> 
<p style="margin-left:0cm;"><img alt="" class="has" height="336" src="https://images2.imgbox.com/4a/a0/HYkp3FjC_o.png" width="847"></p> 
<p style="margin-left:0cm;">     在ResNet-152中，采取的Residual Block的形式如上图右所示。由于层数更多，故在每一个Residual Block中采用了1×1卷积核来减小参数和计算量。 不同层数的ResNet结构如下图所示：</p> 
<p style="text-align:center;"><img alt="" class="has" height="375" src="https://images2.imgbox.com/ce/ec/gJYHAM96_o.png" width="754"></p> 
<p>其中layer数目不包括激活函数以及池化层。</p> 
<p> </p> 
<h4>训练与测试结果</h4> 
<p style="text-align:center;"><img alt="" class="has" height="336" src="https://images2.imgbox.com/26/02/C9BRMBaq_o.png" width="621"></p> 
<p style="margin-left:0cm;">在ISLVRC2012测试集上测试，用plain与ResNet进行比较，结果如下：</p> 
<p style="text-align:center;"><img alt="" class="has" height="277" src="https://images2.imgbox.com/76/b4/s6VRIWhZ_o.png" width="610"></p> 
<p style="margin-left:0cm;">在ImageNet验证集上，ResNet-152单一结构的top-5错误率达到了4.49%，这一结果与人类水平相当。</p> 
<p style="text-align:center;"><img alt="" class="has" height="358" src="https://images2.imgbox.com/6c/23/k66hk3eo_o.png" width="571"></p> 
<p style="margin-left:0cm;"><strong><span style="color:#ff0000;">对比</span></strong></p> 
<p style="text-align:center;"><img alt="" class="has" height="237" src="https://images2.imgbox.com/f8/08/W9wtmlRb_o.png" width="593"></p> 
<p style="text-align:center;"><img alt="" class="has" height="275" src="https://images2.imgbox.com/0c/52/GNJAKFqW_o.png" width="612"></p> 
<p> </p> 
<p> </p> 
<p> </p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/535cc666a4d56844b636c185e5426a12/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">h5测试</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d0a51d4b3438c61691bcaee07aae5d29/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">MySQL数据库学习</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>