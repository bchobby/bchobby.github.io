<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Python网络爬虫（九）：爬取顶点小说网站全部小说，并存入MongoDB - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Python网络爬虫（九）：爬取顶点小说网站全部小说，并存入MongoDB" />
<meta property="og:description" content="前言：本篇博客将爬取顶点小说网站全部小说、涉及到的问题有：Scrapy架构、断点续传问题、Mongodb数据库相关操作。 背景： Python版本：Anaconda3
运行平台：Windows
IDE：PyCharm
数据库：MongoDB
浏览器工具： Chrome浏览器
前面的博客中已经对Scrapy作了相当多的介绍所以这里不再对Scrapy技术作过多的讲解。 一、爬虫准备工作： 此次我们爬取的是免费小说网站：顶点小说 http://www.23us.so/ 我们要想把它全部的小说爬取下来，是不是得有全部 小说的链接？ 我们看到顶点小说网站上有一个总排行榜。 点击进入后我们看到，这里有网站上所有的小说，一共有1144页，每页大约20本小说，算下来一共大约有两万两千多本，是一个庞大的数据量，并且小说的数量还在不断的增长中。 好！我们遇到了第一个问题，如何获取总排行榜中的页数呢？也就是现在的“1144”。 1、获取排行榜页面数： 最好的方法就是用Xpath。 我们先用F12审查元素，看到“1144”放在了“id”属性为“pagestats”的em节点中。 我们再用Scrapy Shell分析一下网页。 注意：Scrapy Shell是一个非常好的工具，我们在编写爬虫过程中，可以用它不断的测试我们编写的Xpath语句，非常方便。 输入命令： scrapy shell &#34;http://www.23us.so/top/allvisit_2.html&#34; 然后就进入了scrapy shell 因为页数放在“id”属性为“pagestats”的em节点中，所以我们可以在shell中输入如下指令获取。 response.xpath(&#39;//*[@id=&#34;pagestats&#34;]/text()&#39;).extract_first() 我们可以看到，Xpath一如既往的简单高效，页面数已经被截取下来了。
2、获取小说主页链接、小说名称： 接下来，我们遇到新的问题，如何获得每个页面上的小说的链接呢？我们再来看页面的HTML代码。 小说的链接放在了“a”节点里，而且这样的a节点区别其他的“a”节点的是，没有“title”属性。 所以我们用shell测试一下，输入命令： response.xpath(&#39;//td/a[not(@title)]/@href&#39;).extract() 我们看到，小说的链接地址我们抓到了。
同样还有小说名， response.xpath(&#39;//td/a[not(@title)]/text()&#39;).extract() 我们可以看到页面上的小说名称我们也已经抓取到了。
3、获取小说详细信息： 我们点开页面上的其中一个小说链接： 这里有小说的一些相关信息和小说章节目录的地址。 我们想要的数据首先是小说全部章节目录的地址，然后是小说类别、小说作者、小说状态、小说最后更新时间。 我们先看小说全部章节目录的地址。用F12，我们看到： 小说全部章节地址放在了“class”属性为“btnlinks”的“p”节点的第一个“a”节点中。 我们还是用scrapy shell测试一下我们写的xpath语句。 键入命令，进入shell界面 scrapy shell &#34;http://www.23us.so/xiaoshuo/13007.html&#34; 在shell中键入命令： response.xpath(&#39;//p[@class=&#34;btnlinks&#34;]/a[1]/@href&#39;).extract_first() 小说的章节目录页面我们已经截取下来了。
类似的还有小说类别、小说作者、小说状态、小说最后更新时间，命令分别是： #小说类别 response.xpath(&#39;//table/tr[1]/td[1]/a/text()&#39;).extract_first() #小说作者 response.xpath(&#39;//table/tr[1]/td[2]/text()&#39;).extract_first() #小说状态 response.xpath(&#39;//table/tr[1]/td[3]/text()&#39;).extract_first() #小说最后更新时间 response.xpath(&#39;//table/tr[2]/td[3]/text()&#39;).extract_first() 4、获取小说全部章节： 我们点开“最新章节”，来到小说全部章节页面。 我们如何获得这些链接呢？答案还是Xpath。 用F12看到，各章节地址和章节名称放在了一个“table”中： 退出上次的scrapy shell ,分析 全部章节页面。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/cac86d5cd2cf024f9cd88cd93aefdeae/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-02-02T19:44:38+08:00" />
<meta property="article:modified_time" content="2018-02-02T19:44:38+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Python网络爬虫（九）：爬取顶点小说网站全部小说，并存入MongoDB</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h4 id="前言本篇博客将爬取顶点小说网站全部小说涉及到的问题有scrapy架构断点续传问题mongodb数据库相关操作">前言：本篇博客将爬取顶点小说网站全部小说、涉及到的问题有：Scrapy架构、断点续传问题、Mongodb数据库相关操作。</h4> 
<h4 id="背景">背景：</h4> 
<p>Python版本：Anaconda3</p> 
<p>运行平台：Windows</p> 
<p>IDE：PyCharm</p> 
<p>数据库：MongoDB</p> 
<p>浏览器工具： Chrome浏览器</p> 
<h5 id="前面的博客中已经对scrapy作了相当多的介绍所以这里不再对scrapy技术作过多的讲解">前面的博客中已经对Scrapy作了相当多的介绍所以这里不再对Scrapy技术作过多的讲解。</h5> 
<h4 id="一爬虫准备工作">一、爬虫准备工作：</h4> 
<h5 id="此次我们爬取的是免费小说网站顶点小说">此次我们爬取的是免费小说网站：顶点小说</h5> 
<h5 id="httpwww23usso"><a href="http://www.23us.so/" rel="nofollow">http://www.23us.so/</a></h5> 
<h5 id="我们要想把它全部的小说爬取下来是不是得有全部">我们要想把它全部的小说爬取下来，是不是得有全部</h5> 
<h5 id="小说的链接">小说的链接？</h5> 
<h5 id="我们看到顶点小说网站上有一个总排行榜">我们看到顶点小说网站上有一个总排行榜。</h5> 
<p><img src="https://images2.imgbox.com/35/4b/sG9gB0Lg_o.png" alt="这里写图片描述" title=""> </p> 
<h5 id="点击进入后我们看到这里有网站上所有的小说一共有1144页每页大约20本小说算下来一共大约有两万两千多本是一个庞大的数据量并且小说的数量还在不断的增长中">点击进入后我们看到，这里有网站上所有的小说，一共有1144页，每页大约20本小说，算下来一共大约有两万两千多本，是一个庞大的数据量，并且小说的数量还在不断的增长中。</h5> 
<h5 id="好我们遇到了第一个问题如何获取总排行榜中的页数呢也就是现在的1144">好！我们遇到了第一个问题，如何获取总排行榜中的页数呢？也就是现在的“1144”。</h5> 
<h5 id="1获取排行榜页面数">1、获取排行榜页面数：</h5> 
<h5 id="最好的方法就是用xpath">最好的方法就是用Xpath。</h5> 
<h5 id="我们先用f12审查元素看到1144放在了id属性为pagestats的em节点中">我们先用F12审查元素，看到“1144”放在了“id”属性为“pagestats”的em节点中。</h5> 
<p><img src="https://images2.imgbox.com/1b/66/xZRiCWOh_o.png" alt="这里写图片描述" title=""></p> 
<h5 id="我们再用scrapy-shell分析一下网页">我们再用Scrapy Shell分析一下网页。</h5> 
<h5 id="注意scrapy-shell是一个非常好的工具我们在编写爬虫过程中可以用它不断的测试我们编写的xpath语句非常方便">注意：Scrapy Shell是一个非常好的工具，我们在编写爬虫过程中，可以用它不断的测试我们编写的Xpath语句，非常方便。</h5> 
<h5 id="输入命令">输入命令：</h5> 
<pre class="prettyprint"><code class=" hljs livecodeserver">scrapy <span class="hljs-built_in">shell</span> <span class="hljs-string">"http://www.23us.so/top/allvisit_2.html"</span></code></pre> 
<p>然后就进入了scrapy shell <br> <img src="https://images2.imgbox.com/c7/9d/lRnthZKs_o.png" alt="这里写图片描述" title=""> </p> 
<h5 id="因为页数放在id属性为pagestats的em节点中所以我们可以在shell中输入如下指令获取">因为页数放在“id”属性为“pagestats”的em节点中，所以我们可以在shell中输入如下指令获取。</h5> 
<pre class="prettyprint"><code class=" hljs scilab"><span class="hljs-transposed_variable">response.</span>xpath(<span class="hljs-string">'//*[@id="</span>pagestats<span class="hljs-string">"]/text()'</span>).extract_first()</code></pre> 
<p><img src="https://images2.imgbox.com/c2/57/kTqb70Ku_o.png" alt="这里写图片描述" title=""></p> 
<p>我们可以看到，Xpath一如既往的简单高效，页面数已经被截取下来了。</p> 
<h5 id="2获取小说主页链接小说名称">2、获取小说主页链接、小说名称：</h5> 
<h5 id="接下来我们遇到新的问题如何获得每个页面上的小说的链接呢我们再来看页面的html代码">接下来，我们遇到新的问题，如何获得每个页面上的小说的链接呢？我们再来看页面的HTML代码。</h5> 
<p><img src="https://images2.imgbox.com/db/94/BYviaPOn_o.png" alt="这里写图片描述" title=""></p> 
<h5 id="小说的链接放在了a节点里而且这样的a节点区别其他的a节点的是没有title属性">小说的链接放在了“a”节点里，而且这样的a节点区别其他的“a”节点的是，没有“title”属性。</h5> 
<h5 id="所以我们用shell测试一下输入命令">所以我们用shell测试一下，输入命令：</h5> 
<pre class="prettyprint"><code class=" hljs autohotkey">response.xpath('//td/<span class="hljs-literal">a</span>[<span class="hljs-literal">not</span>(@title)]/@href').extract()</code></pre> 
<p><img src="https://images2.imgbox.com/9e/4c/EmSrwQ0D_o.png" alt="这里写图片描述" title=""> <br> 我们看到，小说的链接地址我们抓到了。</p> 
<h5 id="同样还有小说名">同样还有小说名，</h5> 
<pre class="prettyprint"><code class=" hljs autohotkey">response.xpath('//td/<span class="hljs-literal">a</span>[<span class="hljs-literal">not</span>(@title)]/text()').extract()</code></pre> 
<p><img src="https://images2.imgbox.com/77/7b/lScm7bKQ_o.png" alt="这里写图片描述" title=""> </p> 
<p>我们可以看到页面上的小说名称我们也已经抓取到了。</p> 
<h5 id="3获取小说详细信息">3、获取小说详细信息：</h5> 
<h5 id="我们点开页面上的其中一个小说链接">我们点开页面上的其中一个小说链接：</h5> 
<p><img src="https://images2.imgbox.com/90/c1/ia64zte9_o.png" alt="这里写图片描述" title=""> </p> 
<h5 id="这里有小说的一些相关信息和小说章节目录的地址">这里有小说的一些相关信息和小说章节目录的地址。</h5> 
<h5 id="我们想要的数据首先是小说全部章节目录的地址然后是小说类别小说作者小说状态小说最后更新时间">我们想要的数据首先是小说全部章节目录的地址，然后是小说类别、小说作者、小说状态、小说最后更新时间。</h5> 
<h5 id="我们先看小说全部章节目录的地址用f12我们看到">我们先看小说全部章节目录的地址。用F12，我们看到：</h5> 
<p><img src="https://images2.imgbox.com/77/a5/mpQ5maMy_o.png" alt="这里写图片描述" title=""> </p> 
<h5 id="小说全部章节地址放在了class属性为btnlinks的p节点的第一个a节点中">小说全部章节地址放在了“class”属性为“btnlinks”的“p”节点的第一个“a”节点中。</h5> 
<h5 id="我们还是用scrapy-shell测试一下我们写的xpath语句">我们还是用scrapy shell测试一下我们写的xpath语句。</h5> 
<h5 id="键入命令进入shell界面">键入命令，进入shell界面</h5> 
<pre class="prettyprint"><code class=" hljs livecodeserver">scrapy <span class="hljs-built_in">shell</span> <span class="hljs-string">"http://www.23us.so/xiaoshuo/13007.html"</span></code></pre> 
<h5 id="在shell中键入命令">在shell中键入命令：</h5> 
<pre class="prettyprint"><code class=" hljs haskell"><span class="hljs-title">response</span>.xpath('//p[@<span class="hljs-keyword">class</span>=<span class="hljs-string">"btnlinks"</span>]/a[<span class="hljs-number">1</span>]/@href').extract_first()</code></pre> 
<p><img src="https://images2.imgbox.com/a7/31/qDTrdRni_o.png" alt="这里写图片描述" title=""> </p> 
<p>小说的章节目录页面我们已经截取下来了。</p> 
<h5 id="类似的还有小说类别小说作者小说状态小说最后更新时间命令分别是">类似的还有小说类别、小说作者、小说状态、小说最后更新时间，命令分别是：</h5> 
<pre class="prettyprint"><code class=" hljs vala"><span class="hljs-preprocessor">#小说类别</span>
response.xpath(<span class="hljs-string">'//table/tr[1]/td[1]/a/text()'</span>).extract_first()    
<span class="hljs-preprocessor">#小说作者</span>
response.xpath(<span class="hljs-string">'//table/tr[1]/td[2]/text()'</span>).extract_first()   
<span class="hljs-preprocessor">#小说状态</span>
response.xpath(<span class="hljs-string">'//table/tr[1]/td[3]/text()'</span>).extract_first()   
<span class="hljs-preprocessor">#小说最后更新时间</span>
response.xpath(<span class="hljs-string">'//table/tr[2]/td[3]/text()'</span>).extract_first()</code></pre> 
<p><img src="https://images2.imgbox.com/44/73/lCY6DHZb_o.png" alt="这里写图片描述" title=""> </p> 
<h5 id="4获取小说全部章节">4、获取小说全部章节：</h5> 
<h5 id="我们点开最新章节来到小说全部章节页面">我们点开“最新章节”，来到小说全部章节页面。</h5> 
<p><img src="https://images2.imgbox.com/eb/1a/aCvOOEvE_o.png" alt="这里写图片描述" title=""> </p> 
<h5 id="我们如何获得这些链接呢答案还是xpath">我们如何获得这些链接呢？答案还是Xpath。</h5> 
<h5 id="用f12看到各章节地址和章节名称放在了一个table中">用F12看到，各章节地址和章节名称放在了一个“table”中：</h5> 
<p><img src="https://images2.imgbox.com/8d/6b/oDaV6Akw_o.png" alt="这里写图片描述" title=""> <br> 退出上次的scrapy shell ,分析 全部章节页面。</p> 
<pre class="prettyprint"><code class=" hljs livecodeserver">scrapy <span class="hljs-built_in">shell</span> <span class="hljs-string">"http://www.23us.so/files/article/html/13/13007/index.html"</span></code></pre> 
<p>在shell中键入Xpath语句：</p> 
<pre class="prettyprint"><code class=" hljs bash">response.xpath(<span class="hljs-string">'//table/tr/td/a/@href'</span>).extract()</code></pre> 
<p><img src="https://images2.imgbox.com/fe/b7/05rdIsNo_o.png" alt="这里写图片描述" title=""> </p> 
<h5 id="同样还有各章节名称">同样还有各章节名称</h5> 
<pre class="prettyprint"><code class=" hljs bash">response.xpath(<span class="hljs-string">'//table/tr/td/a/text()'</span>).extract()</code></pre> 
<p><img src="https://images2.imgbox.com/30/fa/YFPZSIC7_o.png" alt="这里写图片描述" title=""> </p> 
<h5 id="5爬取小说章节内容">5、爬取小说章节内容：</h5> 
<h4 id="好了小说各个章节地址我们截取下来了接下来就是小说各个章节的内容">好了，小说各个章节地址我们截取下来了，接下来就是小说各个章节的内容。</h4> 
<h5 id="我们用f12看到章节内容放在了id属性为contents的dd节点中">我们用F12看到，章节内容放在了“id”属性为“contents”的“dd”节点中。</h5> 
<p><img src="https://images2.imgbox.com/45/84/y79b98wT_o.png" alt="这里写图片描述" title=""> </p> 
<h5 id="这里我们再用xpath看一下键入xpath语句">这里我们再用Xpath看一下,键入Xpath语句：</h5> 
<pre class="prettyprint"><code class=" hljs scilab"><span class="hljs-transposed_variable">Response.</span>xpath(<span class="hljs-string">'//dd[@id="</span>contents<span class="hljs-string">"]'</span>).extract()</code></pre> 
<p><img src="https://images2.imgbox.com/8e/39/n3TP8r6G_o.png" alt="这里写图片描述" title=""> </p> 
<h5 id="我们看到小说内容已经让我们截取到了">我们看到，小说内容已经让我们截取到了！</h5> 
<h4 id="二编写爬虫">二、编写爬虫：</h4> 
<h5 id="整个流程上面已经介绍过了还有一个非常重要的问题">整个流程上面已经介绍过了，还有一个非常重要的问题：</h5> 
<h4 id="断点续传问题">断点续传问题</h4> 
<h5 id="我们知道爬虫不可能一次将全部网站爬取下来网站的数据量相当庞大在短时间内不可能完成爬虫工作在下一次启动爬虫时难道再将已经做过的工作再做一次当然不行这样的爬虫太不友好那么我们如何来解决断点续传问题呢">我们知道，爬虫不可能一次将全部网站爬取下来，网站的数据量相当庞大，在短时间内不可能完成爬虫工作，在下一次启动爬虫时难道再将已经做过的工作再做一次？当然不行，这样的爬虫太不友好。那么我们如何来解决断点续传问题呢？</h5> 
<h4 id="我这里的方法是将已经爬取过的小说每一章的链接存入mongodb数据库的一个集合中在爬虫工作时首先检测要爬取的章节链接是否在这个集合中">我这里的方法是，将已经爬取过的小说每一章的链接存入Mongodb数据库的一个集合中。在爬虫工作时首先检测，要爬取的章节链接是否在这个集合中：</h4> 
<h4 id="如果在说明这个章节已经爬取过不需要再次爬取跳过">如果在，说明这个章节已经爬取过，不需要再次爬取，跳过；</h4> 
<h4 id="如果不在说明这个章节没有爬取过则爬取这个章节爬取完成后将这个章节链接存入集合中">如果不在，说明这个章节没有爬取过，则爬取这个章节。爬取完成后，将这个章节链接存入集合中；</h4> 
<h5 id="如此我们就完美实现了断点续传问题十分好用">如此，我们就完美实现了断点续传问题，十分好用。</h5> 
<h4 id="接下来贴出整个项目代码">接下来贴出整个项目代码：</h4> 
<h5 id="注释我写的相当详细熟悉一下就可以看懂">注释我写的相当详细，熟悉一下就可以看懂。</h5> 
<p>items.py</p> 
<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-preprocessor"># -*- coding: utf-8 -*-</span>

<span class="hljs-preprocessor"># Define here the models for your scraped items</span>
<span class="hljs-preprocessor">#</span>
<span class="hljs-preprocessor"># See documentation in:</span>
<span class="hljs-preprocessor"># https://doc.scrapy.org/en/latest/topics/items.html</span>

import scrapy


class DingdianxiaoshuoItem(scrapy<span class="hljs-preprocessor">.Item</span>):
    <span class="hljs-preprocessor"># define the fields for your item here like:</span>
    <span class="hljs-preprocessor"># name = scrapy.Field()</span>
    <span class="hljs-preprocessor">#小说名字</span>
    novel_name=scrapy<span class="hljs-preprocessor">.Field</span>()
    <span class="hljs-preprocessor">#小说类别</span>
    novel_family=scrapy<span class="hljs-preprocessor">.Field</span>()
    <span class="hljs-preprocessor">#小说主页地址</span>
    novel_url=scrapy<span class="hljs-preprocessor">.Field</span>()
    <span class="hljs-preprocessor">#小说作者</span>
    novel_author=scrapy<span class="hljs-preprocessor">.Field</span>()
    <span class="hljs-preprocessor">#小说状态</span>
    novel_status=scrapy<span class="hljs-preprocessor">.Field</span>()
    <span class="hljs-preprocessor">#小说字数</span>
    novel_number=scrapy<span class="hljs-preprocessor">.Field</span>()
    <span class="hljs-preprocessor">#小说所有章节页面</span>
    novel_all_section_url= scrapy<span class="hljs-preprocessor">.Field</span>()
    <span class="hljs-preprocessor">#小说最后更新时间</span>
    novel_updatetime=scrapy<span class="hljs-preprocessor">.Field</span>()

    <span class="hljs-preprocessor">#存放小说的章节地址，程序中存放的是一个列表</span>
    novel_section_urls=scrapy<span class="hljs-preprocessor">.Field</span>()

    <span class="hljs-preprocessor">#存放小说的章节地址和小说章节名称的对应关系，程序中存储的是一个字典</span>
    section_url_And_section_name=scrapy<span class="hljs-preprocessor">.Field</span>()

</code></pre> 
<p>dingdian.py</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span>
<span class="hljs-keyword">import</span> scrapy
<span class="hljs-keyword">from</span> scrapy <span class="hljs-keyword">import</span> Selector
<span class="hljs-keyword">from</span> dingdianxiaoshuo.items <span class="hljs-keyword">import</span> DingdianxiaoshuoItem

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">dingdian</span><span class="hljs-params">(scrapy.Spider)</span>:</span>
    name=<span class="hljs-string">"dingdian"</span>
    allowed_domains=[<span class="hljs-string">"23us.so"</span>]
    start_urls = [<span class="hljs-string">'http://www.23us.so/top/allvisit_1.html'</span>]
    server_link=<span class="hljs-string">'http://www.23us.so/top/allvisit_'</span>
    link_last=<span class="hljs-string">'.html'</span>

    <span class="hljs-comment">#从start_requests发送请求</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">start_requests</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">yield</span> scrapy.Request(url = self.start_urls[<span class="hljs-number">0</span>], callback = self.parse1)


    <span class="hljs-comment">#获取总排行榜每个页面的链接</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse1</span><span class="hljs-params">(self, response)</span>:</span>
        items=[]
        res = Selector(response)
        <span class="hljs-comment">#获取总排行榜小说页码数</span>
        max_num=res.xpath(<span class="hljs-string">'//*[@id="pagestats"]/text()'</span>).extract_first()
        max_num=max_num.split(<span class="hljs-string">'/'</span>)[<span class="hljs-number">1</span>]
        print(<span class="hljs-string">"总排行榜最大页面数为："</span>+max_num)
        <span class="hljs-comment">#for i in max_num+1:</span>
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,int(max_num)):
            <span class="hljs-comment">#构造总排行榜中每个页面的链接</span>
            page_url=self.server_link+str(i)+self.link_last
            <span class="hljs-keyword">yield</span> scrapy.Request(url=page_url,meta={<!-- --><span class="hljs-string">'items'</span>:items},callback=self.parse2)


    <span class="hljs-comment">#访问总排行榜的每个页面</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse2</span><span class="hljs-params">(self,response)</span>:</span>
        print(response.url)
        items=response.meta[<span class="hljs-string">'items'</span>]
        res=Selector(response)
        <span class="hljs-comment">#获得页面上所有小说主页链接地址</span>
        novel_urls=res.xpath(<span class="hljs-string">'//td/a[not(@title)]/@href'</span>).extract()
        <span class="hljs-comment">#获得页面上所有小说的名称</span>
        novel_names=res.xpath(<span class="hljs-string">'//td/a[not(@title)]/text()'</span>).extract()

        page_novel_number=len(novel_urls)
        <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> range(page_novel_number):
            item=DingdianxiaoshuoItem()
            item[<span class="hljs-string">'novel_name'</span>]=novel_names[index]
            item[<span class="hljs-string">'novel_url'</span>] =novel_urls[index]
            items.append(item)

        <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> items:
            <span class="hljs-comment">#访问每个小说主页,传递novel_name</span>
            <span class="hljs-keyword">yield</span> scrapy.Request(url=item[<span class="hljs-string">'novel_url'</span>],meta = {<!-- --><span class="hljs-string">'item'</span>:item},callback = self.parse3)

    <span class="hljs-comment">#访问小说主页，继续完善item</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse3</span><span class="hljs-params">(self, response)</span>:</span>
        <span class="hljs-comment">#接收传递的item</span>
        item=response.meta[<span class="hljs-string">'item'</span>]
        <span class="hljs-comment">#写入小说类别</span>
        item[<span class="hljs-string">'novel_family'</span>]=response.xpath(<span class="hljs-string">'//table/tr[1]/td[1]/a/text()'</span>).extract_first()
        <span class="hljs-comment">#写入小说作者</span>
        item[<span class="hljs-string">'novel_author'</span>]=response.xpath(<span class="hljs-string">'//table/tr[1]/td[2]/text()'</span>).extract_first()
        <span class="hljs-comment">#写入小说状态</span>
        item[<span class="hljs-string">'novel_status'</span>]=response.xpath(<span class="hljs-string">'//table/tr[1]/td[3]/text()'</span>).extract_first()
        <span class="hljs-comment">#写入小说最后更新时间</span>
        item[<span class="hljs-string">'novel_updatetime'</span>]=response.xpath(<span class="hljs-string">'//table/tr[2]/td[3]/text()'</span>).extract_first()
        <span class="hljs-comment">#写入小说全部章节页面</span>
        item[<span class="hljs-string">'novel_all_section_url'</span>]=response.xpath(<span class="hljs-string">'//p[@class="btnlinks"]/a[1]/@href'</span>).extract_first()
        url=response.xpath(<span class="hljs-string">'//p[@class="btnlinks"]/a[@class="read"]/@href'</span>).extract_first()
        <span class="hljs-comment">#访问显示有全部章节地址的页面</span>
        print(<span class="hljs-string">"即将访问"</span>+item[<span class="hljs-string">'novel_name'</span>]+<span class="hljs-string">"全部章节地址"</span>)
        <span class="hljs-comment">#yield item</span>
        <span class="hljs-keyword">yield</span>  scrapy.Request(url=url,meta={<!-- --><span class="hljs-string">'item'</span>:item},callback=self.parse4)

    <span class="hljs-comment">#将小说所有章节的地址和名称构造列表存入item</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse4</span><span class="hljs-params">(self, response)</span>:</span>
        <span class="hljs-comment">#print("这是parse4")</span>
        <span class="hljs-comment">#接收传递的item</span>
        item=response.meta[<span class="hljs-string">'item'</span>]
        <span class="hljs-comment">#这里是一个列表，存放小说所有章节地址</span>
        section_urls=response.xpath(<span class="hljs-string">'//table/tr/td/a/@href'</span>).extract()
        <span class="hljs-comment">#这里是一个列表，存放小说所有章节名称</span>
        section_names=response.xpath(<span class="hljs-string">'//table/tr/td/a/text()'</span>).extract()

        item[<span class="hljs-string">"novel_section_urls"</span>]=section_urls
        <span class="hljs-comment">#计数器</span>
        index=<span class="hljs-number">0</span>
        <span class="hljs-comment">#建立哈希表，存储章节地址和章节名称的对应关系</span>
        section_url_And_section_name=dict(zip(section_urls,section_names))
        <span class="hljs-comment">#将对应关系，写入item</span>
        item[<span class="hljs-string">"section_url_And_section_name"</span>]=section_url_And_section_name


        <span class="hljs-keyword">yield</span> item

</code></pre> 
<p>settings.py</p> 
<pre class="prettyprint"><code class=" hljs vala"><span class="hljs-preprocessor"># -*- coding: utf-8 -*-</span>

<span class="hljs-preprocessor"># Scrapy settings for dingdianxiaoshuo project</span>
<span class="hljs-preprocessor">#</span>
<span class="hljs-preprocessor"># For simplicity, this file contains only settings considered important or</span>
<span class="hljs-preprocessor"># commonly used. You can find more settings consulting the documentation:</span>
<span class="hljs-preprocessor">#</span>
<span class="hljs-preprocessor">#     https://doc.scrapy.org/en/latest/topics/settings.html</span>
<span class="hljs-preprocessor">#     https://doc.scrapy.org/en/latest/topics/downloader-middleware.html</span>
<span class="hljs-preprocessor">#     https://doc.scrapy.org/en/latest/topics/spider-middleware.html</span>

BOT_NAME = <span class="hljs-string">'dingdianxiaoshuo'</span>

SPIDER_MODULES = [<span class="hljs-string">'dingdianxiaoshuo.spiders'</span>]
NEWSPIDER_MODULE = <span class="hljs-string">'dingdianxiaoshuo.spiders'</span>


<span class="hljs-preprocessor"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span>
<span class="hljs-preprocessor">#USER_AGENT = 'dingdianxiaoshuo (+http://www.yourdomain.com)'</span>

<span class="hljs-preprocessor"># Obey robots.txt rules</span>
ROBOTSTXT_OBEY = False

<span class="hljs-preprocessor"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span>
<span class="hljs-preprocessor">#CONCURRENT_REQUESTS = 32</span>

<span class="hljs-preprocessor"># Configure a delay for requests for the same website (default: 0)</span>
<span class="hljs-preprocessor"># See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay</span>
<span class="hljs-preprocessor"># See also autothrottle settings and docs</span>
DOWNLOAD_DELAY = <span class="hljs-number">0.25</span>


<span class="hljs-preprocessor">#CLOSESPIDER_TIMEOUT = 60 # 后结束爬虫</span>


<span class="hljs-preprocessor"># The download delay setting will honor only one of:</span>
<span class="hljs-preprocessor">#CONCURRENT_REQUESTS_PER_DOMAIN = 16</span>
<span class="hljs-preprocessor">#CONCURRENT_REQUESTS_PER_IP = 16</span>

<span class="hljs-preprocessor"># Disable cookies (enabled by default)</span>
COOKIES_ENABLED = False

<span class="hljs-preprocessor"># Disable Telnet Console (enabled by default)</span>
<span class="hljs-preprocessor">#TELNETCONSOLE_ENABLED = False</span>

<span class="hljs-preprocessor"># Override the default request headers:</span>
<span class="hljs-preprocessor">#DEFAULT_REQUEST_HEADERS = {<!-- --></span>
<span class="hljs-preprocessor">#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',</span>
<span class="hljs-preprocessor">#   'Accept-Language': 'en',</span>
<span class="hljs-preprocessor">#}</span>

<span class="hljs-preprocessor"># Enable or disable spider middlewares</span>
<span class="hljs-preprocessor"># See https://doc.scrapy.org/en/latest/topics/spider-middleware.html</span>
<span class="hljs-preprocessor">#SPIDER_MIDDLEWARES = {<!-- --></span>
<span class="hljs-preprocessor">#    'dingdianxiaoshuo.middlewares.DingdianxiaoshuoSpiderMiddleware': 543,</span>
<span class="hljs-preprocessor">#}</span>

<span class="hljs-preprocessor"># Enable or disable downloader middlewares</span>
<span class="hljs-preprocessor"># See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html</span>
<span class="hljs-preprocessor">#DOWNLOADER_MIDDLEWARES = {<!-- --></span>
<span class="hljs-preprocessor">#    'dingdianxiaoshuo.middlewares.DingdianxiaoshuoDownloaderMiddleware': 543,</span>
<span class="hljs-preprocessor">#}</span>

<span class="hljs-preprocessor"># Enable or disable extensions</span>
<span class="hljs-preprocessor"># See https://doc.scrapy.org/en/latest/topics/extensions.html</span>
<span class="hljs-preprocessor">#EXTENSIONS = {<!-- --></span>
<span class="hljs-preprocessor">#    'scrapy.extensions.telnet.TelnetConsole': None,</span>
<span class="hljs-preprocessor">#}</span>

<span class="hljs-preprocessor"># Configure item pipelines</span>
<span class="hljs-preprocessor"># See https://doc.scrapy.org/en/latest/topics/item-pipeline.html</span>
ITEM_PIPELINES = {
    <span class="hljs-string">'dingdianxiaoshuo.pipelines.DingdianxiaoshuoPipeline'</span>: <span class="hljs-number">300</span>,
}

<span class="hljs-preprocessor"># Enable and configure the AutoThrottle extension (disabled by default)</span>
<span class="hljs-preprocessor"># See https://doc.scrapy.org/en/latest/topics/autothrottle.html</span>
<span class="hljs-preprocessor">#AUTOTHROTTLE_ENABLED = True</span>
<span class="hljs-preprocessor"># The initial download delay</span>
<span class="hljs-preprocessor">#AUTOTHROTTLE_START_DELAY = 5</span></code></pre> 
<p>pipeline.py</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span>

<span class="hljs-comment"># Define your item pipelines here</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># Don't forget to add your pipeline to the ITEM_PIPELINES setting</span>
<span class="hljs-comment"># See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html</span>

<span class="hljs-comment">#因为爬取整个网站时间较长，这里为了实现断点续传，我们把每个小说下载完成的</span>
<span class="hljs-comment">#章节地址存入数据库一个单独的集合里，记录已完成抓取的小说章节</span>

<span class="hljs-keyword">from</span> pymongo <span class="hljs-keyword">import</span> MongoClient
<span class="hljs-keyword">from</span> urllib <span class="hljs-keyword">import</span> request
<span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup

<span class="hljs-comment">#在pipeline中我们将实现下载每个小说，存入MongoDB数据库</span>

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DingdianxiaoshuoPipeline</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_item</span><span class="hljs-params">(self, item, spider)</span>:</span>
        <span class="hljs-comment">#print("马衍硕")</span>
        <span class="hljs-comment">#如果获取章节链接进行如下操作</span>
        <span class="hljs-keyword">if</span> <span class="hljs-string">"novel_section_urls"</span> <span class="hljs-keyword">in</span> item:
            <span class="hljs-comment"># 获取Mongodb链接</span>
            client = MongoClient(<span class="hljs-string">"mongodb://127.0.0.1:27017"</span>)
            <span class="hljs-comment">#连接数据库</span>
            db =client.dingdian
            <span class="hljs-comment">#获取小说名称</span>
            novel_name=item[<span class="hljs-string">'novel_name'</span>]
            <span class="hljs-comment">#根据小说名字，使用集合，没有则创建</span>
            novel=db[novel_name]

            <span class="hljs-comment">#使用记录已抓取网页的集合，没有则创建</span>
            section_url_downloaded_collection=db.section_url_collection

            index=<span class="hljs-number">0</span>
            print(<span class="hljs-string">"正在下载："</span>+item[<span class="hljs-string">"novel_name"</span>])


            <span class="hljs-comment">#根据小说每个章节的地址，下载小说各个章节</span>
            <span class="hljs-keyword">for</span> section_url <span class="hljs-keyword">in</span> item[<span class="hljs-string">'novel_section_urls'</span>]:

                <span class="hljs-comment">#根据对应关系，找出章节名称</span>
                section_name=item[<span class="hljs-string">"section_url_And_section_name"</span>][section_url]
                <span class="hljs-comment">#如果将要下载的小说章节没有在section_url_collection集合中，也就是从未下载过，执行下载</span>
                <span class="hljs-comment">#否则跳过</span>
                <span class="hljs-keyword">if</span>  <span class="hljs-keyword">not</span> section_url_downloaded_collection.find_one({<!-- --><span class="hljs-string">"url"</span>:section_url}):
                    <span class="hljs-comment">#使用urllib库获取网页HTML</span>
                    response = request.Request(url=section_url)
                    download_response = request.urlopen(response)
                    download_html = download_response.read().decode(<span class="hljs-string">'utf-8'</span>)
                    <span class="hljs-comment">#利用BeautifulSoup对HTML进行处理，截取小说内容</span>
                    soup_texts = BeautifulSoup(download_html, <span class="hljs-string">'lxml'</span>)
                    content=soup_texts.find(<span class="hljs-string">"dd"</span>,attrs={<!-- --><span class="hljs-string">"id"</span>:<span class="hljs-string">"contents"</span>}).getText()


                    <span class="hljs-comment">#向Mongodb数据库插入下载完的小说章节内容</span>
                    novel.insert({<!-- --><span class="hljs-string">"novel_name"</span>: item[<span class="hljs-string">'novel_name'</span>], <span class="hljs-string">"novel_family"</span>: item[<span class="hljs-string">'novel_family'</span>],
                                  <span class="hljs-string">"novel_author"</span>:item[<span class="hljs-string">'novel_author'</span>], <span class="hljs-string">"novel_status"</span>:item[<span class="hljs-string">'novel_status'</span>],
                                  <span class="hljs-string">"section_name"</span>:section_name,
                                  <span class="hljs-string">"content"</span>: content})
                    index+=<span class="hljs-number">1</span>
                    <span class="hljs-comment">#下载完成，则将章节地址存入section_url_downloaded_collection集合</span>
                    section_url_downloaded_collection.insert({<!-- --><span class="hljs-string">"url"</span>:section_url})


        print(<span class="hljs-string">"下载完成："</span>+item[<span class="hljs-string">'novel_name'</span>])
        <span class="hljs-keyword">return</span> item
</code></pre> 
<h5 id="三启动项目查看运行结果">三、启动项目，查看运行结果：</h5> 
<h5 id="程序编写完成后我们进入项目所在目录键入命令启动项目">程序编写完成后，我们进入项目所在目录，键入命令启动项目：</h5> 
<pre class="prettyprint"><code class=" hljs ">scrapy crawl dingdian </code></pre> 
<p>启动项目后，我们通过Mongodb可视化工具–RoBo看到，我们成功爬取了小说网站，接下来的问题交给时间。 <br> <img src="https://images2.imgbox.com/48/b7/ybJ7MwYd_o.png" alt="这里写图片描述" title=""> </p> 
<p><img src="https://images2.imgbox.com/28/10/TUAYxyNV_o.png" alt="这里写图片描述" title=""></p> 
<h4 id="当想中断爬虫时直接关掉控制台下次开启爬虫时将不会重复上次的工作这就是断点续传的美妙之处严格意义上不会在上次终止的地点开始爬取但是不会重复已经爬取的工作">当想中断爬虫时，直接关掉控制台。下次开启爬虫时将不会重复上次的工作，这就是断点续传的美妙之处。（严格意义上不会在上次终止的地点开始爬取，但是不会重复已经爬取的工作）</h4> 
<h5 id="后续将会开辟scrapy系列博客专门记录scrapy架构的爬虫工作">后续将会开辟scrapy系列博客，专门记录scrapy架构的爬虫工作。</h5>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/bcf8fa92c1d62fe999a6cd9417cbe1b9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">什么是阻抗匹配以及为什么要阻抗匹配</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/761379cecbd1df4b5f12aabc9a91ae19/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Go踩坑之 time.Parse: month out of range</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>