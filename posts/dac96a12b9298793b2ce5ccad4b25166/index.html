<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>大数据培训 Hive 相关知识的全面总结 - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="大数据培训 Hive 相关知识的全面总结" />
<meta property="og:description" content="一、了解Hive
1、Hive的概念及架构
Hive 是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载（ETL ），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。Hive 定义了简单的类 SQL 查询语言，称为 HQL ，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 的开发者开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。
「Hive是SQL解析引擎」，它将SQL语句转译成Map/Reduce Job然后在Hadoop执行。「Hive的表其实就是HDFS的目录」，按表名把文件夹分开。如果是分区表，则分区值是子文件夹，可以直接在Map/Reduce Job里使用这些数据。
「Hive相当于hadoop的客户端工具」，部署时不一定放在集群管理节点中，也可以放在某个节点上。
❝
「数据仓库」，英文名称为「Data Warehouse」，可简写为DW或DWH。数据仓库，是为企业所有级别的决策制定过程，提供所有类型数据支持的战略集合。它出于分析性报告和决策支持目的而创建。为需要业务智能的企业，提供指导业务流程改进、监视时间、成本、质量以及控制。
❞
❝
Hive的版本介绍：0.13和.14版本，稳定版本，但是不支持更新删除操作。1.2.1和1.2.2 版本，稳定版本，为Hive2版本（是主流版本）1.2.1的程序只能连接hive1.2.1 的hiveserver2
❞
2、Hive与传统数据库比较
查询语言。类 SQL 的查询语言 HQL。熟悉 SQL 开发的开发者可以很方便的使用 Hive 进行开发。数据存储位置。所有 Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中。数据格式。Hive 中没有定义专门的数据格式。而在数据库中，所有数据都会按照一定的组织存储，因此，数据库加载数据的过程会比较耗时。数据更新。Hive 对数据的改写和添加比较弱化，0.14版本之后支持，需要启动配置项。而数据库中的数据通常是需要经常进行修改的。索引。Hive 在加载数据的过程中不会对数据进行任何处理。因此访问延迟较高。数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了 Hive 不适合在线数据查询。执行计算。Hive 中执行是通过 MapReduce 来实现的而数据库通常有自己的执行引擎。数据规模。由于 Hive 建立在集群上并可以利用 MapReduce 进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。 3、Hive的数据存储格式
Hive的数据存储基于Hadoop HDFS。Hive没有专门的数据文件格式，常见的有以下几种：TEXTFILE、SEQUENCEFILE、AVRO、RCFILE、ORCFILE、PARQUET。 「下面我们详细的看一下Hive的常见数据格式：」
「TextFile:」 「TEXTFILE 即正常的文本格式，是Hive默认文件存储格式」，因为大多数情况下源数据文件都是以text文件格式保存（便于查看验数和防止乱码）。此种格式的表文件在HDFS上是明文，可用hadoop fs -cat命令查看，从HDFS上get下来后也可以直接读取。 「TEXTFILE 存储文件默认每一行就是一条记录，可以指定任意的分隔符进行字段间的分割。但这个格式无压缩，需要的存储空间很大」。虽然可以结合Gzip、Bzip2、Snappy等使用，使用这种方式，Hive不会对数据进行切分，从而无法对数据进行并行操作。「一般只有与其他系统由数据交互的接口表采用TEXTFILE 格式，其他事实表和维度表都不建议使用。」" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/dac96a12b9298793b2ce5ccad4b25166/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-15T14:42:32+08:00" />
<meta property="article:modified_time" content="2022-07-15T14:42:32+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">大数据培训 Hive 相关知识的全面总结</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><strong>一、了解Hive</strong></p> 
<p><strong>1、Hive的概念及架构</strong></p> 
<p>Hive 是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载（ETL ），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。Hive 定义了简单的类 SQL 查询语言，称为 HQL ，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 的开发者开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。</p> 
<p>「Hive是SQL解析引擎」，它将SQL语句转译成Map/Reduce Job然后在Hadoop执行。「Hive的表其实就是HDFS的目录」，按表名把文件夹分开。如果是分区表，则分区值是子文件夹，可以直接在Map/Reduce Job里使用这些数据。</p> 
<p>「Hive相当于hadoop的客户端工具」，部署时不一定放在集群管理节点中，也可以放在某个节点上。</p> 
<blockquote> 
 <p>❝</p> 
 <p>「数据仓库」，英文名称为「Data Warehouse」，可简写为DW或DWH。数据仓库，是为企业所有级别的决策制定过程，提供所有类型数据支持的战略集合。它出于分析性报告和决策支持目的而创建。为需要业务智能的企业，提供指导业务流程改进、监视时间、成本、质量以及控制。</p> 
 <p>❞</p> 
 <p>❝</p> 
 <p>Hive的版本介绍：0.13和.14版本，稳定版本，但是不支持更新删除操作。1.2.1和1.2.2 版本，稳定版本，为Hive2版本（是主流版本）1.2.1的程序只能连接hive1.2.1 的hiveserver2</p> 
 <p>❞</p> 
</blockquote> 
<p><strong>2、Hive与传统数据库比较</strong></p> 
<p style="text-align:center;"><strong><img alt="" src="https://images2.imgbox.com/b5/c8/aKUfeHWe_o.png"></strong></p> 
<p> </p> 
<p></p> 
<ol><li>查询语言。类 SQL 的查询语言 HQL。熟悉 SQL 开发的开发者可以很方便的使用 Hive 进行开发。</li><li>数据存储位置。所有 Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中。</li><li>数据格式。Hive 中没有定义专门的数据格式。而在数据库中，所有数据都会按照一定的组织存储，因此，数据库加载数据的过程会比较耗时。</li><li>数据更新。Hive 对数据的改写和添加比较弱化，0.14版本之后支持，需要启动配置项。而数据库中的数据通常是需要经常进行修改的。</li><li>索引。Hive 在加载数据的过程中不会对数据进行任何处理。因此访问延迟较高。数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了 Hive 不适合在线数据查询。</li><li>执行计算。Hive 中执行是通过 MapReduce 来实现的而数据库通常有自己的执行引擎。</li><li>数据规模。由于 Hive 建立在集群上并可以利用 MapReduce 进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。</li></ol> 
<p><strong>3、Hive的数据存储格式</strong></p> 
<ul><li>Hive的数据存储基于Hadoop HDFS。</li><li>Hive没有专门的数据文件格式，常见的有以下几种：TEXTFILE、SEQUENCEFILE、AVRO、RCFILE、ORCFILE、PARQUET。</li></ul> 
<p><strong>「下面我们详细的看一下Hive的常见数据格式：」</strong></p> 
<ul><li><strong>「TextFile:」</strong></li></ul> 
<p><strong>「TEXTFILE 即正常的文本格式，是Hive默认文件存储格式」</strong>，因为大多数情况下源数据文件都是以text文件格式保存（便于查看验数和防止乱码）。此种格式的表文件在HDFS上是明文，可用hadoop fs -cat命令查看，从HDFS上get下来后也可以直接读取。 <strong>「TEXTFILE 存储文件默认每一行就是一条记录，可以指定任意的分隔符进行字段间的分割。但这个格式无压缩，需要的存储空间很大」</strong>。虽然可以结合Gzip、Bzip2、Snappy等使用，使用这种方式，Hive不会对数据进行切分，从而无法对数据进行并行操作。<strong>「一般只有与其他系统由数据交互的接口表采用TEXTFILE 格式，其他事实表和维度表都不建议使用。」</strong></p> 
<ul><li><strong>「RCFile」</strong>:</li></ul> 
<p><strong>「Record Columnar的缩写。是Hadoop中第一个列文件格式。能够很好的压缩和快速的查询性能。通常写操作比较慢」</strong>，比非列形式的文件格式需要更多的内存空间和计算量。<strong>「RCFile是一种行列存储相结合的存储方式」</strong>。首先，其将数据按行分块，保证同一个record在一个块上，避免读一个记录需要读取多个block。其次，块数据列式存储，有利于数据压缩和快速的列存取。</p> 
<ul><li><strong>「ORCFile」</strong>:</li></ul> 
<p>Hive从0.11版本开始提供了ORC的文件格式，<strong>「ORC文件」</strong>不仅仅是<strong>「一种列式文件存储格式」</strong>，最重要的是<strong>「有着很高的压缩比」</strong>，并且<strong>「对于MapReduce来说是可切分（Split）的」</strong>。因此，<strong>「在Hive中使用ORC作为表的文件存储格式，不仅可以很大程度的节省HDFS存储资源，而且对数据的查询和处理性能有着非常大的提升」</strong>，因为ORC较其他文件格式压缩比高，查询任务的输入数据量减少，使用的Task也就减少了。<strong>「ORC能很大程度的节省存储和计算资源，但它在读写时候需要消耗额外的CPU资源来压缩和解压缩」</strong>，当然这部分的CPU消耗是非常少的。</p> 
<ul><li><strong>「Parquet:」</strong></li></ul> 
<p>通常我们使用关系数据库存储结构化数据，而<strong>「关系数据库中使用数据模型都是扁平式的」</strong>，遇到诸如List、Map和自定义Struct的时候就需要用户在应用层解析。但是在大数据环境下，通常数据的来源是服务端的埋点数据，很可能需要把程序中的某些对象内容作为输出的一部分，而每一个对象都可能是嵌套的，所以如果能够原生的支持这种数据，这样在查询的时候就不需要额外的解析便能获得想要的结果_<a class="link-info" href="http://www.atguigu.com" rel="nofollow" title="大数据培训">大数据培训</a>。</p> 
<p>Parquet的灵感来自于2010年Google发表的Dremel论文，文中介绍了一种支持嵌套结构的存储格式，并且使用了列式存储的方式提升查询性能。<strong>「Parquet仅仅是一种存储格式，它是语言、平台无关的，并且不需要和任何一种数据处理框架绑定。这也是parquet相较于orc的仅有优势：支持嵌套结构」</strong>。Parquet 没有太多其他可圈可点的地方,比如他<strong>「不支持update操作(数据写成后不可修改),不支持ACID等.」</strong></p> 
<ul><li><strong>「SEQUENCEFILE」</strong>:</li></ul> 
<p><strong>「SequenceFile是Hadoop API 提供的一种二进制文件，它将数据以&lt;key,value&gt;的形式序列化到文件中」</strong>。这种二进制文件内部使用Hadoop 的标准的Writable 接口实现序列化和反序列化。它与Hadoop API中的MapFile 是互相兼容的。Hive 中的SequenceFile 继承自Hadoop API 的SequenceFile，不过它的key为空，使用value 存放实际的值， 这样是为了避免MR 在运行map 阶段的排序过程。<strong>「SequenceFile支持三种压缩选择：NONE, RECORD, BLOCK。Record压缩率低，一般建议使用BLOCK压缩。SequenceFile最重要的优点就是Hadoop原生支持较好，有API」</strong>，但除此之外平平无奇，实际生产中不会使用。</p> 
<ul><li><strong>「AVRO:」</strong></li></ul> 
<p><strong>「Avro是一种用于支持数据密集型的二进制文件格式。它的文件格式更为紧凑」</strong>，若要读取大量数据时，Avro能够提供更好的序列化和反序列化性能。并且<strong>「Avro数据文件天生是带Schema定义的」</strong>，所以它不需要开发者在API 级别实现自己的Writable对象。<strong>「Avro提供的机制使动态语言可以方便地处理Avro数据」</strong>。最近多个Hadoop 子项目都支持Avro 数据格式，如Pig 、Hive、Flume、Sqoop和Hcatalog。</p> 
<blockquote> 
 <p><strong>❝</strong></p> 
 <p>其中的<strong>「TextFile」</strong>、<strong>「RCFile」</strong>、<strong>「ORC」</strong>、<strong>「Parquet」</strong>为Hive<strong>「最常用的四大存储格式」</strong>它们的 存储效率及执行速度比较如下：ORCFile存储文件读操作效率最高，耗时比较（ORC&lt;Parquet&lt;RCFile&lt;TextFile）ORCFile存储文件占用空间少，压缩效率高(ORC&lt;Parquet&lt;RCFile&lt;TextFile)</p> 
 <p>❞</p> 
</blockquote> 
<p><strong>4、Hive操作客户端</strong></p> 
<p><strong>「常用的客户端有两个：CLI，JDBC/ODBC」</strong></p> 
<ul><li>CLI，即Shell命令行</li><li>JDBC/ODBC 是 Hive 的Java，与使用传统数据库JDBC的方式类似。</li></ul> 
<ol><li><strong>「Hive 将元数据存储在数据库中(metastore)，目前只支持 mysql、derby。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等；由解释器、编译器、优化器完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划（plan）的生成。生成的查询计划存储在 HDFS 中，并在随后由 MapReduce 调用执行。」</strong></li><li><strong>「Hive 的数据存储在 HDFS 中，大部分的查询由 MapReduce 完成（包含 * 的查询，比如 select * from table 不会生成 MapRedcue 任务）」</strong></li></ol> 
<p><strong>「Hive的metastore」</strong></p> 
<blockquote> 
 <p><strong>❝</strong></p> 
 <p>metastore是hive元数据的集中存放地。metastore默认使用内嵌的derby数据库作为存储引擎Derby引擎的缺点：一次只能打开一个会话使用MySQL作为外置存储引擎，可以多用户同时访问`</p> 
 <p>❞</p> 
</blockquote> 
<p><strong>二、Hive的基本语法</strong></p> 
<p><strong>1、Hive建表语法</strong></p> 
<p>CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name</p> 
<p>// 定义字段名，字段类型</p> 
<p>[(col_name data_type [COMMENT col_comment], ...)]</p> 
<p>// 给表加上注解</p> 
<p>[COMMENT table_comment]</p> 
<p>// 分区</p> 
<p>[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]</p> 
<p>// 分桶</p> 
<p>[CLUSTERED BY (col_name, col_name, ...)</p> 
<p>// 设置排序字段 升序、降序</p> 
<p>[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]</p> 
<p>[</p> 
<p>// 指定设置行、列分隔符</p> 
<p>[ROW FORMAT row_format]</p> 
<p>// 指定Hive储存格式：textFile、rcFile、SequenceFile 默认为：textFile</p> 
<p>[STORED AS file_format]</p> 
<p></p> 
<p>| STORED BY 'storage.handler.class.name' [ WITH SERDEPROPERTIES (...) ] (Note: only available starting with 0.6.0)</p> 
<p>]</p> 
<p>// 指定储存位置</p> 
<p>[LOCATION hdfs_path]</p> 
<p>// 跟外部表配合使用，比如：映射HBase表，然后可以使用HQL对hbase数据进行查询，当然速度比较慢</p> 
<p>[TBLPROPERTIES (property_name=property_value, ...)] (Note: only available starting with 0.6.0)</p> 
<p>[AS select_statement] (Note: this feature is only available starting with 0.5.0.)</p> 
<p><strong>建表格式1：全部使用默认建表方式</strong></p> 
<p>create table students</p> 
<p>(</p> 
<p>id bigint,</p> 
<p>name string,</p> 
<p>age int,</p> 
<p>gender string,</p> 
<p>clazz string</p> 
<p>)</p> 
<p>ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';</p> 
<p>// 必选，指定列分隔符</p> 
<p><strong>建表格式2：指定location （这种方式也比较常用）</strong></p> 
<p>create table students2</p> 
<p>(</p> 
<p>id bigint,</p> 
<p>name string,</p> 
<p>age int,</p> 
<p>gender string,</p> 
<p>clazz string</p> 
<p>)</p> 
<p>ROW FORMAT DELIMITED FIELDS TERMINATED BY ','</p> 
<p>LOCATION '/input1';</p> 
<p>// 指定Hive表的数据的存储位置，一般在数据已经上传到HDFS，想要直接使用，会指定Location，</p> 
<p>//通常Locaion会跟外部表一起使用，内部表一般使用默认的location</p> 
<p><strong>建表格式3：指定存储格式</strong></p> 
<p>create table students3</p> 
<p>(</p> 
<p>id bigint,</p> 
<p>name string,</p> 
<p>age int,</p> 
<p>gender string,</p> 
<p>clazz string</p> 
<p>)</p> 
<p>ROW FORMAT DELIMITED FIELDS TERMINATED BY ','</p> 
<p>STORED AS rcfile;</p> 
<p>// 指定储存格式为rcfile，inputFormat:RCFileInputFormat,outputFormat:RCFileOutputFormat，</p> 
<p>//如果不指定，默认为textfile，</p> 
<p>//注意：除textfile以外，其他的存储格式的数据都不能直接加载，需要使用从表加载的方式。</p> 
<p><strong>建表格式4：create table xxxx as select_statement(SQL语句) (这种方式比较常用)</strong></p> 
<p>注：</p> 
<ol><li>新建表不允许是外部表。</li><li>select后面表需要是已经存在的表，建表同时会加载数据。</li><li>会启动mapreduce任务去读取源表数据写入新表</li></ol> 
<p>create table students4 as select * from students2;</p> 
<p><strong>建表格式5：create table xxxx like table_name 只想建表，不需要加载数据</strong></p> 
<p>create table students5 like students;</p> 
<p><strong>2、Hive加载数据</strong></p> 
<p><strong>1）、使用hdfs dfs -put '本地数据' 'hive表对应的HDFS目录下'</strong></p> 
<p><strong>2)、使用 load data inpath</strong></p> 
<p><strong>「从hdfs导入数据」</strong>，路径可以是目录，会将目录下所有文件导入，但是文件格式必须一致</p> 
<p>// 将HDFS上的/input1目录下面的数据 移动至 students表对应的HDFS目录下</p> 
<p>// 注意是 移动！移动！移动！</p> 
<p>load data inpath '/input1/students.txt' into table students;</p> 
<p>// 清空表</p> 
<p>truncate table students;</p> 
<p>从本地文件系统导入</p> 
<p>// 加上 local 关键字 可以将Linux本地目录下的文件 上传到 hive表对应HDFS 目录下 原文件不会被删除</p> 
<p>load data local inpath '/usr/local/soft/data/students.txt' into table students;</p> 
<p>// overwrite 覆盖加载</p> 
<p>load data local inpath '/usr/local/soft/data/students.txt' overwrite into table students;</p> 
<p><strong>3)、create table xxx as SQL语句，表对表加载</strong></p> 
<p>create table test.aa as select * from test.bb</p> 
<p><strong>4)、insert into table xxxx SQL语句 （没有as），表对表加载:</strong></p> 
<p>// 将 students表的数据插入到students2</p> 
<p>//这是复制 不是移动 students表中的表中的数据不会丢失</p> 
<p>insert into table students2 select * from students;</p> 
<p>// 覆盖插入 把into 换成 overwrite</p> 
<p>insert overwrite table students2 select * from students;</p> 
<blockquote> 
 <p><strong>❝</strong></p> 
 <p><strong>「注」</strong>：1，如果建表语句没有指定存储路径，不管是外部表还是内部表，存储路径都是会默认在hive/warehouse/xx.db/表名的目录下。加载的数据如果在HDFS上会移动到该表的存储目录下。注意是移动，不是复制2，删除外部表，文件不会删除，对应目录也不会删除</p> 
 <p>❞</p> 
</blockquote> 
<p><strong>3、Hive 内部表（Managed tables）vs 外部表（External tables）</strong></p> 
<p><strong>「外部表和普通表的区别」</strong></p> 
<ol><li>外部表的路径可以自定义，内部表的路径需要在 hive/warehouse/目录下</li><li>删除表后，普通表数据文件和表信息都删除。外部表仅删除表信息</li></ol> 
<p><strong>1)、建表语句：</strong></p> 
<p>// 内部表</p> 
<p>create table students_internal</p> 
<p>(</p> 
<p>id bigint,</p> 
<p>name string,</p> 
<p>age int,</p> 
<p>gender string,</p> 
<p>clazz string</p> 
<p>)</p> 
<p>ROW FORMAT DELIMITED FIELDS TERMINATED BY ','</p> 
<p>LOCATION '/input2';</p> 
<p>// 外部表</p> 
<p>create external table students_external</p> 
<p>(</p> 
<p>id bigint,</p> 
<p>name string,</p> 
<p>age int,</p> 
<p>gender string,</p> 
<p>clazz string</p> 
<p>)</p> 
<p>ROW FORMAT DELIMITED FIELDS TERMINATED BY ','</p> 
<p>LOCATION '/input3';</p> 
<p><strong>2)、加载数据：</strong></p> 
<p>hive&gt; dfs -put /usr/local/soft/data/students.txt /input2/;</p> 
<p>hive&gt; dfs -put /usr/local/soft/data/students.txt /input3/;</p> 
<p><strong>3)、删除表：</strong></p> 
<p>hive&gt; drop table students_internal;</p> 
<p>Moved: 'hdfs://master:9000/input2' to trash at: hdfs://master:9000/user/root/.Trash/Current</p> 
<p>OK</p> 
<p>Time taken: 0.474 seconds</p> 
<p>hive&gt; drop table students_external;</p> 
<p>OK</p> 
<p>Time taken: 0.09 seconds</p> 
<blockquote> 
 <p><strong>❝</strong></p> 
 <p>1、可以看出，删除内部表的时候，表中的数据（HDFS上的文件）会被同表的元数据一起删除；删除外部表的时候，只会删除表的元数据，而不会删除表中的数据（HDFS上的文件）2、一般在公司中，使用外部表多一点，因为数据可以需要被多个程序使用，避免误删，通常外部表会结合location一起使用3、外部表还可以将其他数据源中的数据 映射到 hive中，比如说：hbase，ElasticSearch…4、设计外部表的初衷就是 让 表的元数据 与 数据 解耦</p> 
 <p>❞</p> 
</blockquote> 
<p><strong>4、Hive 分区</strong></p> 
<p>分区表实际上是在表的目录下在以分区命名，建子目录；作用：进行分区裁剪，避免全表扫描，减少MapReduce处理的数据量，提高效率 一般在公司的hive中，所有的表基本上都是分区表，通常按日期分区、地域分区；分区表在使用的时候记得加上分区字段；分区也不是越多越好，一般不超过3级，根据实际业务衡量</p> 
<blockquote> 
 <p><strong>❝</strong></p> 
 <p>分区的概念和分区表：分区表指的是在创建表时指定分区空间，实际上就是在hdfs上表的目录下再创建子目录。在使用数据时如果指定了需要访问的分区名称，则只会读取相应的分区，避免全表扫描，提高查询效率。</p> 
 <p>❞</p> 
</blockquote> 
<p><strong>1)、建立分区表：</strong></p> 
<p>create external table students_pt1</p> 
<p>(</p> 
<p>id bigint,</p> 
<p>name string,</p> 
<p>age int,</p> 
<p>gender string,</p> 
<p>clazz string</p> 
<p>)</p> 
<p>PARTITIONED BY(pt string)</p> 
<p>ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';</p> 
<p><strong>2)、增加一个分区：</strong></p> 
<p>alter table students_pt1 add partition(pt='20210904');</p> 
<p><strong>3)、删除一个分区：</strong></p> 
<p>alter table students_pt drop partition(pt='20210904');</p> 
<p><strong>4)、查看某个表的所有分区</strong></p> 
<p>// 推荐这种方式（直接从元数据中获取分区信息）</p> 
<p>show partitions students_pt;</p> 
<p>// 不推荐</p> 
<p>select distinct pt from students_pt;</p> 
<p><strong>5)、往分区中插入数据：</strong></p> 
<p>insert into table students_pt partition(pt='20210902') select * from students;</p> 
<p>load data local inpath '/usr/local/soft/data/students.txt' into table students_pt partition(pt='20210902');</p> 
<p><strong>6)、查询某个分区的数据：</strong></p> 
<p>// 全表扫描，不推荐，效率低</p> 
<p>select count(*) from students_pt;</p> 
<p>// 使用where条件进行分区裁剪，避免了全表扫描，效率高</p> 
<p>select count(*) from students_pt where pt='20210101';</p> 
<p>// 也可以在where条件中使用非等值判断</p> 
<p>select count(*) from students_pt where pt&lt;='20210112' and pt&gt;='20210110';</p> 
<p><strong>5、Hive动态分区</strong></p> 
<blockquote> 
 <p>❝</p> 
 <p>有的时候我们原始表中的数据里面包含了 ‘‘日期字段 dt’’，我们需要根据dt中不同的日期，分为不同的分区，将原始表改造成分区表。hive默认不开启动态分区动态分区：根据数据中某几列的不同的取值 划分 不同的分区</p> 
 <p>❞</p> 
</blockquote> 
<p># 表示开启动态分区</p> 
<p>hive&gt; set hive.exec.dynamic.partition=true;</p> 
<p># 表示动态分区模式：strict（需要配合静态分区一起使用）、nostrict</p> 
<p># strict：insert into table students_pt partition(dt='anhui',pt) select ......,pt from students;</p> 
<p>hive&gt; set hive.exec.dynamic.partition.mode=nostrict;</p> 
<p># 表示支持的最大的分区数量为1000，可以根据业务自己调整</p> 
<p>hive&gt; set hive.exec.max.dynamic.partitions.pernode=1000;</p> 
<p><strong>1)、建立原始表并加载数据</strong></p> 
<p>create table students_dt</p> 
<p>(</p> 
<p>id bigint,</p> 
<p>name string,</p> 
<p>age int,</p> 
<p>gender string,</p> 
<p>clazz string,</p> 
<p>dt string</p> 
<p>)</p> 
<p>ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';</p> 
<p><strong>2)、建立分区表并加载数据</strong></p> 
<p>create table students_dt_p</p> 
<p>(</p> 
<p>id bigint,</p> 
<p>name string,</p> 
<p>age int,</p> 
<p>gender string,</p> 
<p>clazz string</p> 
<p>)</p> 
<p>PARTITIONED BY(dt string)</p> 
<p>ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';</p> 
<p><strong>3)、使用动态分区插入数据</strong></p> 
<p>// 分区字段需要放在 select 的最后，如果有多个分区字段 同理，</p> 
<p>//它是按位置匹配，不是按名字匹配</p> 
<p>insert into table students_dt_p partition(dt) select id,name,age,gender,clazz,dt from students_dt;</p> 
<p>// 比如下面这条语句会使用age作为分区字段，而不会使用student_dt中的dt作为分区字段</p> 
<p>insert into table students_dt_p partition(dt) select id,name,age,gender,dt,age from students_dt;</p> 
<p><strong>4)、多级分区</strong></p> 
<p>create table students_year_month</p> 
<p>(</p> 
<p>id bigint,</p> 
<p>name string,</p> 
<p>age int,</p> 
<p>gender string,</p> 
<p>clazz string,</p> 
<p>year string,</p> 
<p>month string</p> 
<p>)</p> 
<p>ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';</p> 
<p>create table students_year_month_pt</p> 
<p>(</p> 
<p>id bigint,</p> 
<p>name string,</p> 
<p>age int,</p> 
<p>gender string,</p> 
<p>clazz string</p> 
<p>)</p> 
<p>PARTITIONED BY(year string,month string)</p> 
<p>ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';</p> 
<p>insert into table students_year_month_pt partition(year,month) select id,name,age,gender,clazz,year,month from students_year_month;</p> 
<p><strong>6、Hive分桶</strong></p> 
<p><strong>「分桶实际上是对文件（数据）的进一步切分；Hive默认关闭分桶；分桶的作用」</strong>：在往分桶表中插入数据的时候，会根据 clustered by 指定的字段 进行hash分组 对指定的buckets个数 进行取余，进而可以将数据分割成buckets个数个文件，以达到数据均匀分布，可以解决Map端的“数据倾斜”问题，方便我们取抽样数据，提高Map join效率；<strong>「分桶字段」</strong> 需要根据业务进行设定</p> 
<p><strong>1)、开启分桶开关</strong></p> 
<p>hive&gt; set hive.enforce.bucketing=true;</p> 
<p><strong>2)、建立分桶表</strong></p> 
<p>create table students_buks</p> 
<p>(</p> 
<p>id bigint,</p> 
<p>name string,</p> 
<p>age int,</p> 
<p>gender string,</p> 
<p>clazz string</p> 
<p>)</p> 
<p>CLUSTERED BY (clazz) into 12 BUCKETS</p> 
<p>ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';</p> 
<p><strong>3)、往分桶表中插入数据</strong></p> 
<p>// 直接使用load data 并不能将数据打散</p> 
<p>load data local inpath '/usr/local/soft/data/students.txt' into table students_buks;</p> 
<p>// 需要使用下面这种方式插入数据，才能使分桶表真正发挥作用</p> 
<p>insert into students_buks select * from students;</p> 
<p><strong>7、Hive连接JDBC</strong></p> 
<p><strong>1)、启动hiveserver2的服务</strong></p> 
<p>hive --service hiveserver2 &amp;</p> 
<p><strong>2)、 新建maven项目并添加两个依赖</strong></p> 
<p>&lt;dependency&gt;</p> 
<p>&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</p> 
<p>&lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</p> 
<p>&lt;version&gt;2.7.6&lt;/version&gt;</p> 
<p>&lt;/dependency&gt;</p> 
<p>&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-jdbc --&gt;</p> 
<p>&lt;dependency&gt;</p> 
<p>&lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</p> 
<p>&lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;</p> 
<p>&lt;version&gt;1.2.1&lt;/version&gt;</p> 
<p>&lt;/dependency&gt;</p> 
<p><strong>3)、 编写Java通过JDBC连接Hive</strong></p> 
<p>import java.sql.*;</p> 
<p>public class HiveJDBC {<!-- --></p> 
<p>public static void main(String[] args) throws ClassNotFoundException, SQLException {<!-- --></p> 
<p>Class.forName("org.apache.hive.jdbc.HiveDriver");</p> 
<p>Connection conn = DriverManager.getConnection("jdbc:hive2://master:10000/test3");</p> 
<p>Statement stat = conn.createStatement();</p> 
<p>ResultSet rs = stat.executeQuery("select * from students limit 10");</p> 
<p>while (rs.next()) {<!-- --></p> 
<p>int id = rs.getInt(1);</p> 
<p>String name = rs.getString(2);</p> 
<p>int age = rs.getInt(3);</p> 
<p>String gender = rs.getString(4);</p> 
<p>String clazz = rs.getString(5);</p> 
<p>System.out.println(id + "," + name + "," + age + "," + gender + "," + clazz);</p> 
<p>}</p> 
<p>rs.close();</p> 
<p>stat.close();</p> 
<p>conn.close();</p> 
<p>}</p> 
<p>}</p> 
<p><strong>三、Hive的数据类型</strong></p> 
<p><strong>1、基本数据类型</strong></p> 
<p><strong>数值型：</strong></p> 
<p>TINYINT — 微整型，只占用1个字节，只能存储0-255的整数。</p> 
<p>SMALLINT– 小整型，占用2个字节，存储范围–32768 到 32767。</p> 
<p>INT– 整型，占用4个字节，存储范围-2147483648到2147483647。</p> 
<p>BIGINT– 长整型，占用8个字节，存储范围-2^63到2^63-1。</p> 
<p><strong>布尔型</strong></p> 
<p>BOOLEAN — TRUE/FALSE</p> 
<p><strong>浮点型</strong></p> 
<p>FLOAT– 单精度浮点数。</p> 
<p>DOUBLE– 双精度浮点数。</p> 
<p><strong>字符串型</strong></p> 
<p>STRING– 不设定长度。</p> 
<p><strong>2、日期类型</strong></p> 
<ul><li>时间戳 timestamp</li><li>日期 date</li></ul> 
<p>create table testDate(</p> 
<p>ts timestamp</p> 
<p>,dt date</p> 
<p>) row format delimited fields terminated by ',';</p> 
<p>// 2021-01-14 14:24:57.200,2021-01-11</p> 
<ul><li>时间戳与时间字符串转换</li></ul> 
<p>// from_unixtime 传入一个时间戳以及pattern（yyyy-MM-dd）</p> 
<p>//可以将 时间戳转换成对应格式的字符串</p> 
<p>select from_unixtime(1630915221,'yyyy年MM月dd日 HH时mm分ss秒')</p> 
<p>// unix_timestamp 传入一个时间字符串以及pattern，</p> 
<p>//可以将字符串按照pattern转换成时间戳</p> 
<p>select unix_timestamp('2021年09月07日 11时00分21秒','yyyy年MM月dd日 HH时mm分ss秒');</p> 
<p>select unix_timestamp('2021-01-14 14:24:57.200')</p> 
<p><strong>3、复杂数据类型</strong></p> 
<p><strong>「主要有三种复杂数据类型：Structs，Maps，Arrays」</strong></p> 
<p><strong>四、Hive HQL使用语法</strong></p> 
<blockquote> 
 <p><strong>❝</strong></p> 
 <p>我们知道SQL语言可以分为5大类：(1）DDL(Data Definition Language) 数据定义语言用来定义数据库对象：数据库，表，列等。关键字：create，drap,alter等( 2）DML(Data Manipulation Language) 数据操作语言用来对数据库中表的数据进行增删改。关键字：insert,delete,update等( 3）DQL(Data Query Language)数据查询语言用来查询数据库表的记录（数据）。关键字：select,where 等( 4）DCL(Data Control Language) 数据控制语言用来定义数据库的访问权限和安全级别，及创建用户。关键字：GRANT，REVOKE等(5)TCL(Transaction Control Language) 事务控制语言T CL经常被用于快速原型开发、脚本编程、GUI和测试等方面，关键字: commit、rollback等。</p> 
 <p>❞</p> 
</blockquote> 
<p><strong>1、HQL语法-DDL</strong></p> 
<p>创建数据库 create database xxxxx;</p> 
<p>查看数据库 show databases；</p> 
<p>删除数据库 drop database tmp;</p> 
<p>强制删除数据库：drop database tmp cascade;</p> 
<p>查看表：SHOW TABLES；</p> 
<p>查看表的元信息：</p> 
<p>desc test_table;</p> 
<p>describe extended test_table;</p> 
<p>describe formatted test_table;</p> 
<p>查看建表语句：show create table table_XXX</p> 
<p>重命名表：</p> 
<p>alter table test_table rename to new_table;</p> 
<p>修改列数据类型：alter table lv_test change column colxx string;</p> 
<p>增加、删除分区：</p> 
<p>alter table test_table add partition (pt=xxxx)</p> 
<p>alter table test_table drop if exists partition(...);</p> 
<p><strong>2、HQL语法-DML</strong></p> 
<p>where 用于过滤，分区裁剪，指定条件</p> 
<p>join 用于两表关联，left outer join ，join，mapjoin（1.2版本后默认开启）</p> 
<p>group by 用于分组聚合，通常结合聚合函数一起使用</p> 
<p>order by 用于全局排序，要尽量避免排序，是针对全局排序的，即对所有的reduce输出是有序的</p> 
<p>sort by :当有多个reduce时，只能保证单个reduce输出有序，不能保证全局有序</p> 
<p>cluster by = distribute by + sort by</p> 
<p>distinct 去重</p> 
<p><strong>五、Hive HQL使用注意</strong></p> 
<ol><li>count(1)、count(*)和count(字段名)执行效果上的区别</li><li>count(*)包括了所有的列，相当于行数，在统计结果的时候，不会忽略列值为NULL</li><li>count(1)包括了忽略所有列，用1代表代码行，在统计结果的时候，不会忽略列值为NULL</li><li>count(列名)只包括列名那一列，在统计结果的时候，会忽略列值为空（这里的空不是只空字符串或者0，而是表示null）的计数，即某个字段值为NULL时，不统计。</li><li>HQL 执行优先级：from、where、 group by 、having、order by、join、select 、limit</li><li>where 条件里不支持不等式子查询，实际上是支持 in、not in、exists、not exists</li><li>hive中大小写不敏感</li><li>在hive中，数据中如果有null字符串，加载到表中的时候会变成 null （不是字符串）如果需要判断 null，使用 某个字段名 is null 这样的方式来判断;或者使用 nvl() 函数，不能 直接 某个字段名 == null</li><li>使用explain查看SQL执行计划</li></ol> 
<p><strong>六、Hive 的函数使用</strong></p> 
<p><strong>1、Hive-常用函数</strong></p> 
<p><strong>（1）关系运算</strong></p> 
<blockquote> 
 <p><strong>❝</strong></p> 
 <p>// 等值比较 = == &lt;=&gt;// 不等值比较 != &lt;&gt;// 区间比较：select * from default.students where id between 1500100001 and 1500100010;// 空值/非空值判断：is null、is not null、nvl()、isnull()// like、rlike、regexp用法</p> 
 <p>❞</p> 
</blockquote> 
<p><strong>（2）数值计算</strong></p> 
<p>取整函数(四舍五入)：round</p> 
<p>向上取整：ceil</p> 
<p>向下取整：floor</p> 
<p><strong>（3） 条件函数</strong></p> 
<ul><li>if：if(表达式,如果表达式成立的返回值,如果表达式不成立的返回值)</li></ul> 
<p>select if(1&gt;0,1,0);</p> 
<p>select if(1&gt;0,if(-1&gt;0,-1,1),0);</p> 
<ul><li>COALESCE</li></ul> 
<p>select COALESCE(null,'1','2'); // 1 从左往右 一次匹配 直到非空为止</p> 
<p>select COALESCE('1',null,'2'); // 1</p> 
<ul><li>case when … then … else … end</li></ul> 
<p>select score</p> 
<p>,case when score&gt;120 then '优秀'</p> 
<p>when score&gt;100 then '良好'</p> 
<p>when score&gt;90 then '及格'</p> 
<p>else '不及格'</p> 
<p>end as pingfen</p> 
<p>from default.score limit 20;</p> 
<p># 注意条件的顺序</p> 
<p><strong>（4）日期函数</strong></p> 
<p>select from_unixtime(1610611142,'YYYY/MM/dd HH:mm:ss');</p> 
<p>select from_unixtime(unix_timestamp(),'YYYY/MM/dd HH:mm:ss');</p> 
<p>// '2021年01月14日' -&gt; '2021-01-14'</p> 
<p>select from_unixtime(unix_timestamp('2021年01月14日','yyyy年MM月dd日'),'yyyy-MM-dd');</p> 
<p>// "04牛2021数加16逼" -&gt; "2021/04/16"</p> 
<p>select from_unixtime(unix_timestamp("04牛2021数加16逼","MM牛yyyy数加dd逼"),"yyyy/MM/dd");</p> 
<p><strong>（5) 字符串函数</strong></p> 
<p>concat('123','456'); // 123456</p> 
<p>concat('123','456',null); // NULL</p> 
<p>select concat_ws('#','a','b','c'); // a#b#c</p> 
<p>select concat_ws('#','a','b','c',NULL); // a#b#c 可以指定分隔符，并且会自动忽略NULL</p> 
<p>select concat_ws("|",cast(id as string),name,cast(age as string),gender,clazz) from students limit 10;</p> 
<p>select substring("abcdefg",1); // abcdefg HQL中涉及到位置的时候 是从1开始计数</p> 
<p>// '2021/01/14' -&gt; '2021-01-14'</p> 
<p>select concat_ws("-",substring('2021/01/14',1,4),substring('2021/01/14',6,2),substring('2021/01/14',9,2));</p> 
<p>select split("abcde,fgh",","); // ["abcde","fgh"]</p> 
<p>select split("a,b,c,d,e,f",",")[2]; // c</p> 
<p>select explode(split("abcde,fgh",",")); // abcde</p> 
<p>// fgh</p> 
<p>// 解析json格式的数据</p> 
<p>select get_json_object('{"name":"zhangsan","age":18,"score":[{"course_name":"math","score":100},{"course_name":"english","score":60}]}',"$.score[0].score"); // 100</p> 
<p><strong>2、Hive-高级函数</strong></p> 
<p><strong>（1）窗口函数（开窗函数）：用户分组中开窗</strong></p> 
<p>在sql中有一类函数叫做`聚合函数,例如sum()、avg()、max()等等,这类函数可以将多行数据按照规则聚集为一行,一般来讲聚集后的行数是要少于聚集前的行数的.但是有时我们想要既显示聚集前的数据,又要显示聚集后的数据,这时我们便引入了窗口函数。（开创函数，我们一般用于分组中求 TopN问题）</p> 
<p><strong>「样例演示」</strong>：</p> 
<p>数据：</p> 
<p>111,69,class1,department1</p> 
<p>112,80,class1,department1</p> 
<p>113,74,class1,department1</p> 
<p>114,94,class1,department1</p> 
<p>115,93,class1,department1</p> 
<p>121,74,class2,department1</p> 
<p>122,86,class2,department1</p> 
<p>123,78,class2,department1</p> 
<p>124,70,class2,department1</p> 
<p>211,93,class1,department2</p> 
<p>212,83,class1,department2</p> 
<p>213,94,class1,department2</p> 
<p>214,94,class1,department2</p> 
<p>215,82,class1,department2</p> 
<p>216,74,class1,department2</p> 
<p>221,99,class2,department2</p> 
<p>222,78,class2,department2</p> 
<p>223,74,class2,department2</p> 
<p>224,80,class2,department2</p> 
<p>225,85,class2,department2</p> 
<p>建表：</p> 
<p>create table new_score(</p> 
<p>id int</p> 
<p>,score int</p> 
<p>,clazz string</p> 
<p>,department string</p> 
<p>) row format delimited fields terminated by ",";</p> 
<p><strong>「row_number()：无并列排名」</strong></p> 
<p>使用格式：</p> 
<p>select xxxx, row_number() over(partition by 分组字段 order by 排序字段 desc) as rn from tb group by xxxx</p> 
<p><strong>「dense_rank()：有并列排名，并且依次递增」「rank()：有并列排名，不依次递增」「percent_rank()：(rank的结果-1)/(分区内数据的个数-1)」「cume_dist()：计算某个窗口或分区中某个值的累积分布。」</strong></p> 
<blockquote> 
 <p><strong>❝</strong></p> 
 <p>假定升序排序，则使用以下公式确定累积分布：小于等于当前值x的行数 / 窗口或partition分区内的总行数。其中，x 等于 order by 子句中指定的列的当前行中的值。</p> 
 <p>❞</p> 
</blockquote> 
<p><strong>「NTILE(n)：对分区内数据再分成n组，然后打上组号」「max()、min()、avg()、count()、sum()等函数：是基于每个partition分区内的数据做对应的计算」</strong></p> 
<p><strong>窗口帧：用于从分区中选择指定的多条记录，供窗口函数处理</strong></p> 
<p>Hive 提供了两种定义窗口帧的形式：ROWS 和 RANGE。两种类型都需要配置上界和下界。</p> 
<p>例如，ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW 表示选择分区起始记录到当前记录的所有行；</p> 
<p>SUM(close) RANGE BETWEEN 100 PRECEDING AND 200 FOLLOWING 则通过 字段差值 来进行选择。</p> 
<p>如当前行的 close 字段值是 200，那么这个窗口帧的定义就会选择分区中 close 字段值落在 100 至 400 区间的记录。</p> 
<p>以下是所有可能的窗口帧定义组合。如果没有定义窗口帧，则默认为 RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW。</p> 
<p>注意：窗口帧只能运用在max、min、avg、count、sum、FIRST_VALUE、LAST_VALUE这几个窗口函数上</p> 
<p><strong>「测试」</strong></p> 
<p>SELECT id</p> 
<p>,score</p> 
<p>,clazz</p> 
<p>,SUM(score) OVER w as sum_w</p> 
<p>,round(avg(score) OVER w,3) as avg_w</p> 
<p>,count(score) OVER w as cnt_w</p> 
<p>FROM new_score</p> 
<p>WINDOW w AS (PARTITION BY clazz ORDER BY score rows between 2 PRECEDING and 2 FOLLOWING);</p> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/1b/a5/GqcEFfJp_o.png"></p> 
<p> </p> 
<p><strong>「测试2：」</strong></p> 
<p>select id</p> 
<p>,score</p> 
<p>,clazz</p> 
<p>,department</p> 
<p>,row_number() over (partition by clazz order by score desc) as rn_rk</p> 
<p>,dense_rank() over (partition by clazz order by score desc) as dense_rk</p> 
<p>,rank() over (partition by clazz order by score desc) as rk</p> 
<p>,percent_rank() over (partition by clazz order by score desc) as percent_rk</p> 
<p>,round(cume_dist() over (partition by clazz order by score desc),3) as cume_rk</p> 
<p>,NTILE(3) over (partition by clazz order by score desc) as ntile_num</p> 
<p>,max(score) over (partition by clazz order by score desc range between 3 PRECEDING and 11 FOLLOWING) as max_p</p> 
<p>from new_score;</p> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/0c/30/sc7hPlwj_o.png"></p> 
<p> </p> 
<p><strong>「LAG(col,n)：往前第n行数据LEAD(col,n)：往后第n行数据FIRST_VALUE：取分组内排序后，截止到当前行，第一个值LAST_VALUE：取分组内排序后，截止到当前行，最后一个值，对于并列的排名，取最后一个测试3：」</strong></p> 
<p>select id</p> 
<p>,score</p> 
<p>,clazz</p> 
<p>,department</p> 
<p>,lag(id,2) over (partition by clazz order by score desc) as lag_num</p> 
<p>,LEAD(id,2) over (partition by clazz order by score desc) as lead_num</p> 
<p>,FIRST_VALUE(id) over (partition by clazz order by score desc) as first_v_num</p> 
<p>,LAST_VALUE(id) over (partition by clazz order by score desc) as last_v_num</p> 
<p>,NTILE(3) over (partition by clazz order by score desc) as ntile_num</p> 
<p>from new_score;</p> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/db/4f/reMaLEf4_o.png"></p> 
<p> </p> 
<p><strong>（2）Hive 行转列</strong></p> 
<p>使用关键字：lateral view explode</p> 
<p><strong>「样例演示：」</strong></p> 
<p>建表：</p> 
<p>create table testArray2(</p> 
<p>name string,</p> 
<p>weight array&lt;string&gt;</p> 
<p>)row format delimited</p> 
<p>fields terminated by '\t'</p> 
<p>COLLECTION ITEMS terminated by ',';</p> 
<p>样例数据：</p> 
<p>孙悟空 "150","170","180"</p> 
<p>唐三藏 "150","180","190"</p> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/7b/f9/AISMyVEy_o.png"></p> 
<p> </p> 
<p>select name,col1 from testarray2 lateral view explode(weight) t1 as col1;</p> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/11/fa/hKOUfNxT_o.png"></p> 
<p> </p> 
<p>select key from (select explode(map('key1',1,'key2',2,'key3',3)) as (key,value)) t;</p> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/bf/55/ovDb0EQe_o.png"></p> 
<p> </p> 
<p>select name,col1,col2 from testarray2 lateral view explode(map('key1',1,'key2',2,'key3',3)) t1 as col1,col2;</p> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/df/2e/dVCRQVab_o.png"></p> 
<p> </p> 
<p>select name,pos,col1 from testarray2 lateral view posexplode(weight) t1 as pos,col1;</p> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/28/50/eP2TLZp4_o.png"></p> 
<p> </p> 
<p><strong>（3）Hive 列转行</strong></p> 
<p>数据：</p> 
<p>孙悟空 150</p> 
<p>孙悟空 170</p> 
<p>孙悟空 180</p> 
<p>唐三藏 150</p> 
<p>唐三藏 180</p> 
<p>唐三藏 190</p> 
<p>建表：</p> 
<p>create table testLieToLine(</p> 
<p>name string,</p> 
<p>col1 int</p> 
<p>)row format delimited</p> 
<p>fields terminated by '\t';</p> 
<p><strong>「测试1：」</strong></p> 
<p>select name,collect_list(col1) from testLieToLine group by name;</p> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/e3/99/sgF1fycH_o.png"></p> 
<p> </p> 
<p><strong>「测试2：」</strong></p> 
<p>select t1.name</p> 
<p>,collect_list(t1.col1)</p> 
<p>from (</p> 
<p>select name</p> 
<p>,col1</p> 
<p>from testarray2</p> 
<p>lateral view explode(weight) t1 as col1</p> 
<p>) t1 group by t1.name;</p> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/24/88/V8he424s_o.png"></p> 
<p> </p> 
<p><strong>（4）Hive自定义函数UserDefineFunction</strong></p> 
<p>** UDF：一进一出**</p> 
<ul><li>创建maven项目，并加入依赖</li></ul> 
<p>&lt;dependency&gt;</p> 
<p>&lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</p> 
<p>&lt;artifactId&gt;hive-exec&lt;/artifactId&gt;</p> 
<p>&lt;version&gt;1.2.1&lt;/version&gt;</p> 
<p>&lt;/dependency&gt;</p> 
<ul><li>编写代码，继承org.apache.hadoop.hive.ql.exec.UDF，实现evaluate方法，在evaluate方法中实现自己的逻辑</li></ul> 
<p>import org.apache.hadoop.hive.ql.exec.UDF;</p> 
<p>public class HiveUDF extends UDF {<!-- --></p> 
<p>// hadoop =&gt; #hadoop$</p> 
<p>public String evaluate(String col1) {<!-- --></p> 
<p>// 给传进来的数据 左边加上 # 号 右边加上 $</p> 
<p>String result = "#" + col1 + "$";</p> 
<p>return result;</p> 
<p>}</p> 
<p>}</p> 
<ul><li>打成jar包并上传至Linux虚拟机(小北路径：/usr/local/soft/jars/)\</li><li>在hive shell中，使用 add jar 路径将jar包作为资源添加到hive环境中</li></ul> 
<p>add jar /usr/local/soft/jars/HiveUDF2-1.0.jar;</p> 
<ul><li>使用jar包资源注册一个临时函数，fxxx1是你的函数名，'MyUDF’是主类名</li></ul> 
<p>create temporary function fxxx1 as 'MyUDF';</p> 
<ul><li>使用函数名处理数据</li></ul> 
<p>select fxx1(name) as fxx_name from students limit 10;</p> 
<p>#施笑槐$</p> 
<p>#吕金鹏$</p> 
<p>#单乐蕊$</p> 
<p>#葛德曜$</p> 
<p>#宣谷芹$</p> 
<p>#边昂雄$</p> 
<p>#尚孤风$</p> 
<p>#符半双$</p> 
<p>#沈德昌$</p> 
<p>#羿彦昌$</p> 
<p><strong>「UDTF：一进多出」</strong></p> 
<p>样例数据：</p> 
<p>"key1:value1,key2:value2,key3:value3"</p> 
<p>key1 value1</p> 
<p>key2 value2</p> 
<p>key3 value3</p> 
<p><strong>「方法一：使用 explode+split」</strong></p> 
<p>select split(t.col1,":")[0],split(t.col1,":")[1]</p> 
<p>from (select</p> 
<p>explode(split("key1:value1,key2:value2,key3:value3",",")) as</p> 
<p>col1) t;</p> 
<p><strong>「方法二：自定UDTF」</strong></p> 
<p>//自定义代码</p> 
<p>import org.apache.hadoop.hive.ql.exec.UDFArgumentException;</p> 
<p>import org.apache.hadoop.hive.ql.metadata.HiveException;</p> 
<p>import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</p> 
<p>import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</p> 
<p>import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</p> 
<p>import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;</p> 
<p>import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</p> 
<p>import java.util.ArrayList;</p> 
<p>public class HiveUDTF extends GenericUDTF {<!-- --></p> 
<p>// 指定输出的列名 及 类型</p> 
<p>@Override</p> 
<p>public StructObjectInspector initialize(StructObjectInspector argOIs) throws UDFArgumentException {<!-- --></p> 
<p>ArrayList&lt;String&gt; filedNames = new ArrayList&lt;String&gt;();</p> 
<p>ArrayList&lt;ObjectInspector&gt; filedObj = new ArrayList&lt;ObjectInspector&gt;();</p> 
<p>filedNames.add("col1");</p> 
<p>filedObj.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</p> 
<p>filedNames.add("col2");</p> 
<p>filedObj.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</p> 
<p>return ObjectInspectorFactory.getStandardStructObjectInspector(filedNames, filedObj);</p> 
<p>}</p> 
<p>// 处理逻辑 my_udtf(col1,col2,col3)</p> 
<p>// "key1:value1,key2:value2,key3:value3"</p> 
<p>// my_udtf("key1:value1,key2:value2,key3:value3")</p> 
<p>public void process(Object[] objects) throws HiveException {<!-- --></p> 
<p>// objects 表示传入的N列</p> 
<p>String col = objects[0].toString();</p> 
<p>// key1:value1 key2:value2 key3:value3</p> 
<p>String[] splits = col.split(",");</p> 
<p>for (String str : splits) {<!-- --></p> 
<p>String[] cols = str.split(":");</p> 
<p>// 将数据输出</p> 
<p>forward(cols);</p> 
<p>}</p> 
<p>}</p> 
<p>// 在UDTF结束时调用</p> 
<p>public void close() throws HiveException {<!-- --></p> 
<p>}</p> 
<p>}</p> 
<p><strong>「SQL:」</strong></p> 
<p>select my_udtf("key1:value1,key2:value2,key3:value3");</p> 
<p><strong>「举例说明：」</strong></p> 
<blockquote> 
 <p><strong>❝</strong></p> 
 <p>字段：id,col1,col2,col3,col4,col5,col6,col7,col8,col9,col10,col11,col12共13列数据：a,1,2,3,4,5,6,7,8,9,10,11,12b,11,12,13,14,15,16,17,18,19,20,21,22c,21,22,23,24,25,26,27,28,29,30,31,32转成3列：id,hours,value例如：a,1,2,3,4,5,6,7,8,9,10,11,12a,0时,1a,2时,2a,4时,3a,6时,4</p> 
 <p>❞</p> 
</blockquote> 
<p>建表：</p> 
<p>create table udtfData(</p> 
<p>id string</p> 
<p>,col1 string</p> 
<p>,col2 string</p> 
<p>,col3 string</p> 
<p>,col4 string</p> 
<p>,col5 string</p> 
<p>,col6 string</p> 
<p>,col7 string</p> 
<p>,col8 string</p> 
<p>,col9 string</p> 
<p>,col10 string</p> 
<p>,col11 string</p> 
<p>,col12 string</p> 
<p>)row format delimited fields terminated by ',';</p> 
<p><strong>「java代码：」</strong></p> 
<p>import org.apache.hadoop.hive.ql.exec.UDFArgumentException;</p> 
<p>import org.apache.hadoop.hive.ql.metadata.HiveException;</p> 
<p>import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</p> 
<p>import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</p> 
<p>import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</p> 
<p>import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;</p> 
<p>import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</p> 
<p>import java.util.ArrayList;</p> 
<p>public class HiveUDTF2 extends GenericUDTF {<!-- --></p> 
<p>@Override</p> 
<p>public StructObjectInspector initialize(StructObjectInspector argOIs) throws UDFArgumentException {<!-- --></p> 
<p>ArrayList&lt;String&gt; filedNames = new ArrayList&lt;String&gt;();</p> 
<p>ArrayList&lt;ObjectInspector&gt; fieldObj = new ArrayList&lt;ObjectInspector&gt;();</p> 
<p>filedNames.add("col1");</p> 
<p>fieldObj.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</p> 
<p>filedNames.add("col2");</p> 
<p>fieldObj.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</p> 
<p>return ObjectInspectorFactory.getStandardStructObjectInspector(filedNames, fieldObj);</p> 
<p>}</p> 
<p>public void process(Object[] objects) throws HiveException {<!-- --></p> 
<p>int hours = 0;</p> 
<p>for (Object obj : objects) {<!-- --></p> 
<p>hours = hours + 1;</p> 
<p>String col = obj.toString();</p> 
<p>ArrayList&lt;String&gt; cols = new ArrayList&lt;String&gt;();</p> 
<p>cols.add(hours + "时");</p> 
<p>cols.add(col);</p> 
<p>forward(cols);</p> 
<p>}</p> 
<p>}</p> 
<p>public void close() throws HiveException {<!-- --></p> 
<p>}</p> 
<p>}</p> 
<p><strong>「添加jar资源:」</strong></p> 
<p>add jar /usr/local/soft/HiveUDF2-1.0.jar;</p> 
<p><strong>「注册udtf函数：」</strong></p> 
<p>create temporary function my_udtf as 'MyUDTF';</p> 
<p><strong>「SQL:」</strong></p> 
<p>select id</p> 
<p>,hours</p> 
<p>,value from udtfData lateral view</p> 
<p>my_udtf(col1,col2,col3,col4,col5,col6,col7,col8,col9,col10,col11,col12)</p> 
<p>t as hours,value ;</p> 
<p><strong>「UDAF：多进一出」</strong></p> 
<p><strong>3、Hive 中的wordCount</strong></p> 
<p>建表：</p> 
<p>create table words(</p> 
<p>words string</p> 
<p>)row format delimited fields terminated by '|';</p> 
<p>数据：</p> 
<p>hello,java,hello,java,scala,python</p> 
<p>hbase,hadoop,hadoop,hdfs,hive,hive</p> 
<p>hbase,hadoop,hadoop,hdfs,hive,hive</p> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/fd/90/CFKBl2Wn_o.png"></p> 
<p> </p> 
<p>select word,count(*) from (select explode(split(words,',')) word from words) a group by a.word;</p> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/45/14/OacBUnTC_o.png"></p> 
<p> </p> 
<p><strong>七、Hive 的Shell使用</strong></p> 
<p><strong>第一种shell</strong></p> 
<p>hive -e "select * from test03.students limit 10"</p> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/9d/f3/0w7UMqI0_o.png"></p> 
<p> </p> 
<p><strong>第二种shell</strong></p> 
<p>hive -f hql文件路径</p> 
<p># 将HQL写在一个文件里，再使用 -f 参数指定该文件</p> 
<p><span style="color:#d7d8d9;">文章来源于数据分析与开发</span></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/933d345dd411fe5707da3ad0588e4be9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">java培训4种Map遍历 key-value 的方法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8f61f565a1bacbcfd382dd92463b6f29/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">python中列表常用方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>