<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>elk&#43;kafka日志分析系统搭建 - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="elk&#43;kafka日志分析系统搭建" />
<meta property="og:description" content="目录
一、elk引入
二、认识elk&#43;kafka&#43;filebeat
Elasticsearch
Logstash
Kibana
kafka
三、elk&#43;kafka架构图
四、elasticsearch部署
1、服务器情况
2、添加环境变量
3、系统性能优化
4、node1节点部署
5、启动集群
6、查看集群状态
五、kibana
1、部署环境
2、安装kibana
3、启动kibana
六、部署kafka
1、部署环境
2、安装kafka
3、启动zookeeper
4、启动kafka
5、验证kafka
七、logstash部署
1、环境部署
2、安装lostash
3、启动logstash
八、filebeat部署
1、环境部署
2、安装filebeat
3、启动filebeat
九、启动nginx验证kibana是否正常展示数据
1、启动nginx
2、登录kibana验证
一、elk引入 如果在实际生产情况下，查看日志大都通过SSH客户端登服务器去看，使用较多的命令就是 less 、cat、head、tail。如果服务部署了几十台甚至上百台服务器，就要分别登录到这些台机器上看，这样大大的降低了我们的工作效率，等到了分布式和微服务架构流行时代，一个从APP或H5发起的请求除了需要登陆服务器去排查日志，往往还会经过远程软件远程到了别的主机继续处理，开发人员定位问题可能还需要根据TraceID或者业务唯一主键去跟踪服务的链路日志，基于传统SSH方式登陆主机查看日志的方式就像图中排查线路的工人一样困难，线上服务器几十上百之多，出了问题难以快速响应，因此需要高效、实时的日志存储和检索平台，ELK就提供这样一套解决方案。
二、认识elk&#43;kafka&#43;filebeat ELK是三个开源软件的缩写，分别表示：Elasticsearch , Logstash, Kibana , 它们都是开源软件。Filebeat是用于转发和集中日志数据的轻量级传送工具。Filebeat监视您指定的日志文件或位置，收集日志事件，并将它们转发到Elasticsearch或 Logstash进行索引。
首先，elk作为日志分析架构，主要使用这几个组合：filebeat（采集）&#43;logstash（管道）&#43;elasticsearch（存储和搜索）&#43;kibana（日志应用）的这种组合比较常见。
kafka作为适合大吞吐数据的临时队列，比较适合放置在filebeat和logstash之间
Elasticsearch Elasticsearch 是一个实时的分布式存储、搜索、分析的引擎。
Elasticsearch 是一个分布式的、开源的搜索分析引擎，支持各种数据类型，包括文本、数字、地理、结构化、非结构化。
Elasticsearch 因其简单的 REST API、分布式特性、告诉、可扩展而闻名。
Elasticsearch 是 Elastic 产品栈的核心，Elastic 产品栈是个开源工具集合，用于数据接收、存储、分析、可视化。
Logstash Logstash是具有实时流水线能力的开源的数据收集引擎。Logstash可以动态统一不同来源的数据，并将数据标准化到您选择的目标输出。它提供了大量插件，可帮助我们解析，丰富，转换和缓冲任何类型的数据
Kibana Kibana是一个开源的分析与可视化平台，设计出来用于和Elasticsearch一起使用的。你可以用kibana搜索、查看存放在Elasticsearch中的数据。Kibana与Elasticsearch的交互方式是各种不同的图表、表格、地图等，直观的展示数据，从而达到高级的数据分析与可视化的目的。
kafka Kafka是Apache旗下的一款分布式流媒体平台，Kafka是一种高吞吐量、持久性、分布式的发布订阅的消息队列系统。 它最初由LinkedIn(领英)公司发布，使用Scala语言编写，与2010年12月份开源，成为Apache的顶级子项目。 它主要用于处理消费者规模网站中的所有动作流数据。动作指(网页浏览、搜索和其它用户行动所产生的数据)。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/e0734cebb00fb8bf25410ec07b07f258/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-09-24T18:19:57+08:00" />
<meta property="article:modified_time" content="2023-09-24T18:19:57+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">elk&#43;kafka日志分析系统搭建</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81elk%E5%BC%95%E5%85%A5-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81elk%E5%BC%95%E5%85%A5" rel="nofollow">一、elk引入</a></p> 
<p id="%E4%BA%8C%E3%80%81%E8%AE%A4%E8%AF%86elk%2Bkafka%2Bfilebeat-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81%E8%AE%A4%E8%AF%86elk%2Bkafka%2Bfilebeat" rel="nofollow">二、认识elk+kafka+filebeat</a></p> 
<p id="Elasticsearch-toc" style="margin-left:40px;"><a href="#Elasticsearch" rel="nofollow">Elasticsearch</a></p> 
<p id="Logstash-toc" style="margin-left:40px;"><a href="#Logstash" rel="nofollow">Logstash</a></p> 
<p id="Kibana-toc" style="margin-left:40px;"><a href="#Kibana" rel="nofollow">Kibana</a></p> 
<p id="kafka-toc" style="margin-left:40px;"><a href="#kafka" rel="nofollow">kafka</a></p> 
<p id="%E4%B8%89%E3%80%81elk%2Bkafka%E6%9E%B6%E6%9E%84%E5%9B%BE-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81elk%2Bkafka%E6%9E%B6%E6%9E%84%E5%9B%BE" rel="nofollow">三、elk+kafka架构图</a></p> 
<p id="%E5%9B%9B%E3%80%81elasticsearch%E9%83%A8%E7%BD%B2-toc" style="margin-left:0px;"><a href="#%E5%9B%9B%E3%80%81elasticsearch%E9%83%A8%E7%BD%B2" rel="nofollow">四、elasticsearch部署</a></p> 
<p id="1%E3%80%81%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%83%85%E5%86%B5-toc" style="margin-left:80px;"><a href="#1%E3%80%81%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%83%85%E5%86%B5" rel="nofollow">1、服务器情况</a></p> 
<p id="2%E3%80%81%E6%B7%BB%E5%8A%A0%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F-toc" style="margin-left:80px;"><a href="#2%E3%80%81%E6%B7%BB%E5%8A%A0%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F" rel="nofollow">2、添加环境变量</a></p> 
<p id="3%E3%80%81%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-toc" style="margin-left:80px;"><a href="#3%E3%80%81%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96" rel="nofollow">3、系统性能优化</a></p> 
<p id="4%E3%80%81node1%E8%8A%82%E7%82%B9%E9%83%A8%E7%BD%B2-toc" style="margin-left:80px;"><a href="#4%E3%80%81node1%E8%8A%82%E7%82%B9%E9%83%A8%E7%BD%B2" rel="nofollow">4、node1节点部署</a></p> 
<p id="5%E3%80%81%E5%90%AF%E5%8A%A8%E9%9B%86%E7%BE%A4-toc" style="margin-left:80px;"><a href="#5%E3%80%81%E5%90%AF%E5%8A%A8%E9%9B%86%E7%BE%A4" rel="nofollow">5、启动集群</a></p> 
<p id="6%E3%80%81%E6%9F%A5%E7%9C%8B%E9%9B%86%E7%BE%A4%E7%8A%B6%E6%80%81-toc" style="margin-left:80px;"><a href="#6%E3%80%81%E6%9F%A5%E7%9C%8B%E9%9B%86%E7%BE%A4%E7%8A%B6%E6%80%81" rel="nofollow">6、查看集群状态</a></p> 
<p id="%E4%BA%94%E3%80%81kibana-toc" style="margin-left:0px;"><a href="#%E4%BA%94%E3%80%81kibana" rel="nofollow">五、kibana</a></p> 
<p id="1%E3%80%81%E9%83%A8%E7%BD%B2%E7%8E%AF%E5%A2%83-toc" style="margin-left:80px;"><a href="#1%E3%80%81%E9%83%A8%E7%BD%B2%E7%8E%AF%E5%A2%83" rel="nofollow">1、部署环境</a></p> 
<p id="2%E3%80%81%E5%AE%89%E8%A3%85kibana-toc" style="margin-left:80px;"><a href="#2%E3%80%81%E5%AE%89%E8%A3%85kibana" rel="nofollow">2、安装kibana</a></p> 
<p id="3%E3%80%81%E5%90%AF%E5%8A%A8kibana-toc" style="margin-left:80px;"><a href="#3%E3%80%81%E5%90%AF%E5%8A%A8kibana" rel="nofollow">3、启动kibana</a></p> 
<p id="%E5%85%AD%E3%80%81%E9%83%A8%E7%BD%B2kafka-toc" style="margin-left:0px;"><a href="#%E5%85%AD%E3%80%81%E9%83%A8%E7%BD%B2kafka" rel="nofollow">六、部署kafka</a></p> 
<p id="1%E3%80%81%E9%83%A8%E7%BD%B2%E7%8E%AF%E5%A2%83-toc" style="margin-left:80px;"><a href="#1%E3%80%81%E9%83%A8%E7%BD%B2%E7%8E%AF%E5%A2%83" rel="nofollow">1、部署环境</a></p> 
<p id="2%E3%80%81%E5%AE%89%E8%A3%85kafka-toc" style="margin-left:80px;"><a href="#2%E3%80%81%E5%AE%89%E8%A3%85kafka" rel="nofollow">2、安装kafka</a></p> 
<p id="3%E3%80%81%E5%90%AF%E5%8A%A8zookeeper-toc" style="margin-left:80px;"><a href="#3%E3%80%81%E5%90%AF%E5%8A%A8zookeeper" rel="nofollow">3、启动zookeeper</a></p> 
<p id="4%E3%80%81%E5%90%AF%E5%8A%A8kafka-toc" style="margin-left:80px;"><a href="#4%E3%80%81%E5%90%AF%E5%8A%A8kafka" rel="nofollow">4、启动kafka</a></p> 
<p id="5%E3%80%81%E9%AA%8C%E8%AF%81kafka-toc" style="margin-left:80px;"><a href="#5%E3%80%81%E9%AA%8C%E8%AF%81kafka" rel="nofollow">5、验证kafka</a></p> 
<p id="%E4%B8%83%E3%80%81logstash%E9%83%A8%E7%BD%B2-toc" style="margin-left:0px;"><a href="#%E4%B8%83%E3%80%81logstash%E9%83%A8%E7%BD%B2" rel="nofollow">七、logstash部署</a></p> 
<p id="1%E3%80%81%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2-toc" style="margin-left:80px;"><a href="#1%E3%80%81%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2" rel="nofollow">1、环境部署</a></p> 
<p id="2%E3%80%81%E5%AE%89%E8%A3%85lostash-toc" style="margin-left:80px;"><a href="#2%E3%80%81%E5%AE%89%E8%A3%85lostash" rel="nofollow">2、安装lostash</a></p> 
<p id="3%E3%80%81%E5%90%AF%E5%8A%A8logstash-toc" style="margin-left:80px;"><a href="#3%E3%80%81%E5%90%AF%E5%8A%A8logstash" rel="nofollow">3、启动logstash</a></p> 
<p id="%E5%85%AB%E3%80%81filebeat%E9%83%A8%E7%BD%B2-toc" style="margin-left:0px;"><a href="#%E5%85%AB%E3%80%81filebeat%E9%83%A8%E7%BD%B2" rel="nofollow">八、filebeat部署</a></p> 
<p id="1%E3%80%81%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2-toc" style="margin-left:80px;"><a href="#1%E3%80%81%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2" rel="nofollow">1、环境部署</a></p> 
<p id="2%E3%80%81%E5%AE%89%E8%A3%85filebeat-toc" style="margin-left:80px;"><a href="#2%E3%80%81%E5%AE%89%E8%A3%85filebeat" rel="nofollow">2、安装filebeat</a></p> 
<p id="3%E3%80%81%E5%90%AF%E5%8A%A8filebeat-toc" style="margin-left:80px;"><a href="#3%E3%80%81%E5%90%AF%E5%8A%A8filebeat" rel="nofollow">3、启动filebeat</a></p> 
<p id="%E4%B9%9D%E3%80%81%E5%90%AF%E5%8A%A8nginx%E9%AA%8C%E8%AF%81kibana%E6%98%AF%E5%90%A6%E6%AD%A3%E5%B8%B8%E5%B1%95%E7%A4%BA%E6%95%B0%E6%8D%AE-toc" style="margin-left:0px;"><a href="#%E4%B9%9D%E3%80%81%E5%90%AF%E5%8A%A8nginx%E9%AA%8C%E8%AF%81kibana%E6%98%AF%E5%90%A6%E6%AD%A3%E5%B8%B8%E5%B1%95%E7%A4%BA%E6%95%B0%E6%8D%AE" rel="nofollow">九、启动nginx验证kibana是否正常展示数据</a></p> 
<p id="1%E3%80%81%E5%90%AF%E5%8A%A8nginx-toc" style="margin-left:80px;"><a href="#1%E3%80%81%E5%90%AF%E5%8A%A8nginx" rel="nofollow">1、启动nginx</a></p> 
<p id="2%E3%80%81%E7%99%BB%E5%BD%95kibana%E9%AA%8C%E8%AF%81-toc" style="margin-left:80px;"><a href="#2%E3%80%81%E7%99%BB%E5%BD%95kibana%E9%AA%8C%E8%AF%81" rel="nofollow">2、登录kibana验证</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="%E4%B8%80%E3%80%81elk%E5%BC%95%E5%85%A5">一、elk引入</h2> 
<blockquote> 
 <p>    如果在实际生产情况下，查看日志大都通过SSH客户端登服务器去看，使用较多的命令就是 less 、cat、head、tail。如果服务部署了几十台甚至上百台服务器，就要分别登录到这些台机器上看，这样大大的降低了我们的工作效率，等到了分布式和微服务架构流行时代，一个从APP或H5发起的请求除了需要登陆服务器去排查日志，往往还会经过远程软件远程到了别的主机继续处理，开发人员定位问题可能还需要根据TraceID或者业务唯一主键去跟踪服务的链路日志，基于传统SSH方式登陆主机查看日志的方式就像图中排查线路的工人一样困难，线上服务器几十上百之多，出了问题难以快速响应，因此需要高效、实时的日志存储和检索平台，ELK就提供这样一套解决方案。</p> 
</blockquote> 
<h2 id="%E4%BA%8C%E3%80%81%E8%AE%A4%E8%AF%86elk%2Bkafka%2Bfilebeat">二、认识elk+kafka+filebeat</h2> 
<blockquote> 
 <p><strong>ELK</strong>是三个开源软件的缩写，分别表示：<strong>Elasticsearch , Logstash, Kibana</strong> , 它们都是开源软件。<strong>Filebeat</strong>是用于转发和集中日志数据的轻量级传送工具。Filebeat监视您指定的日志文件或位置，收集日志事件，并将它们转发到Elasticsearch或 Logstash进行索引。</p> 
 <p><strong>首先</strong>，elk作为日志分析架构，主要使用这几个组合：filebeat（采集）+logstash（管道）+elasticsearch（存储和搜索）+kibana（日志应用）的这种组合比较常见。</p> 
 <p><a href="https://www.zhihu.com/search?q=kafka&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A1641824640%7D" rel="nofollow" title="kafka">kafka</a>作为适合大吞吐数据的临时队列，比较适合放置在filebeat和logstash之间</p> 
 <p></p> 
</blockquote> 
<h3 id="Elasticsearch">Elasticsearch</h3> 
<blockquote> 
 <p>Elasticsearch 是一个<strong>实时</strong>的<strong>分布式存储、搜索、分析</strong>的引擎。</p> 
 <p>Elasticsearch 是一个分布式的、开源的搜索分析引擎，支持各种数据类型，包括文本、数字、地理、结构化、非结构化。<br> Elasticsearch 因其简单的 REST API、分布式特性、告诉、可扩展而闻名。<br> Elasticsearch 是 Elastic 产品栈的核心，Elastic 产品栈是个开源工具集合，用于数据接收、存储、分析、可视化。</p> 
</blockquote> 
<h3 id="Logstash">Logstash</h3> 
<blockquote> 
 <p>Logstash是具有实时流水线能力的开源的数据收集引擎。Logstash可以动态统一不同来源的数据，并将数据标准化到您选择的目标输出。它提供了大量插件，可帮助我们解析，丰富，转换和缓冲任何类型的数据</p> 
</blockquote> 
<h3 id="Kibana" style="background-color:transparent;">Kibana</h3> 
<blockquote> 
 <p>Kibana是一个开源的分析与可视化平台，设计出来用于和Elasticsearch一起使用的。你可以用kibana搜索、查看存放在Elasticsearch中的数据。Kibana与Elasticsearch的交互方式是各种不同的图表、表格、地图等，直观的展示数据，从而达到高级的数据分析与可视化的目的。</p> 
</blockquote> 
<h3 id="kafka">kafka</h3> 
<blockquote> 
 <p>Kafka是Apache旗下的一款分布式流媒体平台，Kafka是一种高吞吐量、持久性、分布式的发布订阅的消息队列系统。 它最初由LinkedIn(领英)公司发布，使用Scala语言编写，与2010年12月份开源，成为Apache的顶级子项目。 它主要用于处理消费者规模网站中的所有动作流数据。动作指(网页浏览、搜索和其它用户行动所产生的数据)。</p> 
</blockquote> 
<h2 id="%E4%B8%89%E3%80%81elk%2Bkafka%E6%9E%B6%E6%9E%84%E5%9B%BE">三、elk+kafka架构图</h2> 
<p><img alt="" height="435" src="https://images2.imgbox.com/be/ce/DvZUm94B_o.png" width="1200"></p> 
<blockquote> 
 <p><span style="background-color:#a2e043;">上图是本次搭建elk+kafka的部署流程图，但是存在单点故障问题，</span></p> 
 <p>--比如我的kafka量过大，会造成把kafka打满，导致无法接收数据，建议在生产情况下根据自己的实际使用情况考虑是否使用集群的方式</p> 
 <p>--es-master 如果master挂掉后，整个集群将不可用，所以在正常的生产环境下，es-maser应部署奇数节点 比如三台、五台等等、当然node节点也可以随之扩容</p> 
</blockquote> 
<blockquote> 
 <p><span style="background-color:#a2e043;">还有一个问题就是在分部署日志架构中我们使用elk部署即可为什么还要使用kafka了？</span></p> 
 <p>原因是：</p> 
 <p>       ELK架构优点是搭建简单，易于上手。缺点是Logstash消耗系统资源比较大，运行时占用CPU和内存资源较高。另外，由于没有<a href="https://cloud.tencent.com/product/cmq?from_column=20065&amp;from=20065" rel="nofollow" title="消息队列">消息队列</a>缓存，可能存在数据丢失的风险，<strong>适合于数据量小的环境使用</strong>。</p> 
 <p>      而<strong>引入Kafka的典型ELK架构是</strong></p> 
 <p>为保证日志传输数据的可靠性和稳定性，先将数据传递给消息队列，接着、将格式化的数据传递给Elasticsearch进行存储、最后，由Kibana将日志和数据呈现给用户。由于引入了Kafka缓冲机制，即使远端Logstash server因故障停止运行，数据也不会丢失，可靠性得到了大大的提升。</p> 
</blockquote> 
<h2 id="%E5%9B%9B%E3%80%81elasticsearch%E9%83%A8%E7%BD%B2">四、elasticsearch部署</h2> 
<h4 id="1%E3%80%81%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%83%85%E5%86%B5">1、服务器情况</h4> 
<table border="1" cellpadding="1" cellspacing="1"><tbody><tr><td>服务器ip</td><td>部署应用</td><td>软件版本</td></tr><tr><td>192.168.12.11</td><td>es-mater、fiebeat、nginx</td><td>elasticsearch-7.13.0、filebeat-7.12.0、nginx-1.22.1</td></tr><tr><td>192.168.12.12</td><td>es-node1</td><td>elasticsearch-7.13.0</td></tr><tr><td>192.168.12.13</td><td>es-node2</td><td>elasticsearch-7.13.0</td></tr><tr><td>192.168.12.14</td><td>kafka</td><td>kafka_2.12-2.8.0</td></tr><tr><td>192.168.12.15</td><td>kibana</td><td>kibana-7.13.0</td></tr><tr><td>192.168.12.16</td><td>logstash</td><td> <p>logstash-7.12.0</p> </td></tr></tbody></table> 
<h4 id="2%E3%80%81%E6%B7%BB%E5%8A%A0%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F">2、添加环境变量</h4> 
<blockquote> 
 <p>[root@control ~]# vim /etc/profile<br> [root@control ~]#<br> export JAVA_HOME=/usr/java/jdk18<br> export PATH=$PATH:$JAVA_HOME/bin</p> 
</blockquote> 
<h4 id="3%E3%80%81%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96">3、系统性能优化</h4> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;">es节点服务器均需要使用管理员root权限解除打开文件数的限制和调大普通用户虚拟内存。保存后执行sysctl -p</p> 
</blockquote> 
<pre><code>cat /etc/security/limits.conf
* soft nofile 655360
* hard nofile 655360

cat /etc/sysctl.conf
vm.max_map_count = 655360
sysctl -p</code></pre> 
<p>master节点部署</p> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;">       master节点主要功能是维护元数据，管理集群各个节点的状态，数据的导入和查询都不会走master节点，所以master节点的压力相对较小，因此master节点的内存分配也可以相对少些；但是master节点是最重要的，如果master节点挂了或者发生脑裂了，你的元数据就会发生混乱，那样你集群里的全部数据可能会发生丢失，所以一定要保证master节点的稳定性。</p> 
</blockquote> 
<p>192.168.12.11节点部署</p> 
<pre><code>[www@control es-master]$ ll
总用量 319484
drwxr-xr-x 3 www www        19 9月  19 10:01 data
-rw-r--r-- 1 www www 327145839 9月  18 16:37 elasticsearch-7.13.0-linux-x86_64.tar.gz
drwxr-xr-x 9 www www       155 5月  20 2021 elasticsearch-master
drwxr-xr-x 2 www www      4096 9月  23 19:33 logs
[www@control es-master]$
[www@control es-master]$
[www@control es-master]$ pwd   #这是master的安装目录
/data01/elk/es-master
[www@control es-master]$
[www@control es-master]$ cd /data01/elk/es-master

[www@control es-master]$
[www@control es-master]$tar -xf elasticsearch-7.13.0-linux-x86_64.tar.gz
[www@control es-master]$
[www@control es-master]$
[www@control es-master]$mv elasticsearch-7.13.0  elasticsearch-master
[www@control es-master]$
[www@control es-master]$mkdir data  &amp;&amp; mkdir logs



</code></pre> 
<p>编辑elasticsearch的配置文件</p> 
<p style="margin-left:.0001pt;text-align:justify;">vim /data01/elk/es-master/elasticsearch-master/config/elasticsearch.yml</p> 
<pre><code>#集群名称
cluster.name: "cs-ES"
##节点名称
node.name: es-master
##是否可以成为master
node.master: true
#cluster.initial_master_nodes: ["es-master"]
##是否允许该节点存储数据
node.data: false
##网络绑定
network.host: ["0.0.0.0"]
##设置对外服务的http端口，默认为9200
http.port: 9200
##支持跨域访问-head插件支持
http.cors.enabled: true
http.cors.allow-origin: "*"
http.cors.allow-headers: Authorization,X-Requested-With,Content-Type,Content-Length
##设置节点间交互的tcp端口,默认是9300
transport.tcp.port: 9300
##集群发现的节点ip
discovery.seed_hosts: ["192.168.12.12:9300","192.168.12.13:9300","192.168.12.11:9300"]
##可以成为 master的节点ip
#icluster.initial_master_nodes:["192.168.12.12:9300","192.168.12.13:9300","192.168.12.11:9300"]
##数据存储路径
path.data: /data01/elk/es-master/data
##日志存储路径
path.logs: /data01/elk/es-master/logs
##组成集群所需要的最少主节点候选节点数
discovery.zen.minimum_master_nodes: 1
##开启x-pack功能，并指定证书位置
xpack.security.enabled: true
xpack.security.transport.ssl.enabled: true
xpack.security.transport.ssl.verification_mode: certificate
xpack.security.transport.ssl.keystore.path: elastic-certificates.p12
xpack.security.transport.ssl.truststore.path: elastic-certificates.p12

</code></pre> 
<h5 style="margin-left:.0001pt;text-align:justify;"></h5> 
<p>修改es内存使用的最大值和最小值</p> 
<pre><code>-Xms1g
-Xmx1g

因为我是模拟生产环境使用虚拟机搭建 所用这里 大小我给的1g 如果是在正常的生产环境下 根据服务器的内存大小进行设置即可</code></pre> 
<p><span style="color:#4d4d4d;">使用</span><span style="color:#4d4d4d;">任意</span><span style="color:#4d4d4d;">节点生成证书</span></p> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="color:#4d4d4d;">证书会生成在config目录下，证书名为：</span><span style="color:#0d0016;"><span style="background-color:#a2e043;">elastic-certificates.p12</span></span></p> 
<pre><code>[www@control es-master]$ cd elasticsearch-master/config/
[www@control config]$ ll
总用量 48
-rw------- 1 www www  3443 9月  19 08:30 elastic-certificates.p12
-rw-rw---- 1 www www   199 9月  19 09:53 elasticsearch.keystore
-rw-rw---- 1 www www  4082 9月  20 08:41 elasticsearch.yml
-rw-r----- 1 www www  2739 9月  19 08:20 elasticsearch.yml.bak
-rw-rw---- 1 www www  3104 9月  19 08:20 jvm.options
drwxr-x--- 2 www www     6 5月  20 2021 jvm.options.d
-rw-rw---- 1 www www 18626 5月  20 2021 log4j2.properties
-rw-rw---- 1 www www   473 5月  20 2021 role_mapping.yml
-rw-rw---- 1 www www   197 5月  20 2021 roles.yml
-rw-rw---- 1 www www     0 5月  20 2021 users
-rw-rw---- 1 www www     0 5月  20 2021 users_roles
[www@control config]$
</code></pre> 
<p>将生成的证书拷贝到其他的节点</p> 
<pre><code> scp elastic-certificates.p12 root@192.168.12.12:/data01/elk/es-node1/elasticsearch-node1/config/
 scp elastic-certificates.p12 root@192.168.12.13:/data01/elk/es-node2/elasticsearch-node1/config/
</code></pre> 
<h4 id="4%E3%80%81node1%E8%8A%82%E7%82%B9%E9%83%A8%E7%BD%B2">4、node1节点部署</h4> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#ffffff;">    当node.data为true时，这个节点作为一个数据节点，数据节点主要是存储索引数据的节点，主要对文档进行增删改查操作，聚合操作等。数据节点对cpu，内存，io要求较高， 在优化的时候需要监控数据节点的状态，当资源不够的时候，需要在集群中添加新的节点。</span></p> 
</blockquote> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;">拷贝master节点到node节点</p> 
 <p style="margin-left:.0001pt;text-align:justify;">然后修改配置文件</p> 
</blockquote> 
<p style="margin-left:.0001pt;text-align:justify;">jvm启动内存修改</p> 
<pre><code>
[www@node2 config]$ cat jvm.options
-Xms1g
-Xmx1g

</code></pre> 
<p>es-node节点配置文件修改</p> 
<pre><code>[www@node2 config]$ cat elasticsearch.yml
#集群名称
cluster.name: "cs-ES"
###节点名称
node.name: es-node1
###是否可以成为master
node.master: false
###是否允许该节点存储数据
node.data: true
###网络绑定
network.host: ["0.0.0.0"]
###设置对外服务的http端口，默认为9200
http.port: 9200
###支持跨域访问-head插件支持
http.cors.enabled: true
http.cors.allow-origin: "*"
#http.cors.allow-headers: Authorization,X-Requested-With,Content-Type,Content-Length
###设置节点间交互的tcp端口,默认是9300
transport.tcp.port: 9300
###集群发现的节点ip
discovery.seed_hosts: ["192.168.12.12:9300","192.168.12.13:9300","192.168.12.11:9300"]
###可以成为 master的节点ip
##icluster.initial_master_nodes:["133.38.37.221:9300","133.38.37.222:9300","133.38.37.223:9300"]
###数据存储路径
path.data: /data01/elk/es-node1/data
###日志存储路径
path.logs: /data01/elk/es-node1/logs
###组成集群所需要的最少主节点候选节点数
discovery.zen.minimum_master_nodes: 1
###开启x-pack功能，并指定证书位置
xpack.security.enabled: true
xpack.security.transport.ssl.enabled: true
xpack.security.transport.ssl.verification_mode: certificate
xpack.security.transport.ssl.keystore.path: elastic-certificates.p12
xpack.security.transport.ssl.truststore.path: elastic-certificates.p12
</code></pre> 
<p><span style="background-color:#a2e043;">node2的部署方式同上</span></p> 
<h4 id="5%E3%80%81%E5%90%AF%E5%8A%A8%E9%9B%86%E7%BE%A4">5、启动集群</h4> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;">集群启动的方式为先启动master，再启动node节点。观察日志是否集群启动成功，</p> 
 <p style="margin-left:.0001pt;text-align:justify;">启动命令如下：</p> 
</blockquote> 
<pre><code>[www@control root]$ cd /data01/elk/es-master/elasticsearch-master/bin/
[www@control bin]$ ll
总用量 21532
-rwxr-xr-x 1 www www     2896 5月  20 2021 elasticsearch
-rwxr-xr-x 1 www www      501 5月  20 2021 elasticsearch-certgen
-rwxr-xr-x 1 www www      493 5月  20 2021 elasticsearch-certutil
-rwxr-xr-x 1 www www      996 5月  20 2021 elasticsearch-cli
-rwxr-xr-x 1 www www      443 5月  20 2021 elasticsearch-croneval
-rwxr-xr-x 1 www www     4856 5月  20 2021 elasticsearch-env
-rwxr-xr-x 1 www www     1828 5月  20 2021 elasticsearch-env-from-file
-rwxr-xr-x 1 www www      168 5月  20 2021 elasticsearch-geoip
-rwxr-xr-x 1 www www      184 5月  20 2021 elasticsearch-keystore
-rwxr-xr-x 1 www www      450 5月  20 2021 elasticsearch-migrate
-rwxr-xr-x 1 www www      126 5月  20 2021 elasticsearch-node
-rwxr-xr-x 1 www www      172 5月  20 2021 elasticsearch-plugin
-rwxr-xr-x 1 www www      441 5月  20 2021 elasticsearch-saml-metadata
-rwxr-xr-x 1 www www      439 5月  20 2021 elasticsearch-service-tokens
-rwxr-xr-x 1 www www      448 5月  20 2021 elasticsearch-setup-passwords
-rwxr-xr-x 1 www www      118 5月  20 2021 elasticsearch-shard
-rwxr-xr-x 1 www www      483 5月  20 2021 elasticsearch-sql-cli
-rwxr-xr-x 1 www www 21951112 5月  20 2021 elasticsearch-sql-cli-7.13.0.jar
-rwxr-xr-x 1 www www      436 5月  20 2021 elasticsearch-syskeygen
-rwxr-xr-x 1 www www      436 5月  20 2021 elasticsearch-users
-rwxr-xr-x 1 www www      356 5月  20 2021 x-pack-env
-rwxr-xr-x 1 www www      364 5月  20 2021 x-pack-security-env
-rwxr-xr-x 1 www www      363 5月  20 2021 x-pack-watcher-env
[www@control bin]$

[www@control bin]$ ./elasticsearch -d   #后台启动es-master
warning: usage of JAVA_HOME is deprecated, use ES_JAVA_HOME
Future versions of Elasticsearch will require Java 11; your Java version from [/usr/java/jdk18/jre] does not meet this requirement. Consider switching to a distribution of Elasticsearch with a bundled JDK. If you are already using a distribution with a bundled JDK, ensure the JAVA_HOME environment variable is not set.
warning: usage of JAVA_HOME is deprecated, use ES_JAVA_HOME
Future versions of Elasticsearch will require Java 11; your Java version from [/usr/java/jdk18/jre] does not meet this requirement. Consider switching to a distribution of Elasticsearch with a bundled JDK. If you are already using a distribution with a bundled JDK, ensure the JAVA_HOME environment variable is not set.
[www@control bin]$
</code></pre> 
<p>查看日志</p> 
<blockquote> 
 <p>[www@control bin]$ tail -f /data01/elk/es-master/logs/cs-ES.log<br> [2023-09-24T16:14:46,409][INFO ][o.e.t.TransportService   ] [es-master] publish_address {172.17.0.1:9300}, bound_addresses {[::]:9300}<br> [2023-09-24T16:14:46,938][INFO ][o.e.b.BootstrapChecks    ] [es-master] bound or publishing to a non-loopback address, enforcing bootstrap checks<br> [2023-09-24T16:14:46,941][INFO ][o.e.c.c.Coordinator      ] [es-master] cluster UUID [bvD-MVnPQaKht_VRyiUrjQ]<br> [2023-09-24T16:14:47,156][INFO ][o.e.c.s.MasterService    ] [es-master] elected-as-master ([1] nodes joined)[{es-master}{LcRDDrhxQLu0N7FskDX-Jg}{4mLq7txaRM2UbbD9G9VKIQ}{172.17.0.1}{172.17.0.1:9300}{ilmr} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 9, version: 392, delta: master node changed {previous [], current [{es-master}{LcRDDrhxQLu0N7FskDX-Jg}{4mLq7txaRM2UbbD9G9VKIQ}{172.17.0.1}{172.17.0.1:9300}{ilmr}]}<br> [2023-09-24T16:14:47,343][INFO ][o.e.c.s.ClusterApplierService] [es-master] master node changed {previous [], current [{es-master}{LcRDDrhxQLu0N7FskDX-Jg}{4mLq7txaRM2UbbD9G9VKIQ}{172.17.0.1}{172.17.0.1:9300}{ilmr}]}, term: 9, version: 392, reason: Publication{term=9, version=392}<br> [2023-09-24T16:14:47,446][INFO ][o.e.h.AbstractHttpServerTransport] [es-master] publish_address {172.17.0.1:9200}, bound_addresses {[::]:9200}<br> [2023-09-24T16:14:47,446][INFO ][o.e.n.Node               ] [es-master] started<br> [2023-09-24T16:14:48,436][INFO ][o.e.l.LicenseService     ] [es-master] license [47e693d1-1baf-4abf-880d-2b7df859bbaf] mode [basic] - valid<br> [2023-09-24T16:14:48,438][INFO ][o.e.x.s.s.SecurityStatusChangeListener] [es-master] Active license is now [BASIC]; Security is enabled<br> [2023-09-24T16:14:48,444][INFO ][o.e.g.GatewayService     ] [es-master] recovered [14] indices into cluster_state</p> 
</blockquote> 
<p>查看端口</p> 
<blockquote> 
 <p>[www@control bin]$ ss -ntl<br> State       Recv-Q Send-Q        Local Address:Port     Peer Address:Port<br> LISTEN      0      128                           *:22                         *:*<br> LISTEN      0      100                   127.0.0.1:25                    *:*<br> LISTEN      0      128                        [::]:9200                     [::]:*<br> LISTEN      0      128                        [::]:9300                     [::]:*<br> LISTEN      0      128                        [::]:22                         [::]:*<br> LISTEN      0      100                       [::1]:25                        [::]:*<br> [www@control bin]$</p> 
</blockquote> 
<p>自动生成认证密码</p> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;"><span style="color:#4d4d4d;">      在其中</span><span style="color:#4d4d4d;">任意</span><span style="color:#4d4d4d;">一个</span><span style="color:#4d4d4d;">主</span><span style="color:#4d4d4d;">节点设置密码即可，设置完之后，数据会自动同步到其他节点。不需要进行拷贝，</span><span style="color:#4d4d4d;">扩充节点只要把认证文件拷贝到该节点上，启动扩容节点，集群会自动同步密码，不需要重启整个集群。</span></p> 
</blockquote> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="color:#4d4d4d;">到主节点/bin目录下执行</span></p> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;">./elasticsearch-setup-passwords auto</p> 
</blockquote> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#a2e043;">自动生成的密码切勿改动！！！！！牢记</span></p> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="572" src="https://images2.imgbox.com/e9/43/75xszu8V_o.png" width="1118"></p> 
 <p style="margin-left:.0001pt;text-align:justify;"></p> 
 <p style="margin-left:.0001pt;text-align:justify;"></p> 
 <p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#a2e043;">node节点和master节点的启动方式相同 这里就不在演示</span></p> 
 <p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#a2e043;">注意启动es一律使用普通用户启动  如果使用root启动会报错  切记切记  重点！！！！！！！！！！！</span></p> 
</blockquote> 
<p style="margin-left:.0001pt;text-align:justify;">还是说一下：</p> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;">[www@node3 root]$ cd /data01/elk/es-node2/elasticsearch-node2/bin/<br> [www@node3 bin]$ ll<br> 总用量 21532<br> -rwxr-xr-x. 1 www www     2896 5月  20 2021 elasticsearch<br> -rwxr-xr-x. 1 www www      501 5月  20 2021 elasticsearch-certgen<br> -rwxr-xr-x. 1 www www      493 5月  20 2021 elasticsearch-certutil<br> -rwxr-xr-x. 1 www www      996 5月  20 2021 elasticsearch-cli<br> -rwxr-xr-x. 1 www www      443 5月  20 2021 elasticsearch-croneval<br> -rwxr-xr-x. 1 www www     4856 5月  20 2021 elasticsearch-env<br> -rwxr-xr-x. 1 www www     1828 5月  20 2021 elasticsearch-env-from-file<br> -rwxr-xr-x. 1 www www      168 5月  20 2021 elasticsearch-geoip<br> -rwxr-xr-x. 1 www www      184 5月  20 2021 elasticsearch-keystore<br> -rwxr-xr-x. 1 www www      450 5月  20 2021 elasticsearch-migrate<br> -rwxr-xr-x. 1 www www      126 5月  20 2021 elasticsearch-node<br> -rwxr-xr-x. 1 www www      172 5月  20 2021 elasticsearch-plugin<br> -rwxr-xr-x. 1 www www      441 5月  20 2021 elasticsearch-saml-metadata<br> -rwxr-xr-x. 1 www www      439 5月  20 2021 elasticsearch-service-tokens<br> -rwxr-xr-x. 1 www www      448 5月  20 2021 elasticsearch-setup-passwords<br> -rwxr-xr-x. 1 www www      118 5月  20 2021 elasticsearch-shard<br> -rwxr-xr-x. 1 www www      483 5月  20 2021 elasticsearch-sql-cli<br> -rwxr-xr-x. 1 www www 21951112 5月  20 2021 elasticsearch-sql-cli-7.13.0.jar<br> -rwxr-xr-x. 1 www www      436 5月  20 2021 elasticsearch-syskeygen<br> -rwxr-xr-x. 1 www www      436 5月  20 2021 elasticsearch-users<br> -rwxr-xr-x. 1 www www      356 5月  20 2021 x-pack-env<br> -rwxr-xr-x. 1 www www      364 5月  20 2021 x-pack-security-env<br> -rwxr-xr-x. 1 www www      363 5月  20 2021 x-pack-watcher-env<br> [www@node3 bin]$ ./elasticsearch -d  <br> warning: usage of JAVA_HOME is deprecated, use ES_JAVA_HOME<br> Future versions of Elasticsearch will require Java 11; your Java version from [/home/jdk1.8.0_191/jre] does not meet this requirement. Consider switching to a distribution of Elasticsearch with a bundled JDK. If you are already using a distribution with a bundled JDK, ensure the JAVA_HOME environment variable is not set.<br> warning: usage of JAVA_HOME is deprecated, use ES_JAVA_HOME<br> Future versions of Elasticsearch will require Java 11; your Java version from [/home/jdk1.8.0_191/jre] does not meet this requirement. Consider switching to a distribution of Elasticsearch with a bundled JDK. If you are already using a distribution with a bundled JDK, ensure the JAVA_HOME environment variable is not set.<br> [www@node3 bin]$<br>  </p> 
</blockquote> 
<h4 id="6%E3%80%81%E6%9F%A5%E7%9C%8B%E9%9B%86%E7%BE%A4%E7%8A%B6%E6%80%81" style="margin-left:.0001pt;text-align:justify;">6、查看集群状态</h4> 
<p style="margin-left:.0001pt;text-align:justify;">命令用法：</p> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;">[www@control bin]$ curl -u 用户名:密码  -X GET 'http://192.168.12.11:9200/_cluster/health?pretty'<br> {<!-- --><br>   "cluster_name" : "cs-ES",<br>   "status" : "green",<br>   "timed_out" : false,<br>   "number_of_nodes" : 3,<br>   "number_of_data_nodes" : 2,<br>   "active_primary_shards" : 1,<br>   "active_shards" : 2,<br>   "relocating_shards" : 0,<br>   "initializing_shards" : 0,<br>   "unassigned_shards" : 0,<br>   "delayed_unassigned_shards" : 0,<br>   "number_of_pending_tasks" : 0,<br>   "number_of_in_flight_fetch" : 0,<br>   "task_max_waiting_in_queue_millis" : 0,<br>   "active_shards_percent_as_number" : 100.0<br>   </p> 
</blockquote> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="390" src="https://images2.imgbox.com/53/10/p8FtH3nW_o.png" width="1198"></p> 
<blockquote> 
 <p><code>  </code><code>"cluster_name"</code> <code>: </code><code>"cs-ES"</code><code>,   </code><code>#集群名称</code></p> 
 <p><code>  </code><code>"status"</code> <code>: </code><code>"green"</code><code>,</code><code>为 green 则代表健康没问题，如果是 yellow 或者 red 则是集群有问题</code></p> 
 <p><code>  </code><code>"timed_out"</code> <code>: </code><code>false</code><code>,         </code><code>#是否有超时</code></p> 
 <p><code>  </code><code>"number_of_nodes"</code> <code>: 3,         </code><code>#集群中的节点数量</code></p> 
 <p><code>  </code><code>"number_of_data_nodes"</code> <code>: 2,</code></p> 
 <p><code>  </code><code>"active_primary_shards"</code> <code>: 2234,</code></p> 
 <p><code>  </code><code>"active_shards"</code> <code>: 4468,</code></p> 
 <p><code>  </code><code>"relocating_shards"</code> <code>: 0,</code></p> 
 <p><code>  </code><code>"initializing_shards"</code> <code>: 0,</code></p> 
 <p><code>  </code><code>"unassigned_shards"</code> <code>: 0,</code></p> 
 <p><code>  </code><code>"delayed_unassigned_shards"</code> <code>: 0,</code></p> 
 <p><code>  </code><code>"number_of_pending_tasks"</code> <code>: 0,</code></p> 
 <p><code>  </code><code>"number_of_in_flight_fetch"</code> <code>: 0,</code></p> 
 <p><code>  </code><code>"task_max_waiting_in_queue_millis"</code> <code>: 0,</code></p> 
 <p><code>  </code><code>"active_shards_percent_as_number"</code> <code>: 100.0      </code><code>#集群分片的可用性百分比，如果为0则表示不可用</code></p> 
</blockquote> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="689" src="https://images2.imgbox.com/6f/17/0D1NTpia_o.png" width="1200"></p> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;">[www@control bin]$<br> health:  green代表健康；yellow代表分配了所有主分片，但至少缺少一个副本，此时集群数据仍旧完整；red代表部分主分片不可用，可能已经丢失数据。<br> pri：primary缩写，主分片数量<br> rep：副分片数量<br> docs.count： Lucene 级别的文档数量<br> docs.deleted： 删除的文档<br> store.size：全部分片大小（包含副本）<br> pri.store.size：主分片大小</p> 
</blockquote> 
<h2 id="%E4%BA%94%E3%80%81kibana" style="margin-left:.0001pt;text-align:justify;">五、kibana</h2> 
<h4 id="1%E3%80%81%E9%83%A8%E7%BD%B2%E7%8E%AF%E5%A2%83">1、部署环境</h4> 
<table border="1" cellpadding="1" cellspacing="1"><tbody><tr><td>服务器ip</td><td>应用</td><td>安装目录</td><td>端口</td></tr><tr><td>192.168.12.15</td><td>kibana</td><td>/data01/kibana</td><td>15601</td></tr><tr><td></td><td></td><td></td><td></td></tr></tbody></table> 
<h4 id="2%E3%80%81%E5%AE%89%E8%A3%85kibana" style="background-color:transparent;">2、安装kibana</h4> 
<blockquote> 
 <p>[root@node5 kibana]# tar -xf kibana-7.13.0-linux-x86_64.tar.gz   #解压kibana</p> 
 <p>[root@node5 kibana]# cd kibana-7.13<br> [root@node5 kibana-7.13]# ls<br> bin  config  data  LICENSE.txt  node  node_modules  NOTICE.txt  package.json  plugins  README.txt  src  x-pack<br> [root@node5 kibana-7.13]# cd config/<br> [root@node5 config]#<br> [root@node5 config]#<br> [root@node5 config]# pwd<br> /data01/kibana/kibana-7.13/config</p> 
 <p></p> 
 <p>[root@node5 config]# vim kibana.yml   #添加以下配置<br> [root@node5 config]#<br><br> server.port: 15601<br> server.host: "0.0.0.0"<br> elasticsearch.hosts: ["http://192.168.12.12:9200","http://192.168.12.11:9200","http://192.168.12.13:9200"]<br> elasticsearch.username: "kibana_system"<br> elasticsearch.password: "密码"<br> i18n.locale: "zh-CN"</p> 
</blockquote> 
<h4 id="3%E3%80%81%E5%90%AF%E5%8A%A8kibana">3、启动kibana</h4> 
<blockquote> 
 <p>[root@node5 config]# cd ../bin/<br> [root@node5 bin]# ll<br> 总用量 2396<br> -rwxr-xr-x 1 www www     850 5月  20 2021 kibana<br> -rwxr-xr-x 1 www www     783 5月  20 2021 kibana-encryption-keys<br> -rwxr-xr-x 1 www www     776 5月  20 2021 kibana-keystore<br> -rwxr-xr-x 1 www www     813 5月  20 2021 kibana-plugin<br> -rw------- 1 www www 2434587 9月  24 00:00 nohup.out<br> [root@node5 bin]# nohup ./kibana &amp;<br> [1] 1415<br> [root@node5 bin]# nohup: 忽略输入并把输出追加到"nohup.out"</p> 
</blockquote> 
<p>4、使用浏览器访问kibana</p> 
<p>http://192.168.12.15:15601</p> 
<p><img alt="" height="778" src="https://images2.imgbox.com/e8/13/ox0Byh32_o.png" width="1200"></p> 
<p>输入es账号密码登录即可</p> 
<h2 id="%E5%85%AD%E3%80%81%E9%83%A8%E7%BD%B2kafka">六、部署kafka</h2> 
<h4>1、部署环境</h4> 
<table border="1" cellpadding="1" cellspacing="1"><tbody><tr><td>服务器ip</td><td>应用</td><td>安装目录</td><td>端口</td></tr><tr><td>192.168.12.14</td><td>kafka</td><td>/data01/kafka</td><td>9092</td></tr><tr><td></td><td></td><td></td><td></td></tr></tbody></table> 
<h4 id="2%E3%80%81%E5%AE%89%E8%A3%85kafka">2、安装kafka</h4> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;">mkdir -p /data01kafka<br> cd /data01/kafka<br> mkdir  /data01/kafka/zookeeper_data <br> mkdir  data01/kafka/kafka-logs<br> tar -xzvf kafka_2.12-2.8.0.tgz</p> 
</blockquote> 
<p style="text-align:justify;">修改自带zookeeper配置文件zookeeper.properties</p> 
<blockquote> 
 <p style="text-align:justify;">[root@node4 config]# vim zookeeper.properties<br> [root@node4 config]#<br><br> dataDir=/data01/kafka/zookeeper_data<br> # the port at which the clients will connect<br> clientPort=2181<br> # disable the per-ip limit on the number of connections since this is a non-production config<br> maxClientCnxns=0<br> # Disable the adminserver by default to avoid port conflicts.<br> # Set the port to something non-conflicting if choosing to enable this<br> admin.enableServer=false<br> # admin.serverPort=8080</p> 
</blockquote> 
<p style="margin-left:.0001pt;text-align:justify;">修改kafka配置文件server.properties</p> 
<blockquote> 
 <p>[root@node4 config]# vim server.properties<br> [root@node4 config]#<br><br> broker.id=0</p> 
 <p>listeners=PLAINTEXT://0.0.0.0:9092<br> advertised.listeners=PLAINTEXT://192.168.12.14:9092</p> 
 <p>#数据存储目录和时效<br> log.dirs=/data01/kafka/kafka-logs<br> log.retention.hours=12<br> #是否允许删除主题<br> delete.topic.enable=true</p> 
 <p>#broker处理消息的最大线程数，一般情况下数量为cpu核数<br> num.network.threads=1<br> #broker处理磁盘IO的线程数，数值为cpu核数2倍<br> num.io.threads=2<br> #每个topic的分区个数，若是在topic创建时候没有指定的话会被topic创建时的指定参数覆盖<br> num.partitions =1<br>  </p> 
</blockquote> 
<h4 id="3%E3%80%81%E5%90%AF%E5%8A%A8zookeeper" style="text-align:justify;">3、启动zookeeper</h4> 
<blockquote> 
 <p>[root@node4 bin]# nohup ./zookeeper-server-start.sh ../config/zookeeper.properties &amp;<br> [1] 1319<br> [root@node4 bin]# nohup: 忽略输入并把输出追加到"nohup.out"<br> [root@node4 bin]#<br>  </p> 
</blockquote> 
<h4 id="4%E3%80%81%E5%90%AF%E5%8A%A8kafka">4、启动kafka</h4> 
<blockquote> 
 <p>[root@node4 bin]# nohup ./kafka-server-start.sh  ../config/server.properties &amp;<br> [2] 1676<br> [root@node4 bin]# nohup: 忽略输入并把输出追加到"nohup.out"</p> 
 <p>[root@node4 bin]#<br>  </p> 
</blockquote> 
<h4 id="5%E3%80%81%E9%AA%8C%E8%AF%81kafka">5、验证kafka</h4> 
<blockquote> 
 <p><span style="background-color:#a2e043;">创建一个test主题(topic)</span><br> [root@node4 bin]# ./kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic  test<br> Created topic test.<br> [root@node4 bin]#</p> 
 <p><br><span style="background-color:#a2e043;">查看主题列表</span><br> [root@node4 bin]# ./kafka-topics.sh --list --zookeeper localhost:2181<br> test<br> [root@node4 bin]#</p> 
 <p><br><span style="background-color:#a2e043;">删除一个主题（并删除对应文件夹）</span><br> ./kafka-topics.sh --delete --zookeeper localhost:2181  --topic test</p> 
 <p><br><span style="background-color:#a2e043;">生产消息</span><br> [root@node4 bin]# ./kafka-console-producer.sh --broker-list localhost:9092 --topic test<br> &gt;12312313131<br> &gt;12313131313216<br> &gt;1222nksnananklnfkanfa<br> &gt;jslknflkan<br> &gt;mnslnalnf<br> &gt;snkoann</p> 
 <p>##################################################################</p> 
 <p><span style="background-color:#a2e043;">消费消息</span></p> 
 <p>[root@node4 bin]# ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning<br> 12312313131<br> 12313131313216<br> 1222nksnananklnfkanfa<br> jslknflkan<br> mnslnalnf<br> snkoann</p> 
 <p><span style="background-color:#a2e043;">看到输入的信息则消费成功</span></p> 
 <p>00000000000000000000.index<br> -rw-r--r--. 1 root root 4.0K 9月  21 10:18 00000000000000000000.log<br> -rw-r--r--. 1 root root  10M 9月  21 09:21 00000000000000000000.timeindex<br> -rw-r--r--. 1 root root    8 9月  21 09:21 leader-epoch-checkpoint</p> 
</blockquote> 
<h2 id="%E4%B8%83%E3%80%81logstash%E9%83%A8%E7%BD%B2">七、logstash部署</h2> 
<h4 id="1%E3%80%81%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2">1、环境部署</h4> 
<table border="1" cellpadding="1" cellspacing="1"><tbody><tr><td>服务器ip</td><td>应用</td><td>安装目录</td><td>端口</td></tr><tr><td>192.168.12.16</td><td>logstash</td><td>/data01/logstash</td><td>9600</td></tr><tr><td></td><td></td><td></td><td></td></tr></tbody></table> 
<h4 id="2%E3%80%81%E5%AE%89%E8%A3%85lostash">2、安装lostash</h4> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;">cd /data01/logstash</p> 
 <p style="margin-left:.0001pt;text-align:justify;">tar -xf logstash-7.12.0-linux-x86_64.tar.gz</p> 
 <p style="margin-left:.0001pt;text-align:justify;"></p> 
 <p style="margin-left:.0001pt;text-align:justify;">cd logstash-7.12.0/config/<br> vim jvm.options<br><span style="background-color:#a2e043;">#根据主机本配置进行调整</span><br> -Xms512g<br> -Xmx512g<br> vim logstash.yml<br> # pipeline线程数，官方建议是等于CPU内核数<br> pipeline.workers: 1<br> # 实际output时的线程数<br> pipeline.output.workers: 2<br> # 每次发送的事件数<br> pipeline.batch.size: 5000<br> # 发送延时</p> 
 <p style="margin-left:.0001pt;text-align:justify;">pipeline.batch.delay: 10</p> 
 <p style="margin-left:.0001pt;text-align:justify;"></p> 
</blockquote> 
<p style="margin-left:.0001pt;text-align:justify;">配置logstash过滤规则</p> 
<blockquote> 
 <p><span style="background-color:#a2e043;">#这里没有配置filter 过滤的原因是我的nginx日志已经是json格式了 没必要在进行过滤了</span><br> [root@node6 config]# cat logstash-nginx.conf<br> input{<!-- --><br>     kafka {<!-- --><br>       bootstrap_servers =&gt; ["192.168.12.14:9092"]<br>       auto_offset_reset =&gt; "latest"<br>       consumer_threads =&gt; 1<br>       decorate_events =&gt; true<br>       topics =&gt; ["intfng-logs"]<br>       codec =&gt; "json"</p> 
 <p>}<br> }</p> 
 <p>output {<!-- --><br>     elasticsearch {<!-- --><br>       hosts =&gt; ["192.168.12.11:9200"]<br>       user =&gt; "elastic"<br>       password =&gt; "iewmrmeXBXEhmjNbW8UK"<br>       index =&gt; "intfng-logs-%{+YYYY.MM.dd}"<br>         }<br> }<br>  </p> 
</blockquote> 
<h4 id="3%E3%80%81%E5%90%AF%E5%8A%A8logstash" style="margin-left:.0001pt;text-align:justify;">3、启动logstash</h4> 
<blockquote> 
 <p>[root@node6 bin]# pwd<br> /data01/logstash/logstash-7.12.0/bin<br> [root@node6 bin]#<br> [root@node6 bin]#<br> [root@node6 bin]#  nohup ./logstash -f ../config/logstash-nginx.conf &amp;<br> [1] 1450<br> [root@node6 bin]# nohup: 忽略输入并把输出追加到"nohup.out"</p> 
 <p>[root@node6 bin]#<br>  </p> 
 <p>#查看端口</p> 
 <p>[root@node6 bin]# ss -ntl<br> State       Recv-Q Send-Q        Local Address:Port            Peer Address:Port<br> LISTEN      0      128                              *:22                               *:*<br> LISTEN      0      100                      127.0.0.1:25                           *:*<br> LISTEN      0      128                           [::]:22                                 [::]:*<br> LISTEN      0      100                          [::1]:25                                [::]:*<br> LISTEN      0      50                   [::ffff:127.0.0.1]:9600                    [::]:*<br> [root@node6 bin]#<br>  </p> 
 <p><span style="background-color:#a2e043;">#这里logstash端口我是使用默认的端口 如果你要使用其他端口 可以修改logstash.ym配置文件</span></p> 
</blockquote> 
<h2 id="%E5%85%AB%E3%80%81filebeat%E9%83%A8%E7%BD%B2">八、filebeat部署</h2> 
<h4>1、环境部署</h4> 
<table border="1" cellpadding="1" cellspacing="1"><tbody><tr><td>服务器ip</td><td>应用</td><td>安装目录</td><td>端口</td></tr><tr><td>192.168.12.11</td><td>filebeat</td><td>/data01/filebeat</td><td>无</td></tr><tr><td></td><td></td><td></td><td></td></tr></tbody></table> 
<h4 id="2%E3%80%81%E5%AE%89%E8%A3%85filebeat">2、安装filebeat</h4> 
<blockquote> 
 <p>[root@control filebeat]# ls<br> filebeat-7.12  filebeat-7.12.0-linux-x86_64.tar.gz<br> [root@control filebeat]#<br> [root@control filebeat]#</p> 
 <p>tar -xf  filebeat-7.12.0-linux-x86_64.tar.gz</p> 
 <p></p> 
 <p><span style="background-color:#a2e043;">#配置filebeat.yml</span></p> 
 <p>[root@control filebeat-7.12]# cat filebeat.yml<br> filebeat.inputs:<br>  - type: log<br>    enabled: true<br>    paths:<br>     - /data01/nginx/logs/access.log<br>    fields:<br>     type: "nglog"<br> output.kafka:<br>   hosts: ["192.168.12.14:9092"]<br>   topic: "intfng-logs"<br>  # version: "2.8.0"<br>   partition.round_robin:<br>     reachable_only: false<br>   required_acks: 1<br> #processors:<br> # - drop_fields:<br> #     fields: ["log","host","input","agent","ecs"]]</p> 
 <p>[root@control filebeat-7.12]#<br>  </p> 
</blockquote> 
<h4 id="3%E3%80%81%E5%90%AF%E5%8A%A8filebeat">3、启动filebeat</h4> 
<blockquote> 
 <p>[root@control filebeat-7.12]# pwd<br> /data01/filebeat/filebeat-7.12<br> [root@control filebeat-7.12]# nohup ./filebeat -e -c filebeat.yml &gt; filebeat.log &amp;<br> [1] 3597<br> [root@control filebeat-7.12]# nohup: 忽略输入重定向错误到标准输出端<br>  </p> 
 <p><span style="background-color:#a2e043;">#查看filebeat是否启动</span></p> 
 <p>[www@control filebeat-7.12]$ tail -f filebeat.log<br> 2023-09-24T17:56:32.108+0800    INFO    instance/beat.go:468    filebeat start running.<br> 2023-09-24T17:56:32.108+0800    INFO    [monitoring]    log/log.go:117  Starting metrics logging every 30s<br> 2023-09-24T17:56:32.109+0800    INFO    memlog/store.go:119     Loading data file of '/data01/filebeat/filebeat-7.12/data/registry/filebeat' succeeded. Active transaction id=0<br> 2023-09-24T17:56:32.110+0800    INFO    memlog/store.go:124     Finished loading transaction log file for '/data01/filebeat/filebeat-7.12/data/registry/filebeat'. Active transaction id=100<br> 2023-09-24T17:56:32.110+0800    WARN    beater/filebeat.go:381  Filebeat is unable to load the Ingest Node pipelines for the configured modules because the Elasticsearch output is not configured/enabled. If you have already loaded the Ingest Node pipelines or are using Logstash pipelines, you can ignore this warning.<br> 2023-09-24T17:56:32.111+0800    INFO    [registrar]     registrar/registrar.go:109      States Loaded from registrar: 1<br> 2023-09-24T17:56:32.111+0800    INFO    [crawler]       beater/crawler.go:71    Loading Inputs: 1<br> 2023-09-24T17:56:32.111+0800    INFO    log/input.go:157        Configured paths: [/data01/nginx/logs/access.log]<br> 2023-09-24T17:56:32.111+0800    INFO    [crawler]       beater/crawler.go:141   Starting input (ID: 15608654469098508912)<br> 2023-09-24T17:56:32.111+0800    INFO    [crawler]       beater/crawler.go:108   Loading and starting Inputs completed. Enabled inputs: 1<br> ^C<br> [www@control filebeat-7.12]$ ^C<br> [www@control filebeat-7.12]$ ps -ef | grep filebeat<br> www        3623   3607  0 17:56 pts/0    00:00:00 ./filebeat -e -c filebeat.yml<br> www        3633   3607  0 17:57 pts/0    00:00:00 grep --color=auto filebeat<br> [www@control filebeat-7.12]$<br>  </p> 
</blockquote> 
<h2 id="%E4%B9%9D%E3%80%81%E5%90%AF%E5%8A%A8nginx%E9%AA%8C%E8%AF%81kibana%E6%98%AF%E5%90%A6%E6%AD%A3%E5%B8%B8%E5%B1%95%E7%A4%BA%E6%95%B0%E6%8D%AE">九、启动nginx验证kibana是否正常展示数据</h2> 
<h4 id="1%E3%80%81%E5%90%AF%E5%8A%A8nginx">1、启动nginx</h4> 
<blockquote> 
 <p>[www@control data01]$ cd nginx/sbin/<br> [www@control sbin]$ pwd<br> /data01/nginx/sbin<br> [www@control sbin]$<br> [www@control sbin]$<br> [www@control sbin]$ l<br> bash: l: 未找到命令<br> [www@control sbin]$ ll<br> 总用量 5980<br> -rwxr-xr-x 1 www www 6122600 7月   8 09:44 nginx<br> [www@control sbin]$</p> 
 <p>[www@control sbin]$ <span style="background-color:#a2e043;">sudo ./nginx  #启动nginx</span><br> [www@control sbin]$</p> 
</blockquote> 
<p>访问nginx生成日志filebeat采集</p> 
<p><img alt="" height="378" src="https://images2.imgbox.com/61/33/UusgacWO_o.png" width="1200"></p> 
<h4 id="2%E3%80%81%E7%99%BB%E5%BD%95kibana%E9%AA%8C%E8%AF%81">2、登录kibana验证</h4> 
<p><img alt="" height="772" src="https://images2.imgbox.com/d9/2e/BLuA4LJF_o.png" width="1200"></p> 
<p><img alt="" height="936" src="https://images2.imgbox.com/8a/43/2PON7Exr_o.png" width="1200"></p> 
<p>我们可以看到kibana上数据已经正常展示了，关于elk+kafka的搭建到这里就结束了，如果有不足之处欢迎评论区留言</p> 
<p></p> 
<p></p> 
<p></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c100d4ea86a40d89fcebf84c415d4f9b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Python 逢七拍手小游戏1.0</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/826f69663dee27b760320a1b1cf16500/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">数据结构 线性表习题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>