<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>DeepLearning - Overview of Sequence model - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="DeepLearning - Overview of Sequence model" />
<meta property="og:description" content="I have had a hard time trying to understand recurrent model. Compared to Ng&#39;s deep learning course, I found Deep Learning book written by Ian, Yoshua and Aaron much easier to understand.
This post is structure in following order:
Intuitive interpretation of RNNBasic RNN computation and different structureRNN limitation and variants Why we need Recurrent Neural Network The main reason behind Convolution neural network is using sparse connection and parameter sharing to reduce the scale of NN." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/eaaa132b7c7807629009b87ce8963fa2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-04-15T10:17:00+08:00" />
<meta property="article:modified_time" content="2018-04-15T10:17:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">DeepLearning - Overview of Sequence model</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown"> 
 <p>I have had a hard time trying to understand recurrent model. Compared to Ng's deep learning course, I found Deep Learning book written by Ian, Yoshua and Aaron much easier to understand.</p> 
 <p>This post is structure in following order:</p> 
 <ul><li>Intuitive interpretation of RNN</li><li>Basic RNN computation and different structure</li><li>RNN limitation and variants</li></ul> 
 <h3 id="why-we-need-recurrent-neural-network">Why we need Recurrent Neural Network</h3> 
 <p>The main reason behind Convolution neural network is using sparse connection and parameter sharing to reduce the scale of NN. Then why we need recurrent neural network？</p> 
 <p>If we use traditional neural network to deal with a series input <span class="math inline">\((x^{&lt;1&gt;},x^{&lt;2&gt;},x^{&lt;3&gt;},..,x^{&lt;t&gt;})\)</span>, like speech, text, we will encounter 2 main challenges:</p> 
 <ul><li>The input length varies. Of course we can cut or zero-pad the training data to have the same length. But how can we deal with the test data with unknown length?</li><li>For every time stamp, we will need a independent parameter. It makes it hard to capture the pattern in the sequence, compared to traditional time-series model like AR, ARIMA.</li></ul> 
 <p>CNN can partly solve the second problem. By using a kernel with size K, the model can capture the pattern of k neighbors. Therefore, when the input length is fixed or limited, sometimes we do use CNN to deal with time-series data. However compared with sequence model, CNN has a very limited(shallow) parameter sharing. I will further talk about this below.</p> 
 <h3 id="rnn-intuitive-interpretation">RNN Intuitive Interpretation</h3> 
 <p>For a given sequence<span class="math inline">\((x^{&lt;1&gt;},x^{&lt;2&gt;},x^{&lt;3&gt;},..,x^{&lt;t&gt;})\)</span>, RNN takes in input in the following way:</p> 
 <p><img src="https://images2.imgbox.com/95/3e/9zCwrBUZ_o.png" alt="recursion.png-45.8kB"></p> 
 <p>Regardless of how the output looks like, at each timestamp, the information is processed in the following way:<br><span class="math display">\[h^{&lt;t&gt;} = f(h^{&lt;t-1&gt;}, x^{&lt;t&gt;}, \theta)\]</span></p> 
 <p>I consider the above recurrent structure as having a neural network with unlimited number of hidden layer. Each hidden layer takes in information from input and previous hidden layer and they all share same parameter. The number of hidden layer is equal to your input length (time step).</p> 
 <p>There are 2 main advantages of above <strong>Recurrent</strong> structure:</p> 
 <ul><li>The input size is fixed regardless of the the sequence length. Therefore it can be applied to any sequence without limitation. 
   <ul><li>Case1: if the input is a multi-dimensional timeseries like open/high/low/close price in stock market, a 4-dimensional timeseries, then the input size is 4.</li><li>Case2: if we use 10-dimensional vector to represent each word in sentence, then no matter how long the sentence is, the input size is 10.</li></ul></li><li>Same parameter <span class="math inline">\(\theta\)</span> learnt at different timestamp. <span class="math inline">\(\theta\)</span> is not time-related, similar to the transformation matrix in Markov Chain.</li></ul> 
 <p>And if we further unfold the above formula, we will get following:<br><span class="math display">\[ \begin{align} h^{&lt;t&gt;} &amp; = f(h^{&lt;t-1&gt;}, x^{&lt;t&gt;}, \theta) \\&amp; = g^{&lt;t&gt;}(x^{&lt;1&gt;},x^{&lt;2&gt;},..,x^{&lt;t&gt;}) \end{align} \]</span></p> 
 <p>Basically for hidden layer at time <span class="math inline">\(t\)</span>, it can include all the information from the very beginning. Compared to Recurrent structure, CNN can only incorporate limited information from nearest K timestamp, where k is the kernel size.</p> 
 <h3 id="basic-rnn-computation">Basic RNN Computation</h3> 
 <p>Now let's add more details to basic RNN, below is the most representative structure of Basic RNN:</p> 
 <p align="center"> <img src="https://images2.imgbox.com/be/08/kzAv46j9_o.png" width="600" height="400" alt="rnn_1.png"></p> 
 <p>For each hidden layer, it takes both the information from input and the previous hidden layer,then generate an output. Since all the other format of RNN can be derived from this, we will go through the computation of RNN in above structure.</p> 
 <h4 id="forward-propogation">1. Forward propogation</h4> 
 <p>Here I will mainly use the notation from Andrew's Course with small modification.<br><span class="math inline">\(a^{&lt;t&gt;}\)</span> denotes the hidden layer output at time t<br><span class="math inline">\(x^{&lt;t&gt;}\)</span> denotes the input at time t<br><span class="math inline">\(y^{&lt;t&gt;}\)</span> denotes the target output at time t<br> $\hat{y}^{} $ denotes model output at time t<br><span class="math inline">\(o^{&lt;t&gt;}\)</span> denotes the output before activation function.<br><span class="math inline">\(w_{aa}\)</span>, <span class="math inline">\(w_{ax}\)</span>, <span class="math inline">\(b_{a}\)</span> are the weight and bias matrix at hidden layer<br><span class="math inline">\(w_{ya}\)</span>, <span class="math inline">\(b_{y}\)</span> are the weight and bias matrix used at output.</p> 
 <p>We will use <span class="math inline">\(tanh\)</span> activation function at hidden and <span class="math inline">\(softmax\)</span> at output.</p> 
 <p>For a given sample <span class="math inline">\(( (x^{&lt;1&gt;},x^{&lt;2&gt;},..,x^{&lt;T&gt;} ), (y^{&lt;1&gt;},y^{&lt;2&gt;},..,y^{&lt;T&gt;} ) )\)</span>, RNN will start at time 0, and start forward propagation till T. At each time stamp t, following computation is made:<br><span class="math display">\[ \begin{align} a^{&lt;t&gt;} &amp;= tanh(w_{aa}a^{&lt;t-1&gt;} + w_{ax}x^{&lt;t&gt;} + b_{a})\\ o^{&lt;t&gt;} &amp;= w_{ya}a^{&lt;t&gt;} + b{y} \\ \hat{y}^{&lt;t&gt;} &amp;= softmax( o^{&lt;t&gt;} ) \end{align} \]</span></p> 
 <p>We can further simplify the above formula by combining the weight matrix like below:<br><span class="math inline">\(W_a = [w_{aa}|w_{ax}]\)</span>, <span class="math inline">\([a^{&lt;t-1&gt;},x^{&lt;t&gt;}] = [\frac{a^{&lt;t-1&gt;}}{,x^{&lt;t&gt;}}]\)</span></p> 
 <p><span class="math display">\[ \begin{align} a^{&lt;t&gt;} = tanh(w_{a}[a^{&lt;t-1&gt;},x^{&lt;t&gt;}]+ b_{a} \end{align} \]</span><br> The negative log likelihood loss function of a sequence input is defined as following:<br><span class="math display">\[ \begin{align} L({x^{&lt;1&gt;},..,x^{&lt;T&gt;}}, {y^{&lt;1&gt;},..,y^{&lt;T&gt;}}) &amp; = \sum{L^{&lt;t&gt;}} \\ &amp; = -\sum{\log{p_{model}(y^{&lt;t&gt;}|x^{&lt;1&gt;},...,x^{&lt;t&gt;})}} \end{align} \]</span></p> 
 <h4 id="backward-propagation">2. Backward propagation</h4> 
 <p>In recurrent neural network, same parameter is shared at different time. Therefore we need the gradient of all nodes to calculate the gradient of parameter. And don't forget that the gradient of each node not only takes information from output, but also from next hidden neuron. That's why <strong>Back-Propagation-through-time</strong> can not run parallel, but have to follow descending order of time.</p> 
 <p>Start from T, given loss function, and softmax activation function, gradient of <span class="math inline">\(o^{&lt;t&gt;}\)</span> is following:</p> 
 <p><span class="math display">\[ \begin{align} \frac{\partial L}{\partial o^{&lt;T&gt;}} &amp; = \frac{\partial L}{\partial L^{&lt;T&gt;}} \cdot \frac{\partial L^{&lt;T&gt;}}{\partial \hat{y}^{&lt;T&gt;}} \cdot \frac{\partial \hat{y}^{&lt;T&gt;}}{\partial {o}^{&lt;T&gt;}} \\ &amp; = 1 \cdot (-\frac{y^{&lt;T&gt;}}{\hat{y}^{&lt;T&gt;}} + \frac{1-y^{&lt;T&gt;}}{1-\hat{y}^{&lt;T&gt;}}) \cdot (\hat{y}^{&lt;T&gt;}(1-\hat{y}^{&lt;T&gt;})) \\&amp; = \hat{y}^{&lt;T&gt;}-y^{&lt;T&gt;} \end{align} \]</span><br> Therefroe we can get gradient of <span class="math inline">\(a^{&lt;T&gt;}\)</span> as below<br><span class="math display">\[ \begin{align} \frac{\partial L}{\partial a^{&lt;T&gt;}} &amp;= \frac{\partial L}{\partial o^{&lt;T&gt;}} \cdot \frac{\partial o^{&lt;T&gt;}}{\partial a^{&lt;T&gt;}} \\&amp; = W_{ya}^T \cdot \frac{\partial L}{\partial o^{&lt;T&gt;}} \end{align} \]</span><br> For the last node, the gradient only consider <span class="math inline">\(o^{&lt;T&gt;}\)</span>, while for all the other nodes, the gradient need to consider both <span class="math inline">\(o^{&lt;t&gt;}\)</span> and <span class="math inline">\(a^{&lt;t+1&gt;}\)</span>, we will get following:<br><span class="math display">\[ \begin{align} \frac{\partial L}{\partial a^{&lt;t&gt;}} &amp;= \frac{\partial L}{\partial o^{&lt;t&gt;}} \cdot \frac{\partial o^{&lt;t&gt;}}{\partial a^{&lt;t&gt;}} + \frac{\partial L}{\partial a^{&lt;t+1&gt;}} \cdot \frac{\partial a^{&lt;t+1&gt;}}{\partial a^{&lt;t&gt;}} \\ &amp; = W_{ya}^T \cdot \frac{\partial L}{\partial o^{&lt;t&gt;}} + W_{aa}^T \cdot diag(1-(a^{&lt;t+1&gt;})^2) \cdot \frac{\partial L}{\partial a^{&lt;t+1&gt;}} \end{align} \]</span></p> 
 <p>Now we have the gradient for each hidden neuron, we can further get the gradient for all the parameter, given that same parameters are shared at different time stamp. see below:<br><span class="math display">\[ \begin{align} \frac{\partial L}{\partial b_y} &amp;= \sum_t{\frac{\partial L}{\partial o^{&lt;t&gt;}} }\\ \frac{\partial L}{\partial b_a} &amp;= \sum_t{ diag(1-(a^{&lt;t&gt;})^2) \cdot \frac{\partial L}{\partial a^{&lt;t&gt;}} } \\ \frac{\partial L}{\partial w_{ya}} &amp;= \sum_t{ \frac{\partial L}{\partial o^{&lt;t&gt;}} \cdot {a^{&lt;t&gt;}}^T } \\ \frac{\partial L}{\partial w_{aa}} &amp;= \sum_t{ \frac{\partial L}{\partial a^{&lt;t&gt;}} \cdot diag(1-(a^{&lt;t&gt;})^2) \cdot {a^{&lt;t-1&gt;}}^T } \\ \frac{\partial L}{\partial w_{ax}} &amp;= \sum_t{ \frac{\partial L}{\partial a^{&lt;t&gt;}} \cdot diag(1-(a^{&lt;t&gt;})^2) \cdot {x^{&lt;t&gt;}}^T } \\ \end{align} \]</span></p> 
 <h4 id="different-structures-of-rnn">3. Different structures of RNN</h4> 
 <p>So far the RNN we have talked about has 2 main features: input length = output length, and direct connection between hidden neuron. While actually RNN have many variants. We can further classify the difference as following:</p> 
 <ul><li>How recurrent works</li><li>Different input and output length</li></ul> 
 <p>As for how recurrent works, there are 2 basic types:</p> 
 <ol><li>there is recurrent connection between hidden neuron, see below RNN1</li><li>there is recurrent connection between output and hidden neuron, see below RNN2</li></ol> 
 <p align="center"> <img src="https://images2.imgbox.com/dd/9f/7TFjJdjx_o.png" width="400" height="400" alt="RNN1.png"></p> 
 <p align="center"> RNN1. Hidden to Hidden </p> 
 <p align="center"> <img src="https://images2.imgbox.com/7a/a1/VH7K3J9f_o.png" width="400" height="400" alt="RNN2.png"></p> 
 <p align="center"> RNN2. Ouput to Hidden </p> 
 <p>Apparently Hidden to Hidden Recurrent will contain more information about the history. Because output at each timestamp is optimized to simulate the target output, it can't have as much information as its hidden neuron.</p> 
 <p>Then why we need the second type, since it loses information in propagation. To answer to this we will have to talk about <strong>BPTT - Back Propagation Through Time</strong>. Due to the recurrent structure, we have to process hidden layer one after another in both forward and backward propagation. Therefore both memory usage and computation time is O(T) and we can't use parallel computation to speed up.</p> 
 <p>To be able to use parallel computation, we have to disconnect history and present while still using the information from past. One solution is a derived type of Output to hidden recurrent - <strong>Teacher forcing</strong>. Basically it passes the realized output instead of estimated output to the next hidden neuron. Like below:</p> 
 <p></p> 
 <p align="center"><br><img src="https://images2.imgbox.com/d9/ed/fP3F7uLE_o.png" width="400" height="400" alt="Teacher%20forcing.png"><br></p> 
 <p align="center">Teacher forcing Train &amp; Test<br></p> 
 <p>It changes the original maximum log likelihood into conditional log likelihood.<br><span class="math display">\[ \begin{align} &amp; p(y^{(1)},y^{(2)}|x^{(1)},x^{(2)}) \\ &amp;= p(y^{(1)}|X^{(1)},X^{(2)}) + p(y^{(1)}|y^{(1)}, X^{(1)},y^{(1)}) \end{align} \]</span></p> 
 <p>However teacher forcing has a big problem that the input distribution of training and testing is different, since we don't have realized output in testing. Later we will talk about how to overcome such problem.</p> 
 <p>Next Let's talk about input and output length. In the below image, it gives all the possibilities:<br><img src="https://images2.imgbox.com/4c/03/m5hyskAK_o.jpg" alt="diags.jpeg-67.1kB"></p> 
 <p><strong>many-to-many(seq2seq) with same length</strong> is the one we have been talking so far. Let' go through others.</p> 
 <p><strong>many-to-one(seq2one)</strong> can be used in sentiment analysis. The only difference between the RNN we have been talking about and many-to-one is that each sequence input has only 1 ouput.<br></p> 
 <p align="center"><br><img src="https://images2.imgbox.com/4a/67/HqovUxAR_o.png" width="400" height="150" alt="seq2one.PNG"><br></p> 
 <p align="center"><br></p> 
 <p><strong>one-to-many(one2seq)</strong> can be used in music generation. It uses the second recurrent structure - feeding output to next hidden neuron.</p> 
 <p></p> 
 <p align="center"><br><img src="https://images2.imgbox.com/7f/50/V8ILJkLS_o.png" width="400" height="150" alt="one2seq.PNG"><br></p> 
 <p align="center"><br></p> 
 <p><strong>many-to-many(seq2seq) with different length</strong> is wildly known as <strong>Encoder-Decoder System</strong>. It has many application in machine translation due to its flexibility. We will further talk about this in the application post.</p> 
 <p></p> 
 <p align="center"><br><img src="https://images2.imgbox.com/b7/e1/t1DrAYZ4_o.png" width="400" height="250" alt="seq2seq.PNG"><br></p> 
 <p align="center"><br></p> 
 <h3 id="basic-rnn-limitation-and-variants">Basic RNN limitation and variants</h3> 
 <p>Now we see what basic RNN can do. Dose it has any limitation? And how do we get around with it?</p> 
 <h4 id="time-order---bidirection-rnn">1. Time order - Bidirection RNN</h4> 
 <p>We have been emphasizing that RNN has to compute in order of time in propagation, meaning the <span class="math inline">\(\hat{y}^{&lt;t&gt;}\)</span> is only based on the information prior from <span class="math inline">\(x^{&lt;1&gt;}\)</span> to <span class="math inline">\(x^{&lt;t&gt;}\)</span>. However there are situations that we need to consider both prior and post information, for example in translation we need to consider the entire context. Like following:<br></p> 
 <p align="center"><br><img src="https://images2.imgbox.com/43/32/hCjo9evd_o.png" width="300" height="400" alt="bi-RNN.png"><br></p> 
 <p align="center"><br></p> 
 <h4 id="long-term-dependence">2. long-term dependence</h4> 
 <p>We have mentioned the gradient vanishing/exploding problem before for deep neural network. Same idea also applies here. In Deep Learning book, author gives an intuitive explanation.</p> 
 <p>If we consider a simplified RNN without input <span class="math inline">\(x\)</span> and activation function. Then RNN becomes a time-series model with matrix implementation as following:<br><span class="math display">\[ \begin{align} a^{&lt;t&gt;} &amp; = ({W_{aa}}^t)^T \cdot a^{&lt;0&gt;} \\ where \ \ \ \ W_{aa} &amp;= Q \Lambda Q^T \\ Then \ \ \ \ a^{&lt;t&gt;} &amp; = Q {\Lambda}^t Q^T\cdot a^{&lt;0&gt;} \end{align} \]</span><br> The problem lies in Eigenvalue <span class="math inline">\({\Lambda}^t\)</span>, where <span class="math inline">\(\lambda &gt;1\)</span> may lead to gradient exploding, while <span class="math inline">\(\lambda &lt;1\)</span> may lead to gradient vanishing.</p> 
 <p>Gradient exploding can be solved by gradient clipping. Basically we cap the gradient for certain threshold. For gradient vanishing, one popular solution is gated RNN. Gated model has additional parameter to control at each timestamp whether to remember or forget the pass information, in order to maintain useful information for longer time period.</p> 
 <h5 id="gated-rnn-1---gru-unit">Gated RNN 1 - GRU Unit</h5> 
 <p>Based on basic RNN, GRU add more computation within in each hidden neuron to control information flow. We use <span class="math inline">\(c^{&lt;t&gt;}\)</span> to denote hidden layer output. And within each hidden layer, we do following:<br><span class="math display">\[ \begin{align} \Gamma_u &amp; =\sigma(w_{u}[c^{&lt;t-1&gt;},x^{&lt;t&gt;}]+ b_{u})\\ \Gamma_r &amp; =\sigma(w_{r}[c^{&lt;t-1&gt;},x^{&lt;t&gt;}]+ b_{r})\\ \tilde{c}^{&lt;t&gt;} &amp;= tanh( w_{a}[ \Gamma_r \odot c^{&lt;t-1&gt;},x^{&lt;t&gt;}]+ b_{a} )\\ c^{&lt;t&gt;} &amp;= \Gamma_u \odot \tilde{c}^{&lt;t&gt;} + (1-\Gamma_u) \odot \tilde{c}^{&lt;t-1&gt;} \end{align} \]</span></p> 
 <p>Where <span class="math inline">\(\Gamma_u\)</span> is the update Gate, and <span class="math inline">\(\Gamma_r\)</span> is the reset gate. They both measure the relevance between the previous hidden layer and the present input. Please note here <span class="math inline">\(\Gamma_r\)</span> and <span class="math inline">\(\Gamma_u\)</span> has same dimension as <span class="math inline">\(c^{&lt;t&gt;}\)</span>, and element-wise multiplication is applied.</p> 
 <p>If the relevance is low than update gate <span class="math inline">\(\Gamma_u\)</span> will choose to forget history information,and reset gate <span class="math inline">\(\Gamma_r\)</span> will also allow less history information into the current calculation. If <span class="math inline">\(\Gamma_r=1\)</span> and <span class="math inline">\(\Gamma_u=0\)</span> then we get basic RNN.</p> 
 <p>And sometimes only <span class="math inline">\(\Gamma_u\)</span> is used, leading to a simplified GRU with less parameter to train.</p> 
 <h5 id="gated-rnn-2---lstm-unit">Gated RNN 2 - LSTM Unit</h5> 
 <p>Another solution to long-term dependence is LSTM unit. It has 3 gates to train.<br><span class="math display">\[ \begin{align} \Gamma_u &amp; =\sigma(w_{u}[c^{&lt;t-1&gt;},x^{&lt;t&gt;}]+ b_{u})\\ \Gamma_f &amp; =\sigma(w_{f}[c^{&lt;t-1&gt;},x^{&lt;t&gt;}]+ b_{f})\\ \Gamma_o &amp; =\sigma(w_{o}[c^{&lt;t-1&gt;},x^{&lt;t&gt;}]+ b_{o})\\ \tilde{c}^{&lt;t&gt;} &amp;= tanh( w_{a}[ c^{&lt;t-1&gt;},x^{&lt;t&gt;}]+ b_{a} )\\ c^{&lt;t&gt;} &amp;= \Gamma_u \odot \tilde{c}^{&lt;t&gt;} + \Gamma_f \odot \tilde{c}^{&lt;t-1&gt;}\\ a^{&lt;t&gt;} &amp;= \Gamma_o \odot tanh(c^{&lt;t&gt;} ) \end{align} \]</span></p> 
 <p>Where <span class="math inline">\(\Gamma_u\)</span> is the update Gate, and <span class="math inline">\(\Gamma_f\)</span> is the forget gate, and <span class="math inline">\(\Gamma_o\)</span> is the output gate.</p> 
 <p>Beside GRU and LSTM, there are also other variants. It is hard to say which is the best. If your data set is small, maybe you should try simplified RNN, instead of LSTM, which has less parameter to train.</p> 
 <p>So far we have reviewed most of the basic knowledge in Neural Network. Let's have some fun in the next post. The next post for this series will be some cool applications in CNN and RNN.</p> 
 <p>To be continued.</p> 
 <hr> 
 <p>Reference</p> 
 <ol><li>Ian Goodfellow, Yoshua Bengio, Aaron Conrville, "Deep Learning"</li><li>Bishop, "Pattern Recognition and Machine Learning",springer, 2006</li><li>T. Hastie, R. Tibshirani and J. Friedman. “Elements of Statistical Learning”, Springer, 2009.</li></ol> 
</div> 
<p>转载于:https://www.cnblogs.com/gogoSandy/p/8845785.html</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/18ba20f038948c383e6b85e06cf84bf7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">npm: command not found的解决方案</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/771264fd35a78caa686cd9832497d427/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">常用的网站链接</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>