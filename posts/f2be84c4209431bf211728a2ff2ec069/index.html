<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>基于神经网络的2D摄像头的手势识别系统实现（一） - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="基于神经网络的2D摄像头的手势识别系统实现（一）" />
<meta property="og:description" content="一、手势识别的分类 若按照摄像头的种类（2D摄像头、深度摄像头）来分，可分为两类，1）基于2D摄像头的二维手势识别 和 2）基于3D摄像头（如微软的kinnect）三维手势识别。早期的手势识别识别是基于二维彩色图像的识别技术，所谓的二维彩色图像是指通过普通摄像头拍出场景后，得到二维的静态图像，然后再通过计算机图形算法进行图像中内容的识别。二维的手型识别的只能识别出几个静态的手势动作，而且这些动作必须要提前进行预设好。相比较二维手势识别，三维手势识别增加了一个Z轴的信息，它可以识别各种手型、手势和动作。三维手势识别也是现在手势识别发展的主要方向。不过这种包含一定深度信息的手势识别，需要特别的硬件来实现。常见的有通过传感器和光学摄像头来完成。　手势识别中最关键的包括对手势动作的跟踪以及后续的计算机数据处理。关于手势动作捕捉主要是通过光学和传感器两种方式来实现。手势识别推测的算法，包括模板匹配技术（二维手势识别技术使用的）、通过统计样本特征以及深度学习神经网络技术。本文着重于2D摄像头的手势识别。
二、基于2D摄像头的手势识别 1. 2D摄像头的手势识别分类 2D摄像头的手势识别分类又分为：1）静态手势识别（手型识别）也称静态二维手势识别，识别的是手势中最简单的一类。只能识别出几个静态的手势动作，比如握拳或者五指张开，这种技术只能识别手势的“状态”，而不能感知手势的“持续变化”。说到底是一种模式匹配技术，通过计算机视觉算法分析图像，和预设的图像模式进行比对，从而理解这种手势的含义。因此，二维手型识别技术只可以识别预设好的状态，拓展性差，控制感很弱，用户只能实现最基础的人机交互功能。其代表公司是被Google收购的Flutter。使用他家的软件之后，用户可以用几个手型来控制播放器。2）动态手势识别，仍不含深度信息，停留在二维的层面上。这种技术比起二维手型识别来说稍复杂一些，不仅可以识别手型，还可以识别一些简单的二维手势动作，比如对着摄像头挥挥手。二维手势识别拥有了动态的特征，可以追踪手势的运动，进而识别将手势和手部运动结合在一起的复杂动作。这种技术虽然在硬件要求上和二维手型识别并无区别，但是得益于更加先进的计算机视觉算法，可以获得更加丰富的人机交互内容。在使用体验上也提高了一个档次，从纯粹的状态控制，变成了比较丰富的平面控制。其代表公司是来自以色列的PointGrab，EyeSight和ExtremeReality。
2. 2D摄像头的手势识别的原理 手势有三个主要特征：手型，方向，运动轨迹，一个基于视觉手势识别系统的构成应包括：图像的采集，预处理，特征提取和选择，分类器的设计，以及手势识别。其流程大致如下： 其中有三个步骤是识别系统的关键，分别是预处理时手势的分割，特征提取和选择，手势跟踪，以及手势识别算法。不管是手势检测，或者手势跟踪，识别，特征提取和选择是关键:手势本身具有丰富的形变，运动以及纹理特征，选取合理的特征对于手势的识别至关重要。目前 常用的手势特征有:轮廓、边缘、图像矩、图像特征向量以及区域直方图特征等等。其中，手势检测（手势分割），主要受复杂背景，遮挡，直接光源的亮度变化，外部反射等，常见手部检测特征选取有基于肤色，基于表观，基于模型三种手部检测。手势跟踪，主要受快速运动，双手遮挡，非刚体，非线性，非高斯，多模态。通俗的讲，就是 手势非刚体运动，受到缩放，形变，缺少，模糊，旋转，亮度，视角等因素影响，建议使用基于肤色的SIFT&#43;肤色（ROI）&#43;HOG,粒子滤波，MeanShift,基于Sift特征，基于EKF,基于SVM，基于模板匹配等。手势识别，受尺度，角度，光照，同一手势每次演示的差异。目前基于2D视觉的静态手势识别技术主要有三大类，第一类为模板匹配技术，这是一种最简单的识别技术；第二类为统计分析技术，这是一种通过统计样本特征向量来确定分类器的基于概率统计理论的分类方法；第三类为神经网络技术，这种技术具有自组织和自学习能力，具有分布性特点，能有效的抗噪声和处理不完整模式以及具有模式推广能力。基于2D视觉的动态手势识别技术，主要有基于神经网络，基于HMMs,CRFs等。
3.基于神经网络2D摄像头静态手势识别实现 本文主要实现基于2D摄像头的静态手势识别系统，由于是静态所以不用考虑手势追踪，重点关注手势分割、手势识别两部分。其中手势分割采用局部自适应阈值的图像二值化和高斯肤色模型算法提取手掌轮廓；手势识别采用CNN网络进行分类。 3.1 手势分割算法实现 局部自适应阈值的图像二值化
def binaryMask(self, frame, x0, y0, width, height): # print(&#39;use binaryMask model ...&#39;) minValue = 70 # 创建矩形框 cv2.rectangle(frame, (x0, y0), (x0 &#43; width, y0 &#43; height), (0, 255, 0), 1) roi = frame[y0:y0 &#43; height, x0:x0 &#43; width] # 获取灰度图像 gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY) # 高斯模糊:高斯滤波器中像素的权重与其距中心像素的距离成比例 blur = cv2.GaussianBlur(gray, (5, 5), 2) # 图像的二值化提取目标,动态自适应的调整属于自己像素点的阈值，而不是整幅图像都用一个阈值 th3 = cv2." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/f2be84c4209431bf211728a2ff2ec069/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2017-12-21T00:09:34+08:00" />
<meta property="article:modified_time" content="2017-12-21T00:09:34+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">基于神经网络的2D摄像头的手势识别系统实现（一）</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2 id="一手势识别的分类">一、手势识别的分类</h2> 
<p>若按照摄像头的种类（2D摄像头、深度摄像头）来分，可分为两类，1）基于2D摄像头的二维手势识别 和 2）基于3D摄像头（如微软的kinnect）三维手势识别。早期的手势识别识别是基于二维彩色图像的识别技术，所谓的二维彩色图像是指通过普通摄像头拍出场景后，得到二维的静态图像，然后再通过计算机图形算法进行图像中内容的识别。二维的手型识别的只能识别出几个静态的手势动作，而且这些动作必须要提前进行预设好。相比较二维手势识别，三维手势识别增加了一个Z轴的信息，它可以识别各种手型、手势和动作。三维手势识别也是现在手势识别发展的主要方向。不过这种包含一定深度信息的手势识别，需要特别的硬件来实现。常见的有通过传感器和光学摄像头来完成。　手势识别中最关键的包括对手势动作的跟踪以及后续的计算机数据处理。关于手势动作捕捉主要是通过光学和传感器两种方式来实现。手势识别推测的算法，包括模板匹配技术（二维手势识别技术使用的）、通过统计样本特征以及深度学习神经网络技术。本文着重于2D摄像头的手势识别。</p> 
<h2 id="二基于2d摄像头的手势识别">二、基于2D摄像头的手势识别</h2> 
<h3 id="1-2d摄像头的手势识别分类">1. 2D摄像头的手势识别分类</h3> 
<p>2D摄像头的手势识别分类又分为：1）静态手势识别（手型识别）也称静态二维手势识别，识别的是手势中最简单的一类。只能识别出几个静态的手势动作，比如握拳或者五指张开，这种技术只能识别手势的“状态”，而不能感知手势的“持续变化”。说到底是一种模式匹配技术，通过计算机视觉算法分析图像，和预设的图像模式进行比对，从而理解这种手势的含义。因此，二维手型识别技术只可以识别预设好的状态，拓展性差，控制感很弱，用户只能实现最基础的人机交互功能。其代表公司是被Google收购的Flutter。使用他家的软件之后，用户可以用几个手型来控制播放器。2）动态手势识别，仍不含深度信息，停留在二维的层面上。这种技术比起二维手型识别来说稍复杂一些，不仅可以识别手型，还可以识别一些简单的二维手势动作，比如对着摄像头挥挥手。二维手势识别拥有了动态的特征，可以追踪手势的运动，进而识别将手势和手部运动结合在一起的复杂动作。这种技术虽然在硬件要求上和二维手型识别并无区别，但是得益于更加先进的计算机视觉算法，可以获得更加丰富的人机交互内容。在使用体验上也提高了一个档次，从纯粹的状态控制，变成了比较丰富的平面控制。其代表公司是来自以色列的PointGrab，EyeSight和ExtremeReality。</p> 
<h3 id="2-2d摄像头的手势识别的原理">2. 2D摄像头的手势识别的原理</h3> 
<p>手势有三个主要特征：手型，方向，运动轨迹，一个基于视觉手势识别系统的构成应包括：图像的采集，预处理，特征提取和选择，分类器的设计，以及手势识别。其流程大致如下： <br> <img src="https://images2.imgbox.com/29/13/ey2YQaRO_o.png" alt="这里写图片描述" title=""> <br> 其中有三个步骤是识别系统的关键，分别是<strong>预处理时手势的分割</strong>，<strong>特征提取和选择</strong>，<strong>手势跟踪</strong>，以及<strong>手势识别算法</strong>。不管是手势检测，或者手势跟踪，识别，特征提取和选择是关键:手势本身具有丰富的形变，运动以及纹理特征，选取合理的特征对于手势的识别至关重要。目前 常用的手势特征有:<strong>轮廓、边缘、图像矩、图像特征向量以及区域直方图特征</strong>等等。其中，手势检测（手势分割），主要受复杂背景，遮挡，直接光源的亮度变化，外部反射等，常见手部检测特征选取有基于<strong>肤色</strong>，基于<strong>表观</strong>，基于<strong>模型</strong>三种手部检测。手势跟踪，主要受快速运动，双手遮挡，非刚体，非线性，非高斯，多模态。通俗的讲，就是 手势非刚体运动，受到缩放，形变，缺少，模糊，旋转，亮度，视角等因素影响，建议使用<strong>基于肤色的SIFT+肤色（ROI）+HOG</strong>,<strong>粒子滤波</strong>，<strong>MeanShift</strong>,<strong>基于Sift特征</strong>，<strong>基于EKF,基于SVM，基于模板匹配</strong>等。手势识别，受尺度，角度，光照，同一手势每次演示的差异。目前基于2D视觉的<strong>静态手势识别</strong>技术主要有三大类，第一类为<strong>模板匹配</strong>技术，这是一种最简单的识别技术；第二类为<strong>统计分析</strong>技术，这是一种通过统计样本特征向量来确定分类器的基于概率统计理论的分类方法；第三类为<strong>神经网络</strong>技术，这种技术具有自组织和自学习能力，具有分布性特点，能有效的抗噪声和处理不完整模式以及具有模式推广能力。基于2D视觉的<strong>动态手势识别</strong>技术，主要有基于神经网络，基于HMMs,CRFs等。</p> 
<h3 id="3基于神经网络2d摄像头静态手势识别实现">3.基于神经网络2D摄像头静态手势识别实现</h3> 
<p>本文主要实现基于2D摄像头的静态手势识别系统，由于是静态所以不用考虑手势追踪，重点关注<strong>手势分割</strong>、<strong>手势识别</strong>两部分。其中手势分割采用<strong>局部自适应阈值的图像二值化</strong>和<strong>高斯肤色模型</strong>算法提取手掌轮廓；手势识别采用<strong>CNN网络</strong>进行分类。 <br> </p> 
<h4 id="31-手势分割算法实现">3.1 手势分割算法实现 </h4> 
<p></p> 
<p><strong>局部自适应阈值的图像二值化</strong></p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">binaryMask</span><span class="hljs-params">(self, frame, x0, y0, width, height)</span>:</span>
    <span class="hljs-comment"># print('use binaryMask model ...')</span>
    minValue = <span class="hljs-number">70</span>
    <span class="hljs-comment"># 创建矩形框</span>
    cv2.rectangle(frame, (x0, y0), (x0 + width, y0 + height), (<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>), <span class="hljs-number">1</span>)
    roi = frame[y0:y0 + height, x0:x0 + width]
    <span class="hljs-comment"># 获取灰度图像</span>
    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
    <span class="hljs-comment"># 高斯模糊:高斯滤波器中像素的权重与其距中心像素的距离成比例</span>
    blur = cv2.GaussianBlur(gray, (<span class="hljs-number">5</span>, <span class="hljs-number">5</span>), <span class="hljs-number">2</span>)

    <span class="hljs-comment"># 图像的二值化提取目标,动态自适应的调整属于自己像素点的阈值，而不是整幅图像都用一个阈值</span>
    th3 = cv2.adaptiveThreshold(blur, <span class="hljs-number">255</span>, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, <span class="hljs-number">11</span>, <span class="hljs-number">2</span>)
    ret, res = cv2.threshold(th3, minValue, <span class="hljs-number">255</span>, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    <span class="hljs-keyword">return</span> res</code></pre> 
<p><strong>高斯肤色模型</strong></p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">skinMask</span><span class="hljs-params">(self, frame, x0, y0, width, height)</span>:</span>
    print(<span class="hljs-string">'use skin model ...'</span>)
    <span class="hljs-comment"># HSV values</span>
    low_range = np.array([<span class="hljs-number">0</span>, <span class="hljs-number">50</span>, <span class="hljs-number">80</span>])
    upper_range = np.array([<span class="hljs-number">30</span>, <span class="hljs-number">200</span>, <span class="hljs-number">255</span>])

    cv2.rectangle(frame, (x0, y0), (x0 + width, y0 + height), (<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>), <span class="hljs-number">1</span>)
    roi = frame[y0:y0 + height, x0:x0 + width]

    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)

    <span class="hljs-comment"># 设阈值，去除背景部分,低于这个lower_red的值，图像值变为0,高于这个upper_red的值，图像值变为0</span>
    mask = cv2.inRange(hsv, low_range, upper_range)

    <span class="hljs-comment">#腐蚀操作，减少整幅图像的白色区域</span>
    mask = cv2.erode(mask, skinkernel, iterations=<span class="hljs-number">1</span>)
    <span class="hljs-comment">#膨胀操作，增加图像中的白色区域</span>
    mask = cv2.dilate(mask, skinkernel, iterations=<span class="hljs-number">1</span>)

    <span class="hljs-comment"># 用高斯分布权值矩阵与原始图像矩阵做卷积运算</span>
    mask = cv2.GaussianBlur(mask, (<span class="hljs-number">15</span>, <span class="hljs-number">15</span>), <span class="hljs-number">1</span>)
    <span class="hljs-comment"># cv2.imshow("Blur", mask)</span>

    <span class="hljs-comment"># 图像与运算</span>
    res = cv2.bitwise_and(roi, roi, mask=mask)
    <span class="hljs-comment"># color to grayscale</span>
    res = cv2.cvtColor(res, cv2.COLOR_BGR2GRAY)

    <span class="hljs-keyword">return</span> res</code></pre> 
<p></p> 
<h4 id="32-手势识别cnn神经网络实现">3.2 手势识别CNN神经网络实现</h4> 
<p></p> 
<p>将上面图像分割出来的手势图像进行CNN训练，得到比较稳定的网络，然后输入新的图像进行分类输出即可。基于keras的CNN网络模型搭建如训练流程如下：</p> 
<p><strong>网络搭建实现</strong></p> 
<pre class="prettyprint"><code class=" hljs oxygene">    def createCNNModel(<span class="hljs-keyword">self</span>):
        <span class="hljs-keyword">self</span>.model = Sequential()

        <span class="hljs-keyword">self</span>.model.<span class="hljs-keyword">add</span>(Conv2D(nb_filters, (nb_conv, nb_conv),
                              padding=<span class="hljs-string">'valid'</span>,
                              # input_shape=( img_rows, img_cols,img_channels)))
                              input_shape=(img_channels, img_rows, img_cols)))  # theano
        convout1 = Activation(<span class="hljs-string">'relu'</span>)
        <span class="hljs-keyword">self</span>.model.<span class="hljs-keyword">add</span>(convout1)
        <span class="hljs-keyword">self</span>.model.<span class="hljs-keyword">add</span>(Conv2D(nb_filters, (nb_conv, nb_conv)))
        convout2 = Activation(<span class="hljs-string">'relu'</span>)
        <span class="hljs-keyword">self</span>.model.<span class="hljs-keyword">add</span>(convout2)
        <span class="hljs-keyword">self</span>.model.<span class="hljs-keyword">add</span>(MaxPooling2D(pool_size=(nb_pool, nb_pool)))
        <span class="hljs-keyword">self</span>.model.<span class="hljs-keyword">add</span>(Dropout(<span class="hljs-number">0.5</span>))

        <span class="hljs-keyword">self</span>.model.<span class="hljs-keyword">add</span>(Flatten())
        <span class="hljs-keyword">self</span>.model.<span class="hljs-keyword">add</span>(Dense(<span class="hljs-number">128</span>))
        <span class="hljs-keyword">self</span>.model.<span class="hljs-keyword">add</span>(Activation(<span class="hljs-string">'relu'</span>))
        <span class="hljs-keyword">self</span>.model.<span class="hljs-keyword">add</span>(Dropout(<span class="hljs-number">0.5</span>))
        <span class="hljs-string">#11</span>个手势输出,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>,<span class="hljs-number">10</span>
        <span class="hljs-keyword">self</span>.model.<span class="hljs-keyword">add</span>(Dense(nb_classes))
        <span class="hljs-keyword">self</span>.model.<span class="hljs-keyword">add</span>(Activation(<span class="hljs-string">'softmax'</span>))

        # sgd = SGD(lr=<span class="hljs-number">0.01</span>, decay=<span class="hljs-number">1</span>e-<span class="hljs-number">6</span>, momentum=<span class="hljs-number">0.9</span>, nesterov=<span class="hljs-keyword">True</span>)
        <span class="hljs-keyword">self</span>.model.compile(loss=<span class="hljs-string">'categorical_crossentropy'</span>, optimizer=<span class="hljs-string">'adadelta'</span>, metrics=[<span class="hljs-string">'accuracy'</span>])

        # Model summary
        <span class="hljs-keyword">self</span>.model.summary()
        # Model conig details
        <span class="hljs-keyword">self</span>.model.get_config()

        <span class="hljs-keyword">from</span> keras.utils import plot_model
        plot_model(<span class="hljs-keyword">self</span>.model, to_file=<span class="hljs-string">'my_model.png'</span>, show_shapes=<span class="hljs-keyword">True</span>)
        return <span class="hljs-keyword">self</span>.model</code></pre> 
<p><strong>训练流程实现</strong></p> 
<pre class="prettyprint"><code class=" hljs python">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">trainModel</span><span class="hljs-params">(self, train_set_path, weight_name)</span>:</span>
        self.model = self.createCNNModel()
        imlist = []
        self.getImgListPath(train_set_path, imlist)

        image1 = np.array(Image.open(imlist[<span class="hljs-number">0</span>]))
        m, n = image1.shape[<span class="hljs-number">0</span>:<span class="hljs-number">2</span>]
        total_images = len(imlist)

        img_ndarry_list = []

        label_list = np.ones((total_images,), dtype=int)

        <span class="hljs-keyword">for</span> index, img_file <span class="hljs-keyword">in</span> enumerate(imlist):
            single_img_label =label_dict[os.path.basename(img_file).split(<span class="hljs-string">'_'</span>)[<span class="hljs-number">0</span>]]

            single_img_array = np.array(Image.open(img_file).convert(<span class="hljs-string">'L'</span>)).flatten()
            img_ndarry_list.insert(index, single_img_array)
            label_list[index] = single_img_label

        img_matrix = np.array(img_ndarry_list, dtype=<span class="hljs-string">'f'</span>)
        data, label = shuffle(img_matrix, label_list, random_state=<span class="hljs-number">2</span>)
        train_data = [data, label]

        (X, y) = (train_data[<span class="hljs-number">0</span>], train_data[<span class="hljs-number">1</span>])

        <span class="hljs-comment"># Split X and y into training and testing sets</span>
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-number">0.2</span>, random_state=<span class="hljs-number">4</span>)

        X_train = X_train.reshape(X_train.shape[<span class="hljs-number">0</span>], img_channels, img_rows, img_cols)
        X_test = X_test.reshape(X_test.shape[<span class="hljs-number">0</span>], img_channels, img_rows, img_cols)

        X_train = X_train.astype(<span class="hljs-string">'float32'</span>)
        X_test = X_test.astype(<span class="hljs-string">'float32'</span>)

        <span class="hljs-comment"># normalize</span>
        X_train /= <span class="hljs-number">255</span>
        X_test /= <span class="hljs-number">255</span>

        <span class="hljs-comment"># convert integers to dummy variables (one hot encoding)</span>
        Y_train = np_utils.to_categorical(y_train, nb_classes)
        Y_test = np_utils.to_categorical(y_test, nb_classes)
        <span class="hljs-comment"># print ( X_train, X_test, Y_train, Y_test)</span>
        <span class="hljs-comment"># print (img_matrix)</span>

        hist = self.model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch,
                              verbose=<span class="hljs-number">1</span>, validation_split=<span class="hljs-number">0.2</span>)

        <span class="hljs-comment"># 保存模型的权重</span>
        self.model.save(weight_name)
        print(<span class="hljs-string">'train model success!'</span>)</code></pre> 
<p></p> 
<h2 id="4效果展示">4.效果展示</h2> 
<p></p> 
<p>为了方便演示，添加了pyqt4的UI界面，能够方便实现性能和测试和调优，整体效果入下： <br> <img src="https://images2.imgbox.com/16/1e/3mUKrelx_o.png" alt="这里写图片描述" title=""> <br> 上面这个部分可以用来测试本地数据集的识别率，以下这个部分实现对视频手势的实时识别，效果如下： <br> <img src="https://images2.imgbox.com/69/58/xvWTWiBw_o.png" alt="这里写图片描述" title=""> <br> 这个UI界面可以实现本地数据集的录制，手掌分割方法选择，样本的训练，还有直接导入训练好的本地网络等功能。 <br> <a href="http://download.csdn.net/download/xiaopangzi313/10166930">1.代码路径的链接</a> <br> <a href="https://pan.baidu.com/s/1i50997v" rel="nofollow">2.本地训练好的网络链接</a> <br> 参考文献： <br> 1.Gesture recognition via CNN neural network implemented in Keras + Tensorflow + OpenCV</p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6ecc01d2899ac99fae303cfd620dd8e8/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">wxWidgets学习 - 双缓冲技术解决屏幕闪烁问题</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/bfbf2a4309cd1e9d1b172d929f900c0a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">CSDN现在绑定手机还送下载积分吗</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>