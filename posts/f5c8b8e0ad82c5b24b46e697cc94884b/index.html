<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【深度学习-目标检测】01 - R-CNN 论文学习与总结 - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【深度学习-目标检测】01 - R-CNN 论文学习与总结" />
<meta property="og:description" content="论文地址：Rich feature hierarchies for accurate object detection and semantic segmentation
论文学习 摘要（Abstract） 对象检测性能的现状： 在PASCAL VOC数据集上测量的对象检测性能在过去几年已经达到了一个高点。最佳性能的方法通常是复杂的集成系统，结合了多种低级图像特征和高级上下文。 R-CNN方法的提出： 作者提出了一种简单且可扩展的检测算法，该算法在VOC 2012上的平均精度均值（mAP）比之前的最佳结果提高了30%以上，达到了53.3%。这种方法结合了两个关键见解： 可以将高容量的卷积神经网络（CNN）应用于自下而上的区域提议，以定位和分割对象 （将CNN应用到目标检测领域）。当标记训练数据稀缺时，针对辅助任务的监督预训练，随后进行特定领域的微调，可以显著提升性能。（应用预训练模型&#43;微调 可以显著提高性能） R-CNN的命名与特点： 由于这种方法结合了区域提议（Region Proposal）和CNN，因此被称为R-CNN（带有CNN特征的区域）。论文还展示了一些实验，提供了对网络学习内容的洞察，揭示了一个丰富的图像特征层次结构。 1. 引言（Introduction） 在这部分作者强调了特征在视觉识别任务中的重要性，提出了以CNN为特征提取网络的想法。
特征的重要性 过去十年，视觉识别任务的进展很大程度上依赖于 SIFT（尺度不变特征变换）以及 HOG（方向梯度直方图）等特征的使用，尽管这些特征在多个视觉任务中表现出色，但是在PASCAL VOC这一经典的视觉识别任务中，2010-2012年的进展相对来说比较缓慢。SIFT（尺度不变特征变换）与HOG（方向梯度直方图）与视觉通路中初级皮层区域V1的复杂细胞相关联，但是实际上识别过程发生在多个阶段，通过这两种方式获得的特征还不够拥有足够的信息量。
（过去目标检测用的SIFT以及HOG，发展缓慢，且效果不好）CNN在2012年AlexNet网络中发挥了非常大的作用，展示出了CNN在图像分类上任务的优秀性能，是提取特征的优秀的方式，在提高目标检测性能方面具有很大的潜力。（CNN提取特征效果号，CNN有希望取代这两种方式以获取更有信息量的特征） CNN在对象检测中的应用与关联 CNN的兴起，2012年因为AlexNet的提出，并且在ImageNet竞赛中，展现出了CNN在图像分类上的强大能力，标志CNN在视觉识别领域的重要进展。在ILSVRC 2012研讨会上，展开了激烈的讨论，特别是关于CNN在图像分类上的成果能在多大程度上推广到PASCAL VOC挑战上的对象检测中。作者通过将图像分类和对象检测之间的差距桥接起来，明确的回答了这个问题，在这篇论文中展示了CNN可以显著提高对象检测性能的研究，从而证明了CNN在对象检测任务中的有效性和潜力。 2. 对象检测与R-CNN R-CNN方法概述 R-CNN方法的核心： R-CNN 方法结合了 区域提议（Regin Proposals) 和 卷积神经网络（CNN）的特征。这种方法通过在测试的时候生成大约2000个类别独立的区域提议，然后使用CNN从每个区域提议中提取固定长度的特征向量，并且使用线性SVM（支持向量机）对每个区域进行分类。 R-CNN的设计和效率： R-CNN 使用一种简单的技术（仿射图形变形）从每个提议区域中计算固定大小的CNN输入，不论区域的形状如何。（使用这种技术统一了不同大小尺寸的区域提议的特征向量大小）系统的设计使得类别特定的计算仅包括相对较小的矩阵-向量乘积和贪婪的非最大抑制，这是由于特征在所有类别中共享，并且维度比以前使用的区域特征低两个数量级。 R-CNN的训练挑战： 在对象检测中面临的一个挑战是标记数据稀缺，现有的数据量不足以训练大型CNN模型。作者提出的解决方案是：先在大型辅助数据集上进行监督预训练，然后在特定领域的小数据集上进行微调。（使用预训练&#43;微调可以改善数据稀缺的问题，并且能够提高性能） R-CNN的性能提升： 在PASCAL VOC 2010 数据集上，经过微调后R-CNN系统在平均精度上达到 54%，相比于之前基于HOG的模型 33% ，有显著提升。（R-CNN 比 HOG等模型更优秀） R-CNN的应用拓展： 由于R-CNN在区域上操作，自然可以将其扩展到语义分割任务。经过轻微修改后，R-CNN在PASCAL VOC分割任务上也取得了最先进的结果。（R-CNN也能在分割任务获得不错的表现）
区域提议与特征提取 区域提议（Region Proposals）: R-CNN 首先生成大约2000个类别独立的区域提议，这些提议定义了检测器可用的候选检测集。为了生成这些区域提议，R-CNN 使用了多种方法，主要方式是 选择性搜索（Selective Search）方法。 特征提取（Feature Extraction）: 对于每一个区域提议，R-CNN 使用一个大型卷积神经网络（CNN）来提取固定长度的特征向量。这些特征向量随后被用于后续的分类过程。 特征向量的计算： R-CNN 通过一种简单的技术（仿射图像变形）从每个区域提议中计算固定大小的CNN输入，无论区域的形状如何。这种方法允许从各种形状和大小的区域中提取一致的特征表示。 测试时检测 在测试的时候，会使用选择性搜索（Selective Search）方法在测试图像中提取大约2000个区域提议，这些区域提议定义了检测器可用的候选检测集。对于每个区域提议，通过CNN前向传播来提取特征向量，然后对于每个类别，使用该类别训练的SVM对提取的特征向量进行评分。在得到所有的区域提议的评分后，R-CNN会对每个类别独立的应用非极大抑制（NMS），通过这个步骤排除拒绝那些与得分更高的选定区域具有较高交并比（IOU）重叠的区域。 运行时分析 检测效率的关键属性： 首先，所有CNN参数在所有类别之间共享。CNN计算的特征向量维度较低。 计算时间的分配： 生成区域提议和计算特征的时间在所有类别中被摊销（在GPU上为每张图像13秒，在CPU上为53秒）。类别特定的计算仅包括特征与SVM权重直接按的点积运算和非极大抑制（NMS）。 R-CNN的可拓展性： R-CNN 可以拓展到数千个对象类别，无需使用近似技术（如哈希）即使有10万类别，相关的矩阵乘法在CPU上只需要10秒。 3." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/f5c8b8e0ad82c5b24b46e697cc94884b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-25T17:48:29+08:00" />
<meta property="article:modified_time" content="2023-12-25T17:48:29+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【深度学习-目标检测】01 - R-CNN 论文学习与总结</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>论文地址：<a href="https://openaccess.thecvf.com/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf" rel="nofollow">Rich feature hierarchies for accurate object detection and semantic segmentation</a></p> 
<h2><a id="_1"></a>论文学习</h2> 
<h3><a id="Abstract_2"></a>摘要（Abstract）</h3> 
<ol><li>对象检测性能的现状： 
  <ul><li>在PASCAL VOC数据集上测量的对象检测性能在过去几年已经达到了一个高点。</li><li>最佳性能的方法通常是<strong>复杂的集成系统，结合了多种低级图像特征和高级上下文。</strong></li></ul> </li><li>R-CNN方法的提出： 
  <ul><li>作者提出了一种<strong>简单且可扩展的检测算法</strong>，该算法在VOC 2012上的平均精度均值（mAP）比之前的最佳结果<strong>提高了30%以上，达到了53.3%</strong>。</li><li>这种方法结合了两个关键见解： 
    <ul><li>可以将高容量的卷积神经网络（CNN）应用于自下而上的区域提议，以定位和分割对象 <strong>（将CNN应用到目标检测领域）</strong>。</li><li>当标记训练数据稀缺时，针对辅助任务的监督预训练，随后进行特定领域的微调，可以显著提升性能。<strong>（应用预训练模型+微调 可以显著提高性能）</strong></li></ul> </li></ul> </li><li>R-CNN的命名与特点： 
  <ul><li>由于这种方法结合了<strong>区域提议（Region Proposal）<strong>和</strong>CNN</strong>，因此被称为R-CNN（带有CNN特征的区域）。</li><li>论文还展示了一些实验，提供了对网络学习内容的洞察，揭示了一个丰富的图像特征层次结构。</li></ul> </li></ol> 
<h3><a id="1_Introduction_14"></a>1. 引言（Introduction）</h3> 
<p>在这部分作者强调了特征在视觉识别任务中的重要性，提出了以CNN为特征提取网络的想法。</p> 
<h4><a id="_16"></a>特征的重要性</h4> 
<ol><li>过去十年，视觉识别任务的进展很大程度上依赖于 <strong>SIFT</strong>（尺度不变特征变换）以及 <strong>HOG</strong>（方向梯度直方图）等特征的使用，尽管这些特征在多个视觉任务中表现出色，但是在PASCAL VOC这一经典的视觉识别任务中，<strong>2010-2012年的进展相对来说比较缓慢</strong>。</li><li><strong>SIFT</strong>（尺度不变特征变换）与<strong>HOG</strong>（方向梯度直方图）与视觉通路中初级皮层区域V1的复杂细胞相关联，但是实际上识别过程发生在多个阶段，<strong>通过这两种方式获得的特征还不够拥有足够的信息量。</strong><br> <strong>（过去目标检测用的SIFT以及HOG，发展缓慢，且效果不好）</strong></li><li>CNN在2012年AlexNet网络中发挥了非常大的作用，展示出了<strong>CNN在图像分类上任务的优秀性能，是提取特征的优秀的方式，在提高目标检测性能方面具有很大的潜力</strong>。<strong>（CNN提取特征效果号，CNN有希望取代这两种方式以获取更有信息量的特征）</strong></li></ol> 
<h4><a id="CNN_21"></a>CNN在对象检测中的应用与关联</h4> 
<ol><li>CNN的兴起，2012年因为AlexNet的提出，并且在ImageNet竞赛中，展现出了CNN在图像分类上的强大能力，标志CNN在视觉识别领域的重要进展。</li><li>在ILSVRC 2012研讨会上，展开了激烈的讨论，特别是<strong>关于CNN在图像分类上的成果能在多大程度上推广到PASCAL VOC挑战上的对象检测中</strong>。</li><li>作者通过将图像分类和对象检测之间的差距桥接起来，明确的回答了这个问题，在这篇论文中展示了<strong>CNN可以显著提高对象检测性能的研究，从而证明了CNN在对象检测任务中的有效性和潜力</strong>。</li></ol> 
<h3><a id="2_RCNN_26"></a>2. 对象检测与R-CNN</h3> 
<h4><a id="RCNN_27"></a>R-CNN方法概述</h4> 
<ol><li>R-CNN方法的核心： 
  <ul><li>R-CNN 方法结合了 <strong>区域提议</strong>（Regin Proposals) 和 <strong>卷积神经网络</strong>（CNN）的特征。</li><li>这种方法通过在测试的时候<strong>生成大约2000个类别独立的区域提议</strong>，然后使用CNN从每个区域提议中提取固定长度的特征向量，并且<strong>使用线性SVM（支持向量机）对每个区域进行分类</strong>。</li></ul> </li><li>R-CNN的设计和效率： 
  <ul><li>R-CNN 使用一种简单的技术（<strong>仿射图形变形</strong>）从每个提议区域中计算固定大小的CNN输入，不论区域的形状如何。（<strong>使用这种技术统一了不同大小尺寸的区域提议的特征向量大小</strong>）</li><li>系统的设计使得类别特定的计算仅包括相对较小的矩阵-向量乘积和贪婪的非最大抑制，这是由于特征在所有类别中共享，并且维度比以前使用的区域特征低两个数量级。</li></ul> </li><li>R-CNN的训练挑战： 
  <ul><li>在对象检测中面临的一个挑战是标记数据稀缺，现有的数据量不足以训练大型CNN模型。</li><li>作者提出的解决方案是：先在大型辅助数据集上进行监督预训练，然后在特定领域的小数据集上进行微调。<strong>（使用预训练+微调可以改善数据稀缺的问题，并且能够提高性能）</strong></li></ul> </li><li>R-CNN的性能提升： 
  <ul><li>在PASCAL VOC 2010 数据集上，经过微调后R-CNN系统在平均精度上达到 54%，相比于之前基于HOG的模型 33% ，有显著提升。（<strong>R-CNN 比 HOG等模型更优秀</strong>）</li></ul> </li><li>R-CNN的应用拓展： 
  <ul><li>由于R-CNN在区域上操作，自然可以将其扩展到语义分割任务。经过轻微修改后，R-CNN在PASCAL VOC分割任务上也取得了最先进的结果。（<strong>R-CNN也能在分割任务获得不错的表现</strong>）<br> <img src="https://images2.imgbox.com/a9/0b/40RI1vht_o.png" alt="在这里插入图片描述"></li></ul> </li></ol> 
<h4><a id="_45"></a>区域提议与特征提取</h4> 
<ol><li>区域提议（Region Proposals）: 
  <ul><li>R-CNN <strong>首先生成大约2000个类别独立的区域提议</strong>，这些提议定义了检测器可用的候选检测集。</li><li>为了生成这些区域提议，R-CNN 使用了多种方法，主要方式是 <strong>选择性搜索</strong>（Selective Search）方法。</li></ul> </li><li>特征提取（Feature Extraction）: 
  <ul><li>对于每一个区域提议，<strong>R-CNN 使用一个大型卷积神经网络（CNN）来提取固定长度的特征向量</strong>。</li><li>这些特征向量随后被用于后续的分类过程。</li></ul> </li><li>特征向量的计算： 
  <ul><li>R-CNN 通过一种简单的技术（<strong>仿射图像变形</strong>）从每个区域提议中计算固定大小的CNN输入，无论区域的形状如何。</li><li>这种方法允许从各种形状和大小的区域中提取一致的特征表示。</li></ul> </li></ol> 
<h4><a id="_55"></a>测试时检测</h4> 
<ul><li>在测试的时候，会使用<strong>选择性搜索</strong>（Selective Search）方法在测试图像中提取大约2000个区域提议，这些区域提议定义了检测器可用的候选检测集。</li><li>对于每个区域提议，通过CNN前向传播来提取特征向量，然后<strong>对于每个类别，使用该类别训练的SVM对提取的特征向量进行评分。</strong></li><li>在得到所有的区域提议的评分后，<strong>R-CNN会对每个类别独立的应用非极大抑制（NMS），通过这个步骤排除拒绝那些与得分更高的选定区域具有较高交并比（IOU）重叠的区域。</strong></li></ul> 
<h4><a id="_59"></a>运行时分析</h4> 
<ol><li>检测效率的关键属性： 
  <ul><li>首先，<strong>所有CNN参数在所有类别之间共享</strong>。</li><li>CNN计算的特征向量维度较低。</li></ul> </li><li>计算时间的分配： 
  <ul><li>生成区域提议和计算特征的时间在所有类别中被摊销（在GPU上为每张图像13秒，在CPU上为53秒）。</li><li><strong>类别特定的计算仅包括特征与SVM权重直接按的点积运算和非极大抑制（NMS</strong>）。</li></ul> </li><li>R-CNN的可拓展性： 
  <ul><li>R-CNN 可以拓展到数千个对象类别，无需使用近似技术（如哈希）</li><li>即使有10万类别，相关的矩阵乘法在CPU上只需要10秒。</li></ul> </li></ol> 
<h3><a id="3_RCNN_69"></a>3. 训练R-CNN</h3> 
<h4><a id="_70"></a>监督预训练</h4> 
<ol><li>预训练的目的和过程： 
  <ul><li>R-CNN在大型辅助数据集（如ILSVRC 2012）上进行了监督预训练，该数据集包含图像级注释（即没有边界框标签）。</li><li>预训练使用开源的Caffe CNN库进行，<strong>旨在训练CNN以识别大量不同的图像类别</strong>。</li></ul> </li><li>预训练的结果： 
  <ul><li>经过预训练的CNN在ILSVRC 2012验证集上的表现接近Krizhevsky等人的结果，顶级错误率仅高出2.2个百分点。</li><li>这种差异主要是由于训练过程中的简化所导致。</li></ul> </li><li>预训练的重要性： 
  <ul><li>监督预训练是训练大型CNN的有效方法，特别是在标记数据稀缺的情况下。</li><li>通过在大型数据集上进行预训练，CNN能够学习到丰富的特征表示，这对后续的特定任务（如对象检测）至关重要。</li></ul> </li></ol> 
<h4><a id="_80"></a>领域特定微调</h4> 
<ol><li>微调的目的： 
  <ul><li>为了适应新任务（对象检测）和新领域（变形的VOC窗口），作者继续使用随机梯度下降（SGD）训练CNN参数，但仅使用来自VOC的变形区域提议。</li></ul> </li><li>CNN架构的调整： 
  <ul><li>除了将CNN的ImageNet特定的1000类分类层替换为随机初始化的21类分类层（20个VOC类别加上背景）外，CNN架构保持不变。</li></ul> </li><li>微调过程： 
  <ul><li>微调开始时，学习率设为0.001（初始预训练率的1/10），这允许微调取得进展，同时不破坏预训练的初始化。</li><li>在每次SGD迭代中，均匀采样32个正样本窗口（跨所有类别）和96个背景窗口，构成大小为128的小批量（mini-batch）。</li><li>采样过程偏向于正样本窗口，因为与背景相比，它们极为罕见。</li></ul> </li><li>正负样本的定义： 
  <ul><li>所有与地面真实框有大于或等于0.5的交并比（IoU）重叠的区域提议被视为该框类别的正样本，其余的被视为负样本。</li></ul> </li></ol> 
<h4><a id="_91"></a>对象类别分类器</h4> 
<ol><li>分类器的训练： 
  <ul><li><strong>对于每个对象类别，训练一个二元分类器</strong>（如用于检测汽车的分类器）。</li><li>明确区分正样本（紧密包围对象的图像区域）和负样本（与对象无关的背景区域）。</li></ul> </li><li>处理部分重叠的区域： 
  <ul><li>对于部分与对象重叠的区域，使用交并比（IoU）阈值来定义是否将其视为负样本。</li><li>通过在验证集上的网格搜索，选择了0.3作为IoU阈值。</li></ul> </li><li>正样本的定义： 
  <ul><li>正样本简单地定义为每个类别的地面真实边界框。</li></ul> </li><li>线性SVM的优化： 
  <ul><li>提取特征并应用训练标签后，为每个类别优化一个线性支持向量机（SVM）。</li><li>由于训练数据太大，无法全部装入内存，因此采用标准的硬负采样（hard negative mining）方法。</li></ul> </li><li>硬负采样的收敛： 
  <ul><li>硬负采样迅速收敛，在实践中，平均精度均值（mAP）在单次遍历所有图像后停止增长。</li></ul> </li></ol> 
<h3><a id="4_RCNNPASCAL_VOC_105"></a>4. R-CNN在PASCAL VOC上的结果</h3> 
<ol><li>R-CNN的表现： 
  <ul><li>R-CNN在VOC 2010测试集上的平均精度均值（mAP）达到了50.2%。当使用边界框回归（Bounding Box Regression, BB）后，mAP提升到53.7%。</li></ul> </li><li>与其他方法的比较： 
  <ul><li>R-CNN的表现与几个强基线方法进行了比较，包括SegDPM、UV A系统、Regionlets等。</li><li>相比于这些方法，R-CNN在mAP上取得了显著提升，同时运行速度更快。</li></ul> </li><li>R-CNN的优势： 
  <ul><li>R-CNN相比于使用多特征、非线性核SVM方法的UV A系统，实现了从35.1%到53.7%的大幅度mAP提升。</li><li>R-CNN在VOC 2011/12测试集上也取得了类似的表现（53.3% mAP）。</li></ul> </li><li>方法的验证： 
  <ul><li>所有设计决策和超参数在VOC 2007数据集上进行了验证。</li><li>在VOC 2010-12数据集上的最终结果是在VOC 2012训练集上微调CNN，并在VOC 2012训练验证集上优化检测SVM。</li></ul> </li></ol> 
<h3><a id="5__118"></a>5. 可视化、消融研究与错误模式</h3> 
<ol><li>可视化学习特征： 
  <ul><li>通过可视化CNN的不同层，研究者们能够理解网络学习到的特征。例如，第一层的过滤器捕捉到了定向边缘和对立颜色。</li><li>对于更深层次的网络，使用非参数方法直接展示网络学习到的内容。这包括选择特定的单元（特征），并将其作为一个独立的对象检测器来使用。</li></ul> </li><li>消融研究： 
  <ul><li>通过逐层分析CNN的最后三层（pool5、fc6、fc7），研究者们评估了每层对检测性能的贡献。</li><li>发现去除fc7层后，性能并没有显著下降，表明CNN的卷积层比全连接层对其表示能力贡献更大。</li></ul> </li><li>错误模式分析： 
  <ul><li>通过分析检测失败的模式，研究者们能够改进R-CNN方法。例如，引入简单的边界框回归方法显著减少了定位错误，这是主要的错误模式之一。</li></ul> </li><li>不同层性能的比较： 
  <ul><li>没有在PASCAL VOC上微调的CNN（仅在ILSVRC 2012上预训练）的分析显示，从fc6层提取的特征比fc7层的特征更具泛化性。</li><li>即使去除了fc6和fc7层，仅使用pool5层的特征也能取得不错的结果，这表明CNN的大部分表示能力来自其卷积层。</li></ul> </li></ol> 
<h3><a id="6__131"></a>6. 边界框回归</h3> 
<ol><li>减少定位错误的方法： 
  <ul><li>作者实现了<strong>一种简单的边界框回归方法来减少定位错误</strong>。这种方法受到DPM中使用的边界框回归的启发。</li><li>通过训练一个线性回归模型来预测新的检测窗口，基于选择性搜索区域提议的pool5特征。</li></ul> </li><li>边界框回归的实现细节： 
  <ul><li>边界框回归的完整细节在补充材料中给出，论文中并未提到。</li><li>这种方法通过调整检测窗口的位置和大小，以更准确地覆盖真实的对象。</li></ul> </li><li>边界框回归对性能的提升： 
  <ul><li>使用边界框回归后，平均精度均值（mAP）提高了3到4个百分点。</li><li>这表明边界框回归有效地修正了许多由于定位不准确而导致的错误检测。</li></ul> </li><li>错误分析： 
  <ul><li>通过错误分析，作者发现R-CNN相比于其他方法（如DPM），更多的错误是由于定位不准确，而不是与背景或其他对象类别的混淆。</li><li>这表明CNN特征比HOG特征更具辨别力，但由于使用自下而上的区域提议和CNN在整图分类中学习到的位置不变性，导致了较宽松的定位。</li></ul> </li></ol> 
<h3><a id="7_144"></a>7.语义分割</h3> 
<ol><li>语义分割挑战： 
  <ul><li>为了直接与当前领先的语义分割系统（称为O2P，即“二阶池化”）进行比较，作者在O2P的开源框架内工作。</li><li>O2P使用CPMC（Constrained Parametric Min-Cuts）生成每张图像的150个区域提议，然后使用 支持向量回归（SVR）预测每个区域对每个类别的质量。</li></ul> </li><li>CNN特征用于分割： 
  <ul><li>作者评估了三种计算CPMC区域上特征的策略，所有策略都从将矩形窗口变形为227x227开始。</li><li>第一种策略（full）忽略区域的形状，直接在变形窗口上计算CNN特征。</li><li>第二种策略（fg）仅在区域的前景掩码上计算CNN特征。</li><li>第三种策略（full+fg）简单地将full和fg特征连接起来。</li></ul> </li><li>VOC 2011上的结果： 
  <ul><li>在VOC 2011验证集上，full+fg策略取得了平均准确率47.9%的最佳结果，比O2P的结果略高。<br> fg策略略优于full策略，表明掩蔽区域形状提供了更强的信号。</li></ul> </li><li>VOC 2011测试集上的结果： 
  <ul><li>在VOC 2011测试集上，与两个强基线方法（R&amp;P和O2P）相比，R-CNN的full+fg策略在11个类别中取得了最高的分割准确率，总体分割准确率为47.9%。</li></ul> </li></ol> 
<h3><a id="8__159"></a>8. 结论</h3> 
<ol><li> <p>对象检测性能的提升：</p> 
  <ul><li>近年来，对象检测性能有所停滞。最佳的系统是复杂的集成，结合了多种低级图像特征和高级上下文。</li><li>本文提出了一个简单且可扩展的对象检测算法，在PASCAL VOC 2012上相比以前的最佳结果实现了30%的相对提升。</li></ul> </li><li> <p>关键洞察：</p> 
  <ul><li>第一个关键洞察是将高容量的卷积神经网络应用于自下而上的区域提议，以定位和分割对象。</li><li>第二个关键洞察是一种训练大型CNN的范式，即在标记数据稀缺时，先对辅助任务进行监督预训练，然后针对目标任务进行微调。</li></ul> </li><li> <p>预训练和微调的有效性：</p> 
  <ul><li>作者推测，对于许多数据稀缺的视觉问题，“监督预训练/领域特定微调”范式将非常有效。</li></ul> </li><li> <p>计算机视觉与深度学习的结合：</p> 
  <ul><li>作者指出，通过结合计算机视觉的经典工具（如自下而上的区域提议）和深度学习（如卷积神经网络），取得了这些成果。</li><li>这两个领域不是对立的科学探索线路，而是自然且不可避免的合作伙伴。</li></ul> </li></ol> 
<p>这篇论文《Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation》的主要创新点和贡献可以总结如下：</p> 
<ol><li><strong>R-CNN方法的提出</strong>： 
  <ul><li>论文提出了区域卷积神经网络（R-CNN）方法，这是一种结合了区域提议和卷积神经网络（CNN）的对象检测方法。这种方法在对象检测领域是创新性的，因为它有效地将深度学习技术应用于对象检测任务。</li></ul> </li><li><strong>高容量CNN的应用</strong>： 
  <ul><li>R-CNN利用高容量的CNN来从自下而上的区域提议中提取特征，用于定位和分割对象。这种方法的创新之处在于它如何有效地利用深度学习来提取和利用复杂的图像特征。</li></ul> </li><li><strong>监督预训练和领域特定微调</strong>： 
  <ul><li>论文展示了在标记数据稀缺时，先对辅助任务（如图像分类）进行监督预训练，然后针对目标任务（如对象检测）进行微调的有效性。这种方法提高了CNN在特定任务上的性能，是深度学习在数据稀缺领域的一个重要应用。</li></ul> </li><li><strong>显著提高检测性能</strong>： 
  <ul><li>R-CNN在PASCAL VOC数据集上实现了显著的性能提升，相比之前的最佳方法在平均精度均值（mAP）上提高了30%以上。这一成果证明了R-CNN方法的有效性和实用性。</li></ul> </li><li><strong>可视化和错误模式分析</strong>： 
  <ul><li>论文还包括了对CNN学习特征的可视化分析，以及对R-CNN错误模式的深入分析。这些分析有助于理解CNN是如何工作的，以及如何进一步改进对象检测方法。</li></ul> </li><li><strong>语义分割的应用</strong>： 
  <ul><li>论文还探讨了将R-CNN应用于语义分割任务的可能性，并展示了在这一任务上的初步结果。这表明R-CNN方法不仅限于对象检测，还可以扩展到其他视觉识别任务。</li></ul> </li></ol> 
<h2><a id="RCNN_188"></a>R-CNN</h2> 
<p><img src="https://images2.imgbox.com/47/de/cRK8LOUW_o.png" alt="在这里插入图片描述"></p> 
<p>R-CNN 整体工作流程详解：</p> 
<ol><li> <p>使用<strong>选择性搜索</strong>（Selective Search）来选取大约2000个候选区域，也成为区域提议，简单来说就是按照某个特定的算法（选择性搜索），在原始图像中画出大约2000个框，接下来的工作全部围绕这2000个框来开展。</p> </li><li> <p>选择一个<strong>主干卷积神经网络（VGG、AlexNet等等）来进行特征提取</strong>，这些主干卷积神经网络在输出层之前被截断，因为我们需要的是它们的提取的特征图。</p> </li><li> <p>以上2000个框，就是区域提议将会经过一定的方法（论文中是<strong>仿射图形变形</strong>），将大小不一，形状不一的各个区域提议变形为CNN特征提取网络输入的形状，这样，对于大约2000个框，都能得到它的特征图，<strong>这里值得一提的是：R-CNN是对每个区域提议单独进行CNN特征提取，虽然是共享CNN特征提取网络，但是需要进行大约2000次特征提取，速度是比较慢的。</strong></p> </li><li> <p>经过对之前所有区域提议的处理，现在我们获得了大约2000个特征图，每个特征图都需要经过 分类类别数 个 SVM 分类器（假设有1000个类别，就有1000个SVM分类器），每个分类器负责判断一个区域提议是否包含其对应的对象类别，输出通常是一个评分，实数值，代表该区域提议属于该SVM代表分类的可能性或者置信度，最后，假设有1000个类别，每个区域提议将经过1000个SVM分类器处理，获得1000个置信分数，通常选取最大的置信分数，且需要超过一定的阈值。</p> </li><li> <p>同时我们利用获得的每个区域提议的特征图，进行预测框回归，也就是对区域提议进行微调。</p> </li><li> <p>至此，对于每一个区域提议，我们可以获得两个结果：</p> 
  <ul><li><strong>该区域对每个类别的置信分数。</strong></li><li><strong>该区域微调后的结果。</strong></li></ul> </li><li> <p>对于每个对象类别，R-CNN使用<strong>非最大抑制</strong>（NMS）来合并重叠的检测结果。NMS排除那些与得分更高的检测重叠度较高的检测，从而减少重复检测。</p> 
  <ul><li><strong>非极大抑制（NMS）具体过程</strong>： 
    <ol><li><strong>排序</strong>：<br> 所有的区域提议（或检测框）根据它们的置信度评分进行排序。置信度评分通常由分类器（如SVM）提供，表示该区域提议中包含特定对象的可能性。</li><li><strong>选择最高评分的检测框</strong>： 
      <ul><li>从排序后的列表中选择最高评分的检测框作为参考框。</li></ul> </li><li><strong>计算重叠度</strong>： 
      <ul><li>计算这个参考框与列表中其他所有检测框的交并比（Intersection over Union, IoU）。IoU是两个框的交集区域与它们的并集区域的比例。</li></ul> </li><li><strong>去除重叠度高的检测框</strong>： 
      <ul><li>如果任何其他检测框与参考框的IoU超过预设的阈值（例如0.3或0.5），则将其从列表中去除。这一步骤旨在去除与参考框重叠度高的、可能是重复检测的框。</li></ul> </li><li><strong>重复过程</strong>： 
      <ul><li>从剩余的检测框列表中再次选择最高评分的框作为新的参考框，并重复上述过程，直到所有的检测框都被考虑过。</li></ul> </li><li><strong>输出最终检测结果</strong>： 
      <ul><li>经过NMS处理后，剩余的检测框被认为是最终的检测结果。这些框既不会有过多的重叠，也是置信度评分最高的。</li></ul> </li></ol> </li></ul> </li><li> <p>输出最终检测结果：<br> 经过上述步骤后，R-CNN输出每个检测到的对象的类别、置信度（由SVM得分决定）和边界框位置。</p> </li></ol> 
<p><strong>以上内容旨在记录自己的学习过程以及复习，如有错误，欢迎批评指正，谢谢阅读。</strong></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7720b96b7f941a2f8074f58910962721/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">相比国科云解析，传统DNS解析技术有哪些局限性？</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ff58d9ff0db638811de7483ee9fc53ad/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">[PyTorch][chapter 8][李宏毅深度学习][DNN 训练技巧]</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>