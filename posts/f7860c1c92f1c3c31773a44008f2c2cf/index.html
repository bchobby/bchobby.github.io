<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>03.shard_allocation_和_cluster的routing设置 - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="03.shard_allocation_和_cluster的routing设置" />
<meta property="og:description" content="文章目录 1.简述2. cluster 级别的shard allocation 相关的设置1. shard allocation 相关设置1. cluster.routing.allocation.enable2. cluster.routing.allocation.node_concurrent_incoming_recoveries3. cluster.routing.allocation.node_concurrent_outgoing_recoveries4. cluster.routing.allocation.node_concurrent_recoveries5. cluster.routing.allocation.node_initial_primaries_recoveries6. cluster.routing.allocation.same_shard.host 2. shard rebalance相关设置1. cluster.routing.rebalance.enable2. cluster.routing.allocation.allow_rebalance3. cluster.routing.allocation.cluster_concurrent_rebalance 3. shard balancing 的因子设置1. cluster.routing.allocation.balance.shard2. cluster.routing.allocation.balance.index3. cluster.routing.allocation.balance.threshold 4. allocation和rebalance的区别和联系 3. 基于磁盘的shard allocation限制1. cluster.routing.allocation.disk.threshold_enabled2. cluster.routing.allocation.disk.watermark.low3. cluster.routing.allocation.disk.watermark.high4. cluster.routing.allocation.disk.watermark.flood_stage5. cluster.info.update.interval6. cluster.routing.allocation.disk.include_relocations7. 一个使用样例 4. 通过属性配置设置,达到allocation 分配时对node的感知1. 开启集群allocation 感知2. 强制感知是什么呢 5. cluster级别的shard allocation filter 设置1. include2. require3. exclude4. 也可以使用正则来进行配置 1.简述 这里主要是学习master对shard的管理，master决定了一个shard需要被分配到哪个node上面，以及什么时候在cluster中的node之间移动shard来reblace整个cluster
2. cluster 级别的shard allocation 相关的设置 shard alloction 是在某个node上创建某个shard的过程。这个过程会发生在initial recovery, replica allocation, rebalancing, 或者node add或remove的时候" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/f7860c1c92f1c3c31773a44008f2c2cf/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-10-26T10:29:22+08:00" />
<meta property="article:modified_time" content="2020-10-26T10:29:22+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">03.shard_allocation_和_cluster的routing设置</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><ul><li><a href="#1_1" rel="nofollow">1.简述</a></li><li><a href="#2_cluster_shard_allocation__4" rel="nofollow">2. cluster 级别的shard allocation 相关的设置</a></li><li><ul><li><a href="#1_shard_allocation__8" rel="nofollow">1. shard allocation 相关设置</a></li><li><ul><li><a href="#1_clusterroutingallocationenable_10" rel="nofollow">1. cluster.routing.allocation.enable</a></li><li><a href="#2_clusterroutingallocationnode_concurrent_incoming_recoveries_23" rel="nofollow">2. cluster.routing.allocation.node_concurrent_incoming_recoveries</a></li><li><a href="#3_clusterroutingallocationnode_concurrent_outgoing_recoveries_30" rel="nofollow">3. cluster.routing.allocation.node_concurrent_outgoing_recoveries</a></li><li><a href="#4_clusterroutingallocationnode_concurrent_recoveries_37" rel="nofollow">4. cluster.routing.allocation.node_concurrent_recoveries</a></li><li><a href="#5_clusterroutingallocationnode_initial_primaries_recoveries_41" rel="nofollow">5. cluster.routing.allocation.node_initial_primaries_recoveries</a></li><li><a href="#6_clusterroutingallocationsame_shardhost_44" rel="nofollow">6. cluster.routing.allocation.same_shard.host</a></li></ul> 
    </li><li><a href="#2_shard_rebalance_49" rel="nofollow">2. shard rebalance相关设置</a></li><li><ul><li><a href="#1_clusterroutingrebalanceenable_52" rel="nofollow">1. cluster.routing.rebalance.enable</a></li><li><a href="#2_clusterroutingallocationallow_rebalance_62" rel="nofollow">2. cluster.routing.allocation.allow_rebalance</a></li><li><a href="#3_clusterroutingallocationcluster_concurrent_rebalance_69" rel="nofollow">3. cluster.routing.allocation.cluster_concurrent_rebalance</a></li></ul> 
    </li><li><a href="#3_shard_balancing__74" rel="nofollow">3. shard balancing 的因子设置</a></li><li><ul><li><a href="#1_clusterroutingallocationbalanceshard_78" rel="nofollow">1. cluster.routing.allocation.balance.shard</a></li><li><a href="#2_clusterroutingallocationbalanceindex_83" rel="nofollow">2. cluster.routing.allocation.balance.index</a></li><li><a href="#3_clusterroutingallocationbalancethreshold_87" rel="nofollow">3. cluster.routing.allocation.balance.threshold</a></li></ul> 
    </li><li><a href="#4_allocationrebalance_92" rel="nofollow">4. allocation和rebalance的区别和联系</a></li></ul> 
   </li><li><a href="#3_shard_allocation_105" rel="nofollow">3. 基于磁盘的shard allocation限制</a></li><li><ul><li><a href="#1_clusterroutingallocationdiskthreshold_enabled_109" rel="nofollow">1. cluster.routing.allocation.disk.threshold_enabled</a></li><li><a href="#2_clusterroutingallocationdiskwatermarklow_113" rel="nofollow">2. cluster.routing.allocation.disk.watermark.low</a></li><li><a href="#3_clusterroutingallocationdiskwatermarkhigh_120" rel="nofollow">3. cluster.routing.allocation.disk.watermark.high</a></li><li><a href="#4_clusterroutingallocationdiskwatermarkflood_stage_127" rel="nofollow">4. cluster.routing.allocation.disk.watermark.flood_stage</a></li><li><a href="#5_clusterinfoupdateinterval_144" rel="nofollow">5. cluster.info.update.interval</a></li><li><a href="#6_clusterroutingallocationdiskinclude_relocations_148" rel="nofollow">6. cluster.routing.allocation.disk.include_relocations</a></li><li><a href="#7__156" rel="nofollow">7. 一个使用样例</a></li></ul> 
   </li><li><a href="#4_allocation_node_181" rel="nofollow">4. 通过属性配置设置,达到allocation 分配时对node的感知</a></li><li><ul><li><a href="#1_allocation__186" rel="nofollow">1. 开启集群allocation 感知</a></li><li><a href="#2__214" rel="nofollow">2. 强制感知是什么呢</a></li></ul> 
   </li><li><a href="#5_clustershard_allocation_filter__232" rel="nofollow">5. cluster级别的shard allocation filter 设置</a></li><li><ul><li><a href="#1_include_246" rel="nofollow">1. include</a></li><li><a href="#2_require_253" rel="nofollow">2. require</a></li><li><a href="#3_exclude_258" rel="nofollow">3. exclude</a></li><li><a href="#4__264" rel="nofollow">4. 也可以使用正则来进行配置</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h3><a id="1_1"></a>1.简述</h3> 
<p>这里主要是学习master对shard的管理，master决定了一个shard需要被分配到哪个node上面，以及什么时候在cluster中的node之间移动shard来reblace整个cluster</p> 
<h3><a id="2_cluster_shard_allocation__4"></a>2. cluster 级别的shard allocation 相关的设置</h3> 
<p>shard alloction 是在某个node上创建某个shard的过程。这个过程会发生在initial recovery, replica allocation, rebalancing, 或者node add或remove的时候</p> 
<h4><a id="1_shard_allocation__8"></a>1. shard allocation 相关设置</h4> 
<p>这个是</p> 
<h5><a id="1_clusterroutingallocationenable_10"></a>1. cluster.routing.allocation.enable</h5> 
<p>开启或者关闭某种类型的shard的allocation</p> 
<ol><li>all : (default) 允许所有类型的shard被allocate</li><li>primaries : 只允许primaries被allocated</li><li>new_primaries: 只允许primaries被allocated</li><li>none : 不孕育任何shard被allocated</li></ol> 
<p>这个设置不会影响一个node重启的时候对local primary的recovery, 如果一个被重启的node有一份nassigned primary shard 的copy,那么这个shard会立即成为 primary shard， 当然，这个shard的allocation id要和cluster state中记录的active allocation ids一致。</p> 
<p>什么是allocation id 参看<a href="https://www.elastic.co/cn/blog/tracking-in-sync-shard-copies" rel="nofollow">这里</a></p> 
<h5><a id="2_clusterroutingallocationnode_concurrent_incoming_recoveries_23"></a>2. cluster.routing.allocation.node_concurrent_incoming_recoveries</h5> 
<p>这个参数设置了每个节点可以有多少个shard可以接收从外面进来的recovery用的数据。</p> 
<ol><li>一般情况下都是node上的shard都是replica shard，这些shard 接收来自primary shard的数据进行恢复</li><li>如果是relocation操作那么这个node上对应的shard也有可能是primary shard<br> 默认值是2</li></ol> 
<h5><a id="3_clusterroutingallocationnode_concurrent_outgoing_recoveries_30"></a>3. cluster.routing.allocation.node_concurrent_outgoing_recoveries</h5> 
<p>这个和上一个参数正好是相对的，控制了每个node上可以有多少个shard在向外提供shard recovery的数据。</p> 
<ol><li>一般情况下都是node上的shard都是primary shard，这些shard 向replica shard传输数据进行shard恢复</li><li>如果是relocation操作那么这个node上对应的shard也有可能是replica shard</li></ol> 
<p>默认值是2</p> 
<h5><a id="4_clusterroutingallocationnode_concurrent_recoveries_37"></a>4. cluster.routing.allocation.node_concurrent_recoveries</h5> 
<p>这个参数是上面两个参数的综合体，也就是会把上面两个参数设置为一样的<br> cluster.routing.allocation.node_concurrent_incoming_recoveries and cluster.routing.allocation.node_concurrent_outgoing_recoveries.</p> 
<h5><a id="5_clusterroutingallocationnode_initial_primaries_recoveries_41"></a>5. cluster.routing.allocation.node_initial_primaries_recoveries</h5> 
<p>replica shard的恢复一般是通过network从primary恢复，但是unassigned primary shard的恢复则只能是通过原来有这个shard的node被重新启动了来进行恢复。这个应该稍微大一些，以便于更多的unassigned primary shard可以更快的被恢复。</p> 
<h5><a id="6_clusterroutingallocationsame_shardhost_44"></a>6. cluster.routing.allocation.same_shard.host</h5> 
<p>开启一个检查来防止同一个shard的多个instances在同一个host上面，这个是为了让es能够更好的应对es的node挂掉的情况,这种情况一般都是在一个主机上启动了多个node,这样的话这个node挂掉后es的某个shard的数据可能就丢了，一般情况下，同一个集群的的多个node不会在同一个node上面，但是需要注意有时候我们使用的是虚拟机，虚拟机层面不在同一个服务器上，但是实际多个虚拟机上可能在同一个物理机上，这种情况也是应该尽量避免的。否则就会造成数据的丢失情况。<br> 这个值某人是false，也就是不会开启检查</p> 
<h4><a id="2_shard_rebalance_49"></a>2. shard rebalance相关设置</h4> 
<p>下面这些动态设置是用来设置集群层面的shards的rebalance的</p> 
<h5><a id="1_clusterroutingrebalanceenable_52"></a>1. cluster.routing.rebalance.enable</h5> 
<p>开启或者关闭某种shard的rebalance</p> 
<ol><li>all - (default) Allows shard balancing for all kinds of shards.</li><li>primaries - Allows shard balancing only for primary shards.</li><li>replicas - Allows shard balancing only for replica shards.</li><li>none - No shard balancing of any kind are allowed for any indices.</li></ol> 
<h5><a id="2_clusterroutingallocationallow_rebalance_62"></a>2. cluster.routing.allocation.allow_rebalance</h5> 
<p>什么时候允许rebalance操作开始</p> 
<ol><li>always - 任何时候都可以</li><li>indices_primaries_active - 只有当集群中多有的primary shards都被allocated之后才允许</li><li>indices_all_active - (default) 只有当集群中的所有的shard(primaries and replicas) 都被allocated之后才允许rebalance操作</li></ol> 
<h5><a id="3_clusterroutingallocationcluster_concurrent_rebalance_69"></a>3. cluster.routing.allocation.cluster_concurrent_rebalance</h5> 
<p>这个设置控制了集群rebalance的并行度，默认值是2。<br> 需要注意的是这个只能控制因为imbalances 导致的shard的迁移的并行度，并不能限制因为allocation filtering 或者 forced awareness导致的分片的转移。</p> 
<h4><a id="3_shard_balancing__74"></a>3. shard balancing 的因子设置</h4> 
<p>The following settings are used together to determine where to place each shard. The cluster is balanced when no allowed rebalancing operation can bring the weight of any node closer to the weight of any other node by more than the balance.threshold.<br> 以下3个因子共同决定了在哪个node上面放置shard,当任何一个rebalance操作都不能使集群中的node之间的weight差距减小的话，集群就达到了balanced的状态。</p> 
<h5><a id="1_clusterroutingallocationbalanceshard_78"></a>1. cluster.routing.allocation.balance.shard</h5> 
<p>设置了每个node上的shard总数在集群balance中占据的权重因子，默认是0.45f,增加这个值的话就意味着集群的balance更倾向于使每个node上面的shard数量都保持一致。<br> Defines the weight factor for the total number of shards allocated on a node (float). Defaults to 0.45f. Raising this raises the tendency to equalize the number of shards across all nodes in the cluster.</p> 
<h5><a id="2_clusterroutingallocationbalanceindex_83"></a>2. cluster.routing.allocation.balance.index</h5> 
<p>设置了每个index在某个node上的shards数量的权重，默认是0.55f,增大这个值的话意味着集群的balance更倾向于让index的shards平均的分配到cluster的每个node上面。<br> Defines the weight factor for the number of shards per index allocated on a specific node (float). Defaults to 0.55f. Raising this raises the tendency to equalize the number of shards per index across all nodes in the cluster.</p> 
<h5><a id="3_clusterroutingallocationbalancethreshold_87"></a>3. cluster.routing.allocation.balance.threshold</h5> 
<p>shard rebalance的触发阈值，默认是1.0f,增加这个值意味着cluster对集群的balance要求更低，也就是说更不容易触发rebalance。</p> 
<p>Minimal optimization value of operations that should be performed (non negative float). Defaults to 1.0f. Raising this will cause the cluster to be less aggressive about optimizing the shard balance.</p> 
<h4><a id="4_allocationrebalance_92"></a>4. allocation和rebalance的区别和联系</h4> 
<p>这里主要想再强调一下allocation和rebalance的关系，主要从下面两个配置来进行解析</p> 
<ol><li>cluster.routing.allocation.enable</li><li>cluster.routing.rebalance.enable</li></ol> 
<p>对于allocation强调的是shard的分配，不管你这个shard是因为什么原因要进行分配，比如某个node突然挂掉需要重新分配一些unassigned的shard, 手动的relocation的话需要在目标node上allocation新的shard, rebalance的话也需要在目标node上allocation新的shard。<br> 比如说可能某个node突然挂掉了（而且挂掉的node上的数据被清理掉了），导致了某些shard是unassigned的，这个时候如果 <code>cluster.routing.allocation.enable:none</code>那么即使<code>cluster.routing.rebalance.enable:all</code>,这些unassigned的shard也不会被分配到其他节点，因为最根本的shard分配操作被禁止了。</p> 
<p>假如这个时候设置为<br> <code>cluster.routing.rebalance.enable: none</code><br> <code>cluster.routing.allocation.enable: all</code><br> 那么对应的unassigned的shard会被分配到其他几点上面。在分配完成集群编程green的时候重启挂掉的node(该node上面没有数据)，那么该node上面的shard数量会一直是0，因为rebalance被关闭了。当重新设置<code>cluster.routing.rebalance.enable: all</code>的时候，才会将部分shard迁移到新启动的node上面。<br> 综上，rebalance的功能需要依赖allocation功能的开启，allocation没有开启的话是没有办法进行rebalance操作的（手动的relocation理所当然也没有办法进行），当然allocation还会限制shard丢失之后的shard重新分配。</p> 
<h3><a id="3_shard_allocation_105"></a>3. 基于磁盘的shard allocation限制</h3> 
<p>es会考虑一个node现有的磁盘容量来决定是否将一个新的shard分配到这个node上面，或者是否有必要激活relocation操作从这个node上面迁移走一些shard.<br> 下面这些磁盘相关的设置都是动态的，可以通过elasticsearch.yml设置，也可以通过api来进行设置。</p> 
<h4><a id="1_clusterroutingallocationdiskthreshold_enabled_109"></a>1. cluster.routing.allocation.disk.threshold_enabled</h4> 
<p>默认是true,如果设置为false的时候在进行shard allocation的时候就不会考虑磁盘的因素。<br> Defaults to true. Set to false to disable the disk allocation decider.</p> 
<h4><a id="2_clusterroutingallocationdiskwatermarklow_113"></a>2. cluster.routing.allocation.disk.watermark.low</h4> 
<p>低风险水位设置，这个设置的默认值是85%，意味着当一个node的磁盘使用率达到了85%，那么就不会再往这个node上面分配shard了。这个设置对于新创建的index的primary shard不起作用，但是会对replica shard起作用。<br> 这个值也可以直接设置为一个绝对值，比如500mb,这个500mb是指剩余的使用空间哈，不是指已经使用了的空间。 这种在集群磁盘比较大的时候比较有用，比如每个node的数量是3T，操作系统实际需要的可能也就50G，但是按照百分比算的话，1% 也有300G，相对来说会有一些浪费。这个时候我们就可以直接设置50G就完事儿了。</p> 
<p>Controls the low watermark for disk usage. It defaults to 85%, meaning that Elasticsearch will not allocate shards to nodes that have more than 85% disk used. It can also be set to an absolute byte value (like 500mb) to prevent Elasticsearch from allocating shards if less than the specified amount of space is available. This setting has no effect on the primary shards of newly-created indices but will prevent their replicas from being allocated.</p> 
<h4><a id="3_clusterroutingallocationdiskwatermarkhigh_120"></a>3. cluster.routing.allocation.disk.watermark.high</h4> 
<p>高风险水位设置，这个设置是90%， 当某个node的磁盘使用率达到90%的时候，elasticsearch就会考虑将一部分shard从这个node上面relocate away 到别的node上面。同样的，这个也可以设置为一个实际的值，比如500mb。<br> 这个设置会影响所有的shard的allocation，不论是之前已经分配过的shard或者是新创建的index的shard的分配。</p> 
<p>Controls the high watermark. It defaults to 90%, meaning that Elasticsearch will attempt to relocate shards away from a node whose disk usage is above 90%. It can also be set to an absolute byte value (similarly to the low watermark) to relocate shards away from a node if it has less than the specified amount of free space. This setting affects the allocation of all shards, whether previously allocated or not.</p> 
<h4><a id="4_clusterroutingallocationdiskwatermarkflood_stage_127"></a>4. cluster.routing.allocation.disk.watermark.flood_stage</h4> 
<p>濒临崩溃阶段，这个设置默认值是95%，当某个node的磁盘使用达到这个水平以后，这个node上的shard对应的index都会被设置为<code>index.blocks.read_only_allow_delete</code>,也就是只允许读操作和删除操作，这是es为了应对集群崩溃不得不采取的一个操作，而且在cluster中的node解除磁盘风险后需要手动进行<code>index.blocks</code>的只读设置的解除。</p> 
<p>Controls the flood stage watermark. It defaults to 95%, meaning that Elasticsearch enforces a read-only index block (index.blocks.read_only_allow_delete) on every index that has one or more shards allocated on the node that has at least one disk exceeding the flood stage. This is a last resort to prevent nodes from running out of disk space. The index block must be released manually once there is enough disk space available to allow indexing operations to continue.</p> 
<p>You can not mix the usage of percentage values and byte values within these settings. Either all are set to percentage values, or all are set to byte values. This is so that we can we validate that the settings are internally consistent (that is, the low disk threshold is not more than the high disk threshold, and the high disk threshold is not more than the flood stage threshold).</p> 
<p>非常需要注意的一点是</p> 
<pre><code>cluster.routing.allocation.disk.watermark.low
cluster.routing.allocation.disk.watermark.high
cluster.routing.allocation.disk.watermark.flood_stage

</code></pre> 
<p>这三个参数的配置类型要保持一致性，也就是说如果使用的是百分比配置则这三个参数都要使用百分比配置，如果想使用具体的大小值设置则都要使用大小值设置。<br> 同时，使用百分比配置的时候是指已经使用的磁盘占比，使用具体值大小的时候指的是剩余空闲磁盘空间容量。</p> 
<h4><a id="5_clusterinfoupdateinterval_144"></a>5. cluster.info.update.interval</h4> 
<p>elasticsearch检查磁盘使用量的频率，默认是每隔30s检查一次。<br> How often Elasticsearch should check on disk usage for each node in the cluster. Defaults to 30s.</p> 
<h4><a id="6_clusterroutingallocationdiskinclude_relocations_148"></a>6. cluster.routing.allocation.disk.include_relocations</h4> 
<p>这个设置控制了cluster在计算一个node的磁盘的使用量的时候是否会加上relacating的shard的磁盘使用，默认是true。<br> 这种计算方式会在磁盘使用量较高node的磁盘使用量计算上产生误差，因为他可能已经将一个shard的90%都迁移出去了，但是我们统计的时候使用的是整个shard的值。</p> 
<p>Defaults to true, which means that Elasticsearch will take into account shards that are currently being relocated to the target node when computing a node’s disk usage. Taking relocating shards’ sizes into account may, however, mean that the disk usage for a node is incorrectly estimated on the high side, since the relocation could be 90% complete and a recently retrieved disk usage would include the total size of the relocating shard as well as the space already used by the running relocation.</p> 
<h4><a id="7__156"></a>7. 一个使用样例</h4> 
<p>若果我们想将低风险水位设置在磁盘剩余容量100G，高风险水位设置在磁盘剩余容量50G，濒临崩溃的风险水位设置在剩余容量为10G，那么我们可以这样设置。</p> 
<pre><code>PUT _cluster/settings
{
  "transient": {
    "cluster.routing.allocation.disk.watermark.low": "100gb",
    "cluster.routing.allocation.disk.watermark.high": "50gb",
    "cluster.routing.allocation.disk.watermark.flood_stage": "10gb",
    "cluster.info.update.interval": "1m"
  }
}

</code></pre> 
<p>An example of updating the low watermark to at least 100 gigabytes free, a high watermark of at least 50 gigabytes free, and a flood stage watermark of 10 gigabytes free, and updating the information about the cluster every minute:</p> 
<h3><a id="4_allocation_node_181"></a>4. 通过属性配置设置,达到allocation 分配时对node的感知</h3> 
<p>这一块儿的配置咋一看基本上和前文当中对index filter的使用中记录的类似,但是真的是相似而不相同。<br> 这一块儿主要是针对整个集群的配置。</p> 
<h4><a id="1_allocation__186"></a>1. 开启集群allocation 感知</h4> 
<p>1.给对应的node设置attribute,假如我们为每个node标记一个容量size属性，有small,medium,big三个属性，</p> 
<pre><code>
node.attr.rack_id: rack_one

或者

`./bin/elasticsearch -Enode.attr.rack_id=rack_one`

</code></pre> 
<p>2.在每个master-eligible node的elasticsearch.yml文件中开启设置</p> 
<pre><code>cluster.routing.allocation.awareness.attributes: rack_id 

</code></pre> 
<p>也可以通过对应啊<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.2/cluster-update-settings.html" rel="nofollow">api</a>来进行动态设置</p> 
<p>在这种情况下，如果你进行如下操作:</p> 
<ol><li>start 2个配置为<code>node.attr.rack_id:rack_one</code>的node</li><li>创建一个index，这个index有5个primary shard，每个primary有1个replica</li><li>这个时候10个shard会被分配在这两个node上面，但是并不会考虑是否有某个shard的replica和primary在同一个node上面，因为cluster认为两个node是同一个node,因为他们对应的rack_id是一样的</li><li>如果再添加两个配置为<code>node.attr.rack_id:rack_two</code>的node,es会把部分shard迁移到新的node上面，并且会保证同一个shard的primary和replica不会在相同的rack_id的nodes上面</li><li>如果配置为<code>node.attr.rack_id:rack_two</code>的node挂掉了，es会把所有的shard都allocated到<code>node.attr.rack_id:rack_one</code>的node上面</li><li>如果想要同一个shard的primary和replica不会分配到相同的rack_id的nodes上，可以开启强制感知</li></ol> 
<h4><a id="2__214"></a>2. 强制感知是什么呢</h4> 
<p>强制感知可以避免同一个atrribute id的nodes持有某个shard的primary和replica，因为同一个attribute id被认为具有强关联的机器，可能会同时挂掉,通过强制感知可以降低数据丢失的风险<br> 先来看看强制感知如何使用</p> 
<pre><code>cluster.routing.allocation.awareness.attributes: zone
cluster.routing.allocation.awareness.force.zone.values: zone1,zone2 

</code></pre> 
<p>这里设置了强制感知的attribute的值为zone1,zone2</p> 
<p>还拿上面的例子来说</p> 
<ol><li>start 2个配置为<code>node.attr.zone:zone1</code>的node</li><li>创建一个index，这个index有5个primary shard，每个primary有1个replica</li><li>这个时候只有5个primary会被分配到两个node上面，replica shard并不会被分配，直到有<code>node.attr.zone:zone2</code>的node加入到集群当中</li></ol> 
<h3><a id="5_clustershard_allocation_filter__232"></a>5. cluster级别的shard allocation filter 设置</h3> 
<p>在cluster级别设置一些filter和在index级别设置filter的使用方式类似，但是作用范围是cluster级别<br> 使用的样式如下</p> 
<pre><code>PUT _cluster/settings
{
  "transient" : {
    "cluster.routing.allocation.exclude._ip" : "10.0.0.1"
  }
}
</code></pre> 
<p>对应的可以是自定义的node attribute, 或者是是内建的_name, _ip, _host attributes.<br> 对应的setting有</p> 
<h4><a id="1_include_246"></a>1. include</h4> 
<p>cluster.routing.allocation.include.{attribute}</p> 
<p>只需要node的attribute中有一个在当前include的配置列表当中即可<br> Allocate shards to a node whose {attribute} has at least one of the comma-separated values.</p> 
<h4><a id="2_require_253"></a>2. require</h4> 
<p>cluster.routing.allocation.require.{attribute}<br> 对应的node必须有全部的当前配置的attribute才会将分片分配上去<br> Only allocate shards to a node whose {attribute} has all of the comma-separated values.</p> 
<h4><a id="3_exclude_258"></a>3. exclude</h4> 
<p>cluster.routing.allocation.exclude.{attribute}<br> 对应的node没有任何当前配置的的attribute才会将分片分配上去<br> Do not allocate shards to a node whose {attribute} has any of the comma-separated values.</p> 
<h4><a id="4__264"></a>4. 也可以使用正则来进行配置</h4> 
<pre><code>PUT _cluster/settings
{
  "transient": {
    "cluster.routing.allocation.exclude._ip": "192.168.2.*"
  }
}

</code></pre>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/81ec0280ae17c8f66ed3269ad3abdf5c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">axure RP文件如何找回_u盘文件丢失怎么办 u盘文件丢失恢复方法【步骤详解】</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1accf27cff1b6ddfd1105afe06c220df/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">解决pyuic5: error: no such option: -m问题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>