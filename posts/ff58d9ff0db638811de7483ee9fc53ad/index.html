<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>[PyTorch][chapter 8][李宏毅深度学习][DNN 训练技巧] - 编程爱好者博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="[PyTorch][chapter 8][李宏毅深度学习][DNN 训练技巧]" />
<meta property="og:description" content="前言：
DNN 是神经网络的里面基础核心模型之一.这里面结合DNN 介绍一下如何解决
深度学习里面过拟合,欠拟合问题
目录：
DNN 训练常见问题 过拟合处理 欠拟合处理 keras 项目 一 DNN 训练常见问题
我们在深度学习网络训练的时候经常会遇到下面两类问题：
1： 训练集上面很差 ： 欠拟合
2： 训练集上面很好, 测试集上面很差： 过拟合
二 过拟合解决
过拟合解决方案
主要有以下三个处理思路
1 Early Stopped
2 L1 L2 正规化
3 Dropout
4： 增加训练集上面的数据量
2.1 Early Stopping
方案
这个数据集分为3部分： Training Data,validation data,Test Data
1 将训练的数据分为Training Data 和validation data
2 每个epoch结束后（或每N个epoch后)：计算validation data 的 accuracy 3: 更新 最优 validation data accuracy 对应的网络参数
3 随着epoch的增加，如果validation data 连续多次没有提升，则停止训练；" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bchobby.github.io/posts/ff58d9ff0db638811de7483ee9fc53ad/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-25T17:58:08+08:00" />
<meta property="article:modified_time" content="2023-12-25T17:58:08+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程爱好者博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程爱好者博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">[PyTorch][chapter 8][李宏毅深度学习][DNN 训练技巧]</h1>
			
		</header>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>前言：</p> 
<p>   </p> 
<p>       DNN 是神经网络的里面基础核心模型之一.这里面结合DNN 介绍一下如何解决</p> 
<p>深度学习里面过拟合,欠拟合问题</p> 
<p></p> 
<hr> 
<p>目录：</p> 
<p></p> 
<ol><li>     DNN 训练常见问题</li><li>     过拟合处理</li><li>    欠拟合处理</li><li>    keras 项目</li></ol> 
<p></p> 
<hr> 
<p><strong>一  DNN 训练常见问题</strong></p> 
<p><img alt="" height="264" src="https://images2.imgbox.com/96/f0/upQermjH_o.png" width="351"></p> 
<p>  我们在深度学习网络训练的时候经常会遇到下面两类问题：</p> 
<p>         1：  训练集上面很差 ： 欠拟合</p> 
<p>         2： 训练集上面很好, 测试集上面很差： 过拟合</p> 
<p></p> 
<hr> 
<p><strong>二  过拟合解决</strong></p> 
<p></p> 
<table cellspacing="0"><tbody><tr><td style="background-color:#ffffff;width:426.1pt;"> <p style="margin-left:0;text-align:left;"><strong><em><span style="color:#2e74b5;">过拟合解决方案</span></em></strong></p> </td></tr><tr><td style="background-color:#ffffff;width:426.1pt;"> <p style="margin-left:0;text-align:left;">主要有以下三个处理思路</p> </td></tr><tr><td style="background-color:#ffffff;width:426.1pt;"> <p style="margin-left:0;text-align:left;"><strong><em><span style="color:#2e74b5;">1 Early Stopped</span></em></strong></p> </td></tr><tr><td style="background-color:#ffffff;width:426.1pt;"> <p style="margin-left:0;text-align:left;"><strong><em><span style="color:#2e74b5;">2 L1 L2 </span></em></strong><strong><em><span style="color:#2e74b5;">正规化</span></em></strong></p> </td></tr><tr><td style="background-color:#ffffff;width:426.1pt;"> <p style="margin-left:0;text-align:left;"><strong><em><span style="color:#2e74b5;">3 Dropout</span></em></strong></p> <p style="margin-left:0;text-align:left;"></p> <p style="margin-left:0;text-align:left;"><strong><em><span style="color:#2e74b5;">4： 增加训练集上面的数据量</span></em></strong></p> </td></tr></tbody></table> 
<p> 2.1  Early Stopping</p> 
<p><img alt="" height="241" src="https://images2.imgbox.com/6d/e4/0MMRv7sB_o.png" width="387"></p> 
<blockquote> 
 <p>   方案</p> 
 <p>   这个数据集分为3部分： Training Data,validation data,Test Data<br>    1  将训练的数据分为Training Data 和validation data<br>    2  每个epoch结束后（或每N个epoch后)：计算validation data 的 accuracy <br>    3: 更新 最优 validation data accuracy 对应的网络参数<br>    3  随着epoch的增加，如果validation data 连续多次没有提升，则停止训练；<br>    4  将之前validation data 准确率最高时的权重作为网络的最终参数。</p> 
</blockquote> 
<p>2.2  正规化</p> 
<p>      分为L1,L2 正规化.</p> 
<p></p> 
<p><img alt="" height="269" src="https://images2.imgbox.com/fc/d0/bv4Ed7I7_o.png" width="406"></p> 
<p><img alt="" height="289" src="https://images2.imgbox.com/db/bd/Dwa3CKwK_o.png" width="405"></p> 
<p><strong>2.3 Dropout</strong></p> 
<p><img alt="" height="309" src="https://images2.imgbox.com/bf/84/9zHF9uKC_o.png" width="1200"></p> 
<blockquote> 
 <p><strong>原网络结构</strong></p> 
 <p><strong>            <img alt="z^{l+1}=w^{l+1}a^l+b^{l+1}" class="mathcode" src="https://images2.imgbox.com/1c/06/gnANsPc7_o.png"></strong></p> 
 <p><strong>            <img alt="a^{l+1}=\sigma(z^{l+1})" class="mathcode" src="https://images2.imgbox.com/b8/79/qzR3Snz1_o.png"></strong></p> 
 <p><strong>训练：</strong></p> 
 <p><strong>            Dropout</strong></p> 
 <p><strong>            <img alt="a^{l}" class="mathcode" src="https://images2.imgbox.com/37/64/UTUVnNH6_o.png">: 上面每个输入值以p%的概率变为0     </strong></p> 
 <p><strong>           <img alt="z^{l+1}=w^{l+1}a^l+b^{l+1}" class="mathcode" src="https://images2.imgbox.com/00/3c/OUnFr4ax_o.png"></strong></p> 
 <p><strong>            <img alt="a^{l+1}=\sigma(z^{l+1})" class="mathcode" src="https://images2.imgbox.com/14/3e/fVe2fYo9_o.png"></strong></p> 
 <p><strong>测试： </strong></p> 
 <p><strong>          权重系数</strong></p> 
 <p><strong>             <img alt="w^{l}=w^l*(1-p)" class="mathcode" src="https://images2.imgbox.com/4f/b9/ojxyH9Nn_o.png"></strong></p> 
 <p><strong>            一般p 设置为0.5</strong></p> 
 <p><strong>           </strong></p> 
</blockquote> 
<p></p> 
<p><img alt="" height="274" src="https://images2.imgbox.com/31/4e/x3RayUn0_o.png" width="423"></p> 
<p></p> 
<p><img alt="" height="294" src="https://images2.imgbox.com/bb/9e/NP9QhJSq_o.png" width="475"></p> 
<p></p> 
<p><img alt="" height="321" src="https://images2.imgbox.com/cd/81/AmDuctYu_o.png" width="465"></p> 
<p>、<img alt="" height="401" src="https://images2.imgbox.com/74/12/haW2nqHc_o.png" width="538"></p> 
<p>4  增加数据集上面数据量</p> 
<p>      作用  降低方差</p> 
<hr> 
<p><strong>三  欠拟合</strong></p> 
<table cellspacing="0"><tbody><tr><td style="background-color:#ffffff;width:426.1pt;"> <p style="margin-left:0;text-align:left;"><strong><em><span style="color:#2e74b5;">欠拟合处理方案</span></em></strong></p> </td></tr><tr><td style="background-color:#ffffff;width:426.1pt;"> <p style="margin-left:0;text-align:left;">主要有下面5个处理思路：</p> </td></tr><tr><td style="background-color:#ffffff;width:426.1pt;"> <p style="margin-left:0;text-align:left;"><strong><em><span style="color:#2e74b5;">     1 超参数调节： 学习率 训练轮次,batch_size</span></em></strong></p> <p style="margin-left:0;text-align:left;"></p> <p style="margin-left:0;text-align:left;"><strong><em><span style="color:#2e74b5;">     2 更换激活函数</span></em></strong></p> </td></tr><tr><td style="background-color:#ffffff;width:426.1pt;"> <p style="margin-left:0;text-align:left;"><strong><em><span style="color:#2e74b5;">     3 梯度更新算法优化</span></em></strong></p> <p style="margin-left:0;text-align:left;"><strong><em><span style="color:#2e74b5;">     4  网络模型优化</span></em></strong></p> <p style="margin-left:0;text-align:left;"><strong><em><span style="color:#2e74b5;">     5 损失函数 更换</span></em></strong></p> </td></tr></tbody></table> 
<p>3.1  超参数调参</p> 
<p>         主要更换学习率，增加迭代轮数等</p> 
<p><img alt="" height="186" src="https://images2.imgbox.com/4d/ed/qeWeEu8M_o.png" width="619"></p> 
<p> </p> 
<p></p> 
<p>3.2 更换激活函数</p> 
<p></p> 
<p>      DNN 随着网络层数的增加会出现梯度弥散现象，可以通过把激活函数sigmod 更换为</p> 
<p>ReLu 一定程度上面优化该方案。    </p> 
<p>    更换激活函数 ReLu(导数为1,链式求导的时候连乘不会减少)</p> 
<p><img alt="" height="183" src="https://images2.imgbox.com/78/32/v6hWNAAF_o.png" width="269"></p> 
<p><img alt="" height="267" src="https://images2.imgbox.com/ed/3e/ettIDlu2_o.png" width="391"></p> 
<p>        增加,减少 网络层数（梯度弥散,梯度爆炸）</p> 
<p>        <img alt="" height="257" src="https://images2.imgbox.com/40/05/8clyIINX_o.png" width="476"></p> 
<p><strong>3.3 梯度更新优化算法</strong></p> 
<p></p> 
<p><img alt="" height="114" src="https://images2.imgbox.com/de/bb/vGFRUvhG_o.png" width="571"></p> 
<p><strong>      方案1  SGD 随机梯度下降</strong></p> 
<p><strong>         <img alt="\theta=\theta-\eta \bigtriangledown J(\theta)" class="mathcode" src="https://images2.imgbox.com/bc/37/uwXkdOGk_o.png"></strong></p> 
<p><strong>        </strong>当梯度为0，参数无法更新容易陷入到局部极小值点</p> 
<p>        学习率太大： 不容易进入到极小值点，容易发生网络震荡</p> 
<p>         学习率太小： 收敛速度慢</p> 
<p><img alt="" height="297" src="https://images2.imgbox.com/e6/63/A3ovHXu6_o.png" width="526"></p> 
<p></p> 
<p><strong> 方案2 Momentum: 当前的梯度 = 当前的梯度+历史梯度</strong></p> 
<p></p> 
<p><strong>           </strong>SGD 会发生震荡而迟迟不能接近极小值，所以对更新梯度引入<strong>Momentum</strong>概念，加速SGD，并抑制震荡（也就是在SGD基础上引入了一阶动量）</p> 
<blockquote> 
 <p>            初始化动量：</p> 
 <p>                           <img alt="m_0 =0" class="mathcode" src="https://images2.imgbox.com/5a/7c/L9d21b61_o.png">: 动量</p> 
 <p>     </p> 
 <p>                             <img alt="m_{t}=\lambda m_{t-1}+(1-\lambda)\bigtriangledown J(\theta_t)" class="mathcode" src="https://images2.imgbox.com/7b/3a/rER3mNx2_o.png">: 动量</p> 
 <p>                             <img alt="\theta_t=\theta_t -\eta m_t" class="mathcode" src="https://images2.imgbox.com/28/76/Oh3vtcvO_o.png"></p> 
 <p>           整个思想： 有点跟马尔科夫链时序链相似，当前输出值不仅仅跟当前的</p> 
 <p>输入相关,也跟历史值相关。</p> 
</blockquote> 
<p>            </p> 
<p><strong>方案3：Adagrad （Adaptive Gradient，自适应梯度)</strong></p> 
<p><strong>             </strong>不同参数进行不同程度的更新 - 逐参数适应学习率方法</p> 
<blockquote> 
 <p>           方案：</p> 
 <p>          在Adagrad算法中，每个参数的学习率各不相同。计算某参数的学习率时需将该参数前面所有时间步的梯度平方求和，随着时间步的增加，学习率将减小.</p> 
 <p>      <img alt="v_t=\sum_{\tau=0}^{t}g_{\tau }^2" class="mathcode" src="https://images2.imgbox.com/54/2c/txeTmVmj_o.png"></p> 
 <p>       <img alt="\theta_t=\theta_t-\frac{\eta }{\sqrt{v_t+\varepsilon }}g_t" class="mathcode" src="https://images2.imgbox.com/7b/3e/ahuw66br_o.png"></p> 
 <p>      </p> 
 <p></p> 
 <p></p> 
 <p><img alt="v_t:" class="mathcode" src="https://images2.imgbox.com/f1/61/awXUtic9_o.png"> 二阶动量，权重系数里面的每个系数单独计算</p> 
 <p><img alt="\epsilon =1e-7" class="mathcode" src="https://images2.imgbox.com/78/33/4PmdiDN5_o.png"></p> 
 <p><img alt="g_t" class="mathcode" src="https://images2.imgbox.com/50/83/MLyuz8nX_o.png">: 当前权重系数的梯度</p> 
 <p></p> 
 <p>Adgrad方法中，学习率一直在衰减，所以可以起到抑制震荡的作用，</p> 
 <p>对于频繁更新的参数，它们的二阶动量比较大，学习率小；</p> 
 <p>对于不怎么更新的参数，它们的二阶动量比较小，学习率就大。</p> 
 <p>但因为那个分母是单调递增的，会使得学习率单调递减至0，可能会使得训练过程提前结束，即便后续还有数据也无法学到必要的知识</p> 
 <p></p> 
</blockquote> 
<p>    </p> 
<p></p> 
<p><strong>方案4 RMSProp:</strong></p> 
<p>Root Mean Square Propagation，自适应学习率方法，由Geoff Hinton提出，是梯度下降优化算法的扩展。在AdaGrad的基础上，对二阶动量的计算进行了改进：即有历史梯度的信息，但是我又不想让信息一直膨胀，那么只要让历史信息一直衰减就好了。因此得到RMSProp的二阶动量计算公式：</p> 
<p>如下图所示，截图来自：https://arxiv.org/pdf/1609.04747.pdf</p> 
<p><img alt="" height="235" src="https://images2.imgbox.com/96/e4/mqB3LJhz_o.png" width="590"></p> 
<p></p> 
<p></p> 
<p><strong>方案4 Adam算法即自适应时刻估计方法（Adaptive Moment Estimation）</strong></p> 
<p><img alt="" height="132" src="https://images2.imgbox.com/4c/f7/aPSTQHw6_o.png" width="566"></p> 
<p>算法思想  moment+<strong>Adagrad</strong></p> 
<p>同时考虑了动量 和二阶动量</p> 
<p><img alt="" height="348" src="https://images2.imgbox.com/56/ea/GHkW540j_o.png" width="550"></p> 
<p></p> 
<p>3.4  更换损失函数</p> 
<p>      比如mse 更换成CRE</p> 
<p><img alt="" height="107" src="https://images2.imgbox.com/88/1f/U4C3S2L4_o.png" width="607"></p> 
<p>3.5 更换模型</p> 
<p>         增加网络层次，参数例如</p> 
<p><img alt="" height="85" src="https://images2.imgbox.com/ea/3e/bUmEqqPx_o.png" width="550"></p> 
<p>      或者</p> 
<p>           RNN 用LSTM  </p> 
<p>           CNN 里面的ResNet</p> 
<p>             解决梯度弥散问题</p> 
<hr> 
<hr> 
<p>四   <strong> keras </strong></p> 
<p>          Keras是一个由Python编写的开源人工神经网络库，可以作为Tensorflow、Microsoft-CNTK和Theano的高阶应用程序接口，进行深度学习模型的设计、调试、评估、应用和可视化 [1]。<br> Keras在代码结构上由面向对象方法编写，完全模块化并具有可扩展性，其运行机制和说明文档有将用户体验和使用难度纳入考虑，并试图简化复杂算法的实现难度 [1]。Keras支持现代人工智能领域的主流算法，包括前馈结构和递归结构的神经网络，也可以通过封装参与构建统计学习模型 [2]。在硬件和开发环境方面，Keras支持多操作系统下的多GPU并行计算，可以根据后台设置转化为Tensorflow、Microsoft-CNTK等系统下的组件 [3]。<br> Keras的主要开发者是谷歌工程师François Chollet，此外其GitHub项目页面包含6名主要维护者和超过800名直接贡献者 [4]。Keras在其正式版本公开后，除部分预编译模型外，按MIT许可证开放源代码 [1]</p> 
<p><img alt="" height="269" src="https://images2.imgbox.com/51/0e/jERZtRr3_o.png" width="309"></p> 
<p>   keras 创建一个神经网络,训练,测试主要流程如下</p> 
<table border="1" cellpadding="1" cellspacing="1" style="width:500px;"><tbody><tr><td>model</td><td>模型搭建</td></tr><tr><td>compile</td><td>损失函数，loss, batch_size</td></tr><tr><td>fit</td><td>训练</td></tr><tr><td>evaluate</td><td>验证测试集</td></tr><tr><td>predict</td><td>预测</td></tr></tbody></table> 
<pre><code>model = Sequential()

#输入层
model.add(Dense(input_dim=28*28,
units = 500,
activation='relu'))

#1 隐藏层
model.add(Dense(units=500,
activation='relu'))

#2 输出层
model.add(Dense(units=10,
activation='softmax'))


model.compile(loss='categorical_crossentropy',
optimizer='adam'
metrics =['accuracy'])


#3 pick the best function ,完成训练工作
model.fit(x_train, y_train, batch_size=100, epochs=20)

#4 使用该模型
score = model.evaluate(x_test,y_test)
result = model.predict(x_test)</code></pre> 
<p></p> 
<p>参考：</p> 
<p><a href="https://www.bilibili.com/video/BV13x411v7US?p=18&amp;vd_source=a624c4a1aea4b867c580cc82f03c1745" rel="nofollow" title="9-1: Tips for Training DNN_哔哩哔哩_bilibili">9-1: Tips for Training DNN_哔哩哔哩_bilibili</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/34230849" rel="nofollow" title="【优化算法】一文搞懂RMSProp优化算法 - 知乎">【优化算法】一文搞懂RMSProp优化算法 - 知乎</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/594376438" rel="nofollow" title="神经网络-优化器篇-从梯度下降到Adam方法 - 知乎">神经网络-优化器篇-从梯度下降到Adam方法 - 知乎</a></p> 
<p><a href="https://www.cnblogs.com/picassooo/p/12347927.html" rel="nofollow" title="https://www.cnblogs.com/picassooo/p/12347927.html">https://www.cnblogs.com/picassooo/p/12347927.html</a></p>
                </div>
		</div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f5c8b8e0ad82c5b24b46e697cc94884b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【深度学习-目标检测】01 - R-CNN 论文学习与总结</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/dd1c3162d98511a5c00152af9fa12826/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【c&#43;&#43;】入门2</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程爱好者博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

<script src="https://www.w3counter.com/tracker.js?id=151260"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>